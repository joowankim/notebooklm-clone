{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Add API Endpoint Documentation for Notebook Module",
        "description": "Update all notebook API endpoint decorators to include summary, description, and responses documentation following DDD architecture rules with HTTPStatus constants.",
        "details": "For each endpoint in src/notebook/entrypoint/api.py:\n\n1. Add or improve `summary` parameter in @router.post/get/patch/delete decorators\n2. Add or improve detailed `description` parameter \n3. Add `responses` parameter documenting all possible response scenarios with proper HTTP status codes\n4. Ensure all status codes use `http.HTTPStatus` constants rather than hardcoded numbers\n5. Document error responses (404 for not found, 400 for validation errors)\n\nEndpoints to update:\n- POST /notebooks (create_notebook)\n- GET /notebooks (list_notebooks)\n- GET /notebooks/{notebook_id} (get_notebook)\n- PATCH /notebooks/{notebook_id} (update_notebook)\n- DELETE /notebooks/{notebook_id} (delete_notebook)\n\nExample implementation:\n```python\n@router.get(\n    \"/{notebook_id}\",\n    response_model=response.NotebookDetail,\n    summary=\"Get notebook details\",\n    description=\"Retrieve detailed information about a specific notebook by its ID.\",\n    responses={\n        http.HTTPStatus.OK: {\"description\": \"Notebook details retrieved successfully\"},\n        http.HTTPStatus.NOT_FOUND: {\"description\": \"Notebook not found\"},\n    },\n)\n```",
        "testStrategy": "1. Manual review of the code to ensure all endpoints have proper documentation\n2. Verify that the FastAPI auto-generated Swagger documentation displays correct information\n3. Validate that responses include all status codes with proper descriptions\n4. Check that HTTPStatus constants are used consistently\n5. Run the API server and access the /docs endpoint to verify documentation renders correctly",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "2",
        "title": "Add API Endpoint Documentation for Document Module",
        "description": "Update document/source API endpoint decorators to include summary, description, and responses documentation with HTTPStatus constants.",
        "details": "For each endpoint in src/document/entrypoint/api.py:\n\n1. Add or improve `summary` parameter in @router.post/get decorators\n2. Add or improve detailed `description` parameter \n3. Add `responses` parameter documenting all possible response scenarios\n4. Ensure all status codes use `http.HTTPStatus` constants\n5. Document error responses (404 for not found, 400 for validation errors, 409 for duplicate URL)\n\nEndpoints to update:\n- POST /notebooks/{notebook_id}/sources (add_source)\n- GET /notebooks/{notebook_id}/sources (list_sources)\n- GET /documents/{document_id} (get_document)\n\nExample implementation:\n```python\n@router.post(\n    \"\",\n    response_model=response.DocumentId,\n    status_code=http.HTTPStatus.CREATED,\n    summary=\"Add source URL to notebook\",\n    description=\"Add a new source URL to a notebook for ingestion and analysis.\",\n    responses={\n        http.HTTPStatus.CREATED: {\"description\": \"Source created successfully\"},\n        http.HTTPStatus.NOT_FOUND: {\"description\": \"Notebook not found\"},\n        http.HTTPStatus.CONFLICT: {\"description\": \"Source URL already exists in notebook\"},\n    },\n)\n```",
        "testStrategy": "1. Manual review of the code to ensure all endpoints have proper documentation\n2. Verify that the FastAPI auto-generated Swagger documentation displays correct information\n3. Validate that responses include all status codes with proper descriptions\n4. Check that HTTPStatus constants are used consistently\n5. Run the API server and access the /docs endpoint to verify documentation renders correctly",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "3",
        "title": "Implement Delete Document/Source Feature",
        "description": "Create a complete feature for deleting source documents from notebooks, including handler, API endpoint, and CLI command with appropriate cleanup of associated chunks.",
        "details": "1. Create DeleteSourceHandler in src/document/handler/handlers.py:\n```python\nclass DeleteSourceHandler:\n    \"\"\"Handler for deleting source documents from notebooks.\"\"\"\n\n    def __init__(\n        self,\n        document_repository: document_repository_module.DocumentRepository,\n        notebook_repository: notebook_repository_module.NotebookRepository,\n        chunk_repository: chunk_repository_module.ChunkRepository,\n    ) -> None:\n        self._document_repository = document_repository\n        self._notebook_repository = notebook_repository\n        self._chunk_repository = chunk_repository\n\n    async def handle(self, notebook_id: str, document_id: str) -> None:\n        \"\"\"Delete a source document from a notebook.\"\"\"\n        # Verify notebook exists\n        notebook = await self._notebook_repository.find_by_id(notebook_id)\n        if notebook is None:\n            raise exceptions.NotFoundError(f\"Notebook not found: {notebook_id}\")\n\n        # Verify document exists and belongs to notebook\n        document = await self._document_repository.find_by_id(document_id)\n        if document is None:\n            raise exceptions.NotFoundError(f\"Document not found: {document_id}\")\n        \n        if document.notebook_id != notebook_id:\n            raise exceptions.ValidationError(\n                f\"Document {document_id} does not belong to notebook {notebook_id}\"\n            )\n\n        # Delete associated chunks first\n        await self._chunk_repository.delete_by_document(document_id)\n        \n        # Delete document\n        await self._document_repository.delete(document_id)\n```\n\n2. Add handler to src/document/handler/__init__.py exports\n\n3. Add endpoint to src/document/entrypoint/api.py:\n```python\n@router.delete(\n    \"/{document_id}\",\n    status_code=http.HTTPStatus.NO_CONTENT,\n    summary=\"Delete source document\",\n    description=\"Delete a source document and its associated chunks from a notebook.\",\n    responses={\n        http.HTTPStatus.NO_CONTENT: {\"description\": \"Document deleted successfully\"},\n        http.HTTPStatus.NOT_FOUND: {\"description\": \"Notebook or document not found\"},\n        http.HTTPStatus.BAD_REQUEST: {\"description\": \"Document does not belong to notebook\"},\n    },\n)\n@inject\nasync def delete_source(\n    notebook_id: str,\n    document_id: str,\n    handler: handlers.DeleteSourceHandler = fastapi.Depends(\n        Provide[container_module.ApplicationContainer.document.handler.delete_source_handler]\n    ),\n) -> None:\n    \"\"\"Delete a source document from a notebook.\"\"\"\n    await handler.handle(notebook_id, document_id)\n```\n\n4. Update container to register the handler\n\n5. Add CLI command in src/cli/commands/source.py:\n```python\n@app.command(\"delete\")\ndef delete_source(\n    notebook_id: str = typer.Argument(..., help=\"Notebook ID\"),\n    document_id: str = typer.Argument(..., help=\"Document ID\"),\n    force: bool = typer.Option(False, \"--force\", \"-f\", help=\"Skip confirmation\"),\n):\n    \"\"\"Delete a source document from a notebook.\"\"\"\n    asyncio.run(_delete_source(notebook_id, document_id, force))\n\n\nasync def _delete_source(notebook_id: str, document_id: str, force: bool):\n    from src.document.adapter.repository import DocumentRepository\n    from src.chunk.adapter.repository import ChunkRepository\n    from src.notebook.adapter.repository import NotebookRepository\n\n    async with get_session_context() as session:\n        # Verify notebook exists\n        notebook_repo = NotebookRepository(session)\n        notebook = await notebook_repo.find_by_id(notebook_id)\n        if notebook is None:\n            console.print(f\"[red]Notebook not found:[/red] {notebook_id}\")\n            raise typer.Exit(1)\n\n        # Verify document exists\n        document_repo = DocumentRepository(session)\n        document = await document_repo.find_by_id(document_id)\n        if document is None:\n            console.print(f\"[red]Document not found:[/red] {document_id}\")\n            raise typer.Exit(1)\n            \n        if document.notebook_id != notebook_id:\n            console.print(f\"[red]Document does not belong to notebook:[/red] {document_id}\")\n            raise typer.Exit(1)\n\n        if not force:\n            confirm = typer.confirm(\n                f\"Delete document '{document.title or document.url}'? This will also delete all associated chunks.\"\n            )\n            if not confirm:\n                raise typer.Abort()\n\n        # Delete chunks first\n        chunk_repo = ChunkRepository(session)\n        chunks = await chunk_repo.list_by_document(document_id)\n        chunk_count = len(chunks)\n        await chunk_repo.delete_by_document(document_id)\n        \n        # Delete document\n        await document_repo.delete(document_id)\n        await session.commit()\n\n        console.print(f\"[green]Document deleted:[/green] {document_id}\")\n        console.print(f\"[green]{chunk_count} chunks deleted[/green]\")\n```\n\n6. Add the delete_by_document method to ChunkRepository if not already present",
        "testStrategy": "1. Create unit tests for DeleteSourceHandler:\n   - Test happy path (successful deletion)\n   - Test not found notebook\n   - Test not found document\n   - Test document not belonging to notebook\n\n2. Create integration test for the API endpoint:\n   - Test successful deletion and verify 204 status code\n   - Test cascade deletion of chunks\n   - Test error responses for invalid IDs\n   \n3. Manual testing of CLI command with various scenarios",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "4",
        "title": "Implement Notebook Handler Unit Tests",
        "description": "Add comprehensive unit tests for all notebook handlers using mock repositories following the Arrange-Act-Assert pattern.",
        "details": "Create tests/notebook/test_handlers.py with tests for all notebook handlers:\n\n```python\n\"\"\"Tests for Notebook handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.notebook.domain.model import Notebook\nfrom src.notebook.handler.handlers import (\n    CreateNotebookHandler,\n    GetNotebookHandler,\n    ListNotebooksHandler,\n    UpdateNotebookHandler,\n    DeleteNotebookHandler,\n)\nfrom src.notebook.schema.command import CreateNotebook, UpdateNotebook\n\n\nclass TestCreateNotebookHandler:\n    \"\"\"Tests for CreateNotebookHandler.\"\"\"\n\n    def test_create_notebook_success(self):\n        \"\"\"Test successful notebook creation.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.save.return_value = Notebook.create(name=\"Test Notebook\")\n        handler = CreateNotebookHandler(mock_repo)\n        cmd = CreateNotebook(name=\"Test Notebook\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(cmd))\n\n        # Assert\n        assert result.id is not None\n        mock_repo.save.assert_called_once()\n        saved_notebook = mock_repo.save.call_args[0][0]\n        assert saved_notebook.name == \"Test Notebook\"\n\n\nclass TestGetNotebookHandler:\n    \"\"\"Tests for GetNotebookHandler.\"\"\"\n\n    def test_get_notebook_success(self):\n        \"\"\"Test successful notebook retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_repo.find_by_id.return_value = notebook\n        handler = GetNotebookHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(notebook.id)\n        assert result.id == notebook.id\n        assert result.name == notebook.name\n\n    def test_get_notebook_not_found(self):\n        \"\"\"Test notebook not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetNotebookHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListNotebooksHandler:\n    \"\"\"Tests for ListNotebooksHandler.\"\"\"\n\n    def test_list_notebooks_success(self):\n        \"\"\"Test successful notebook listing.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        notebooks = [\n            Notebook.create(name=\"Notebook 1\"),\n            Notebook.create(name=\"Notebook 2\"),\n        ]\n        mock_result = MagicMock()\n        mock_result.items = notebooks\n        mock_result.total = len(notebooks)\n        mock_result.page = 1\n        mock_result.size = 10\n        mock_repo.list.return_value = mock_result\n        handler = ListNotebooksHandler(mock_repo)\n        from src.notebook.schema.query import ListNotebooks\n        query = ListNotebooks(page=1, size=10)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(query))\n\n        # Assert\n        mock_repo.list.assert_called_once()\n        assert result.total == 2\n        assert len(result.items) == 2\n        assert result.items[0].name == \"Notebook 1\"\n        assert result.items[1].name == \"Notebook 2\"\n        assert result.page == 1\n        assert result.size == 10\n\n\nclass TestUpdateNotebookHandler:\n    \"\"\"Tests for UpdateNotebookHandler.\"\"\"\n\n    def test_update_notebook_success(self):\n        \"\"\"Test successful notebook update.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        original = Notebook.create(name=\"Original\")\n        updated = original.update(name=\"Updated\")\n        mock_repo.find_by_id.return_value = original\n        mock_repo.save.return_value = updated\n        handler = UpdateNotebookHandler(mock_repo)\n        cmd = UpdateNotebook(name=\"Updated\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(original.id, cmd))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(original.id)\n        mock_repo.save.assert_called_once()\n        assert result.name == \"Updated\"\n        assert result.id == original.id\n\n    def test_update_notebook_not_found(self):\n        \"\"\"Test update notebook not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = UpdateNotebookHandler(mock_repo)\n        cmd = UpdateNotebook(name=\"Updated\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n\nclass TestDeleteNotebookHandler:\n    \"\"\"Tests for DeleteNotebookHandler.\"\"\"\n\n    def test_delete_notebook_success(self):\n        \"\"\"Test successful notebook deletion.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        notebook = Notebook.create(name=\"Test\")\n        mock_repo.find_by_id.return_value = notebook\n        handler = DeleteNotebookHandler(mock_repo)\n\n        # Act\n        pytest.asyncio.run(handler.handle(notebook.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_repo.delete.assert_called_once_with(notebook.id)\n\n    def test_delete_notebook_not_found(self):\n        \"\"\"Test delete notebook not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = DeleteNotebookHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the notebook handler module\n2. Verify tests for all happy path scenarios\n3. Verify tests for all error cases (not found, validation errors)\n4. Confirm that tests follow the Arrange-Act-Assert pattern\n5. Validate that tests verify the immutability of domain models",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "5",
        "title": "Implement Document Handler Unit Tests",
        "description": "Add comprehensive unit tests for all document handlers, including the new delete handler, with mock repositories.",
        "details": "Create tests/document/test_handlers.py with tests for all document handlers:\n\n```python\n\"\"\"Tests for Document handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.document.domain.model import Document\nfrom src.notebook.domain.model import Notebook\nfrom src.document.handler.handlers import (\n    AddSourceHandler,\n    GetDocumentHandler,\n    ListSourcesHandler,\n    DeleteSourceHandler,  # New handler\n)\nfrom src.document.schema.command import AddSource\n\n\nclass TestAddSourceHandler:\n    \"\"\"Tests for AddSourceHandler.\"\"\"\n\n    def test_add_source_success(self):\n        \"\"\"Test successful source addition.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_ingestion = MagicMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        document = Document.create(notebook_id=notebook.id, url=\"https://example.com\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_doc_repo.find_by_notebook_and_url.return_value = None\n        mock_doc_repo.save.return_value = document\n        \n        handler = AddSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            background_ingestion=mock_ingestion,\n        )\n        \n        cmd = AddSource(url=\"https://example.com\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_doc_repo.find_by_notebook_and_url.assert_called_once()\n        mock_doc_repo.save.assert_called_once()\n        mock_ingestion.trigger_ingestion.assert_called_once_with(document)\n        assert result.id == document.id\n\n    def test_add_source_notebook_not_found(self):\n        \"\"\"Test adding source to non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_ingestion = MagicMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = AddSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            background_ingestion=mock_ingestion,\n        )\n        \n        cmd = AddSource(url=\"https://example.com\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_add_source_duplicate_url(self):\n        \"\"\"Test adding duplicate URL raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_ingestion = MagicMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        existing_document = Document.create(notebook_id=notebook.id, url=\"https://example.com\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_doc_repo.find_by_notebook_and_url.return_value = existing_document\n        \n        handler = AddSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            background_ingestion=mock_ingestion,\n        )\n        \n        cmd = AddSource(url=\"https://example.com\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.ValidationError):\n            pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n\nclass TestGetDocumentHandler:\n    \"\"\"Tests for GetDocumentHandler.\"\"\"\n\n    def test_get_document_success(self):\n        \"\"\"Test successful document retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        document = Document.create(notebook_id=\"notebook123\", url=\"https://example.com\")\n        mock_repo.find_by_id.return_value = document\n        handler = GetDocumentHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(document.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(document.id)\n        assert result.id == document.id\n        assert result.url == document.url\n\n    def test_get_document_not_found(self):\n        \"\"\"Test document not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetDocumentHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListSourcesHandler:\n    \"\"\"Tests for ListSourcesHandler.\"\"\"\n\n    def test_list_sources_success(self):\n        \"\"\"Test successful sources listing.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        documents = [\n            Document.create(notebook_id=notebook.id, url=\"https://example1.com\"),\n            Document.create(notebook_id=notebook.id, url=\"https://example2.com\"),\n        ]\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        mock_result = MagicMock()\n        mock_result.items = documents\n        mock_result.total = len(documents)\n        mock_result.page = 1\n        mock_result.size = 10\n        \n        mock_doc_repo.list_by_notebook.return_value = mock_result\n        \n        handler = ListSourcesHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        from src.document.schema.query import ListSources\n        query = ListSources(notebook_id=notebook.id, page=1, size=10)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(query))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_doc_repo.list_by_notebook.assert_called_once_with(notebook.id, query)\n        assert result.total == 2\n        assert len(result.items) == 2\n        assert result.items[0].url == \"https://example1.com\"\n        assert result.items[1].url == \"https://example2.com\"\n\n    def test_list_sources_notebook_not_found(self):\n        \"\"\"Test listing sources for non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = ListSourcesHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        from src.document.schema.query import ListSources\n        query = ListSources(notebook_id=\"nonexistent\", page=1, size=10)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(query))\n\n\nclass TestDeleteSourceHandler:\n    \"\"\"Tests for DeleteSourceHandler.\"\"\"\n\n    def test_delete_source_success(self):\n        \"\"\"Test successful source deletion.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        document = Document.create(notebook_id=notebook.id, url=\"https://example.com\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_doc_repo.find_by_id.return_value = document\n        \n        handler = DeleteSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n        )\n\n        # Act\n        pytest.asyncio.run(handler.handle(notebook.id, document.id))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_doc_repo.find_by_id.assert_called_once_with(document.id)\n        mock_chunk_repo.delete_by_document.assert_called_once_with(document.id)\n        mock_doc_repo.delete.assert_called_once_with(document.id)\n\n    def test_delete_source_notebook_not_found(self):\n        \"\"\"Test deleting source from non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = DeleteSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", \"document123\"))\n\n    def test_delete_source_document_not_found(self):\n        \"\"\"Test deleting non-existent source raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_doc_repo.find_by_id.return_value = None\n        \n        handler = DeleteSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(notebook.id, \"nonexistent\"))\n\n    def test_delete_source_wrong_notebook(self):\n        \"\"\"Test deleting source from wrong notebook raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        \n        notebook1 = Notebook.create(name=\"Notebook 1\")\n        notebook2 = Notebook.create(name=\"Notebook 2\")\n        document = Document.create(notebook_id=notebook2.id, url=\"https://example.com\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook1\n        mock_doc_repo.find_by_id.return_value = document\n        \n        handler = DeleteSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.ValidationError):\n            pytest.asyncio.run(handler.handle(notebook1.id, document.id))\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the document handler module\n2. Verify tests for all happy path scenarios\n3. Verify tests for all error cases (not found, duplicate URL, validation errors)\n4. Confirm that tests follow the Arrange-Act-Assert pattern\n5. Validate that tests verify async ingestion is triggered correctly",
        "priority": "medium",
        "dependencies": [
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "6",
        "title": "Implement Conversation Handler Unit Tests",
        "description": "Add comprehensive unit tests for all conversation handlers with mock repositories and services.",
        "details": "Create tests/conversation/test_handlers.py with tests for all conversation handlers:\n\n```python\n\"\"\"Tests for Conversation handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.conversation.domain.model import Conversation, Message, MessageRole\nfrom src.notebook.domain.model import Notebook\nfrom src.conversation.handler.handlers import (\n    CreateConversationHandler,\n    GetConversationHandler,\n    ListConversationsHandler,\n    DeleteConversationHandler,\n    SendMessageHandler,\n)\nfrom src.conversation.schema.command import CreateConversation, SendMessage\nfrom src.query.service.retrieval import RetrievalService\nfrom src.query.adapter.pydantic_ai.agent import RAGAgent\n\n\nclass TestCreateConversationHandler:\n    \"\"\"Tests for CreateConversationHandler.\"\"\"\n\n    def test_create_conversation_success(self):\n        \"\"\"Test successful conversation creation.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        saved_conversation = None\n        def save_conversation(conversation):\n            nonlocal saved_conversation\n            saved_conversation = conversation\n            return conversation\n            \n        mock_conv_repo.save.side_effect = save_conversation\n        \n        handler = CreateConversationHandler(\n            conversation_repository=mock_conv_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        cmd = CreateConversation(title=\"Test Conversation\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_conv_repo.save.assert_called_once()\n        assert result.id is not None\n        assert saved_conversation.title == \"Test Conversation\"\n        assert saved_conversation.notebook_id == notebook.id\n        assert len(saved_conversation.messages) == 0\n\n    def test_create_conversation_notebook_not_found(self):\n        \"\"\"Test creating conversation in non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = CreateConversationHandler(\n            conversation_repository=mock_conv_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        cmd = CreateConversation(title=\"Test Conversation\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n\nclass TestGetConversationHandler:\n    \"\"\"Tests for GetConversationHandler.\"\"\"\n\n    def test_get_conversation_success(self):\n        \"\"\"Test successful conversation retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        conversation = Conversation(\n            id=\"conv123\",\n            notebook_id=\"notebook123\",\n            title=\"Test Conversation\",\n            messages=(),\n            created_at=None,\n            updated_at=None,\n        )\n        mock_repo.find_by_id.return_value = conversation\n        handler = GetConversationHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(conversation.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(conversation.id)\n        assert result.id == conversation.id\n        assert result.title == conversation.title\n\n    def test_get_conversation_not_found(self):\n        \"\"\"Test conversation not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetConversationHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListConversationsHandler:\n    \"\"\"Tests for ListConversationsHandler.\"\"\"\n\n    def test_list_conversations_success(self):\n        \"\"\"Test successful conversation listing.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        conversations = [\n            Conversation(id=\"conv1\", notebook_id=notebook.id, title=\"Conv 1\", messages=(), created_at=None, updated_at=None),\n            Conversation(id=\"conv2\", notebook_id=notebook.id, title=\"Conv 2\", messages=(), created_at=None, updated_at=None),\n        ]\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        mock_result = MagicMock()\n        mock_result.items = conversations\n        mock_result.total = len(conversations)\n        mock_result.page = 1\n        mock_result.size = 10\n        \n        mock_conv_repo.list_by_notebook.return_value = mock_result\n        \n        handler = ListConversationsHandler(\n            conversation_repository=mock_conv_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        from src.conversation.schema.query import ListConversations\n        query = ListConversations(notebook_id=notebook.id, page=1, size=10)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(query))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_conv_repo.list_by_notebook.assert_called_once_with(notebook.id, query)\n        assert result.total == 2\n        assert len(result.items) == 2\n        assert result.items[0].title == \"Conv 1\"\n        assert result.items[1].title == \"Conv 2\"\n\n    def test_list_conversations_notebook_not_found(self):\n        \"\"\"Test listing conversations for non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = ListConversationsHandler(\n            conversation_repository=mock_conv_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        from src.conversation.schema.query import ListConversations\n        query = ListConversations(notebook_id=\"nonexistent\", page=1, size=10)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(query))\n\n\nclass TestDeleteConversationHandler:\n    \"\"\"Tests for DeleteConversationHandler.\"\"\"\n\n    def test_delete_conversation_success(self):\n        \"\"\"Test successful conversation deletion.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        conversation = Conversation(\n            id=\"conv123\",\n            notebook_id=\"notebook123\",\n            title=\"Test Conversation\",\n            messages=(),\n            created_at=None,\n            updated_at=None,\n        )\n        mock_repo.find_by_id.return_value = conversation\n        handler = DeleteConversationHandler(mock_repo)\n\n        # Act\n        pytest.asyncio.run(handler.handle(conversation.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(conversation.id)\n        mock_repo.delete.assert_called_once_with(conversation.id)\n\n    def test_delete_conversation_not_found(self):\n        \"\"\"Test deleting non-existent conversation raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = DeleteConversationHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestSendMessageHandler:\n    \"\"\"Tests for SendMessageHandler.\"\"\"\n\n    def test_send_message_success(self):\n        \"\"\"Test successful message sending.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        # Create conversation with initial setup\n        conversation = Conversation(\n            id=\"conv123\",\n            notebook_id=\"notebook123\",\n            title=\"Test Conversation\",\n            messages=(),\n            created_at=None,\n            updated_at=None,\n        )\n        mock_conv_repo.find_by_id.return_value = conversation\n        \n        # Mock retrieved chunks\n        mock_chunks = [MagicMock(), MagicMock()]\n        mock_retrieval.retrieve.return_value = mock_chunks\n        \n        # Mock RAG agent answer\n        mock_answer = MagicMock()\n        mock_answer.answer = \"This is the AI response\"\n        mock_answer.citations = []\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = SendMessageHandler(\n            conversation_repository=mock_conv_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = SendMessage(content=\"What is the meaning of life?\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(conversation.id, cmd))\n\n        # Assert\n        mock_conv_repo.find_by_id.assert_called_once_with(conversation.id)\n        mock_retrieval.retrieve.assert_called_once()\n        mock_rag_agent.answer.assert_called_once()\n        \n        # Verify two messages were added (user + assistant)\n        assert mock_conv_repo.add_message.call_count == 2\n        \n        user_message_call = mock_conv_repo.add_message.call_args_list[0]\n        assert user_message_call[0][0] == conversation.id\n        assert user_message_call[0][1].role == MessageRole.USER\n        assert user_message_call[0][1].content == \"What is the meaning of life?\"\n        \n        assistant_message_call = mock_conv_repo.add_message.call_args_list[1]\n        assert assistant_message_call[0][0] == conversation.id\n        assert assistant_message_call[0][1].role == MessageRole.ASSISTANT\n        assert assistant_message_call[0][1].content == \"This is the AI response\"\n        \n        # Verify response\n        assert result.content == \"This is the AI response\"\n\n    def test_send_message_conversation_not_found(self):\n        \"\"\"Test sending message to non-existent conversation raises error.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        mock_conv_repo.find_by_id.return_value = None\n        \n        handler = SendMessageHandler(\n            conversation_repository=mock_conv_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = SendMessage(content=\"Hello\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_send_message_conversation_context_building(self):\n        \"\"\"Test that conversation context is built correctly for multi-turn conversations.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        # Create conversation with existing messages\n        message1 = Message(id=\"msg1\", role=MessageRole.USER, content=\"First question\", citations=None, created_at=None)\n        message2 = Message(id=\"msg2\", role=MessageRole.ASSISTANT, content=\"First answer\", citations=None, created_at=None)\n        \n        conversation = Conversation(\n            id=\"conv123\",\n            notebook_id=\"notebook123\",\n            title=\"Test Conversation\",\n            messages=(message1, message2),\n            created_at=None,\n            updated_at=None,\n        )\n        mock_conv_repo.find_by_id.return_value = conversation\n        \n        # Mock retrieved chunks\n        mock_chunks = [MagicMock(), MagicMock()]\n        mock_retrieval.retrieve.return_value = mock_chunks\n        \n        # Mock RAG agent answer\n        mock_answer = MagicMock()\n        mock_answer.answer = \"This is the AI response\"\n        mock_answer.citations = []\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = SendMessageHandler(\n            conversation_repository=mock_conv_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = SendMessage(content=\"Follow-up question\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(conversation.id, cmd))\n\n        # Assert\n        mock_rag_agent.answer.assert_called_once()\n        # Check that context has previous messages plus the new user message\n        _, kwargs = mock_rag_agent.answer.call_args\n        conversation_history = kwargs.get('conversation_history', [])\n        assert len(conversation_history) >= 2  # Should have at least the previous turn\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the conversation handler module\n2. Verify tests for all happy path scenarios\n3. Verify tests for all error cases (not found conversation, not found notebook)\n4. Confirm that tests verify multi-turn conversation context is built correctly\n5. Validate that tests properly mock RAG agent and retrieval service",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "7",
        "title": "Implement Evaluation Handler Unit Tests",
        "description": "Add comprehensive unit tests for all evaluation handlers using mock repositories and services.",
        "details": "Create tests/evaluation/test_handlers.py with tests for all evaluation handlers:\n\n```python\n\"\"\"Tests for Evaluation handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\nimport uuid\nimport datetime\n\nfrom src import exceptions\nfrom src.evaluation.domain.model import Dataset, Run, TestCase, TestResult, EvaluationStatus\nfrom src.evaluation.domain.metric import MetricType\nfrom src.notebook.domain.model import Notebook\nfrom src.evaluation.handler.handlers import (\n    GenerateDatasetHandler,\n    GetDatasetHandler,\n    ListDatasetsHandler,\n    RunEvaluationHandler,\n    GetRunHandler,\n)\nfrom src.evaluation.schema.command import GenerateDataset, RunEvaluation\n\n\nclass TestGenerateDatasetHandler:\n    \"\"\"Tests for GenerateDatasetHandler.\"\"\"\n\n    def test_generate_dataset_success(self):\n        \"\"\"Test successful dataset generation.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        mock_generator = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        chunks = [MagicMock() for _ in range(5)]\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_chunk_repo.list_by_notebook.return_value = chunks\n        \n        saved_dataset = None\n        def save_dataset(dataset):\n            nonlocal saved_dataset\n            saved_dataset = dataset\n            return dataset\n            \n        mock_dataset_repo.save.side_effect = save_dataset\n        \n        handler = GenerateDatasetHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n            test_case_generator=mock_generator,\n        )\n        \n        cmd = GenerateDataset(num_cases=10, name=\"Test Dataset\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_chunk_repo.list_by_notebook.assert_called_once_with(notebook.id)\n        mock_dataset_repo.save.assert_called_once()\n        mock_generator.generate_test_cases.assert_called_once()\n        \n        assert result.id is not None\n        assert result.name == \"Test Dataset\"\n        assert result.notebook_id == notebook.id\n        assert result.status == str(EvaluationStatus.PENDING)\n\n    def test_generate_dataset_notebook_not_found(self):\n        \"\"\"Test generating dataset for non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        mock_generator = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = GenerateDatasetHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n            test_case_generator=mock_generator,\n        )\n        \n        cmd = GenerateDataset(num_cases=10, name=\"Test Dataset\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_generate_dataset_no_chunks(self):\n        \"\"\"Test generating dataset with no chunks raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        mock_generator = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_chunk_repo.list_by_notebook.return_value = []\n        \n        handler = GenerateDatasetHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n            test_case_generator=mock_generator,\n        )\n        \n        cmd = GenerateDataset(num_cases=10, name=\"Test Dataset\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.ValidationError):\n            pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n\nclass TestGetDatasetHandler:\n    \"\"\"Tests for GetDatasetHandler.\"\"\"\n\n    def test_get_dataset_success(self):\n        \"\"\"Test successful dataset retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        dataset_id = uuid.uuid4().hex\n        \n        dataset = Dataset(\n            id=dataset_id,\n            notebook_id=\"notebook123\",\n            name=\"Test Dataset\",\n            test_cases=(),\n            status=EvaluationStatus.COMPLETED,\n            created_at=datetime.datetime.now(),\n            completed_at=datetime.datetime.now(),\n        )\n        \n        mock_repo.find_by_id.return_value = dataset\n        handler = GetDatasetHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(dataset_id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(dataset_id)\n        assert result.id == dataset_id\n        assert result.name == \"Test Dataset\"\n\n    def test_get_dataset_not_found(self):\n        \"\"\"Test dataset not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetDatasetHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListDatasetsHandler:\n    \"\"\"Tests for ListDatasetsHandler.\"\"\"\n\n    def test_list_datasets_success(self):\n        \"\"\"Test successful dataset listing.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        datasets = [\n            Dataset(\n                id=uuid.uuid4().hex,\n                notebook_id=notebook.id,\n                name=\"Dataset 1\",\n                test_cases=(),\n                status=EvaluationStatus.COMPLETED,\n                created_at=datetime.datetime.now(),\n                completed_at=datetime.datetime.now(),\n            ),\n            Dataset(\n                id=uuid.uuid4().hex,\n                notebook_id=notebook.id,\n                name=\"Dataset 2\",\n                test_cases=(),\n                status=EvaluationStatus.COMPLETED,\n                created_at=datetime.datetime.now(),\n                completed_at=datetime.datetime.now(),\n            ),\n        ]\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_dataset_repo.list_by_notebook.return_value = datasets\n        \n        handler = ListDatasetsHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_dataset_repo.list_by_notebook.assert_called_once_with(notebook.id)\n        assert len(result) == 2\n        assert result[0].name == \"Dataset 1\"\n        assert result[1].name == \"Dataset 2\"\n\n    def test_list_datasets_notebook_not_found(self):\n        \"\"\"Test listing datasets for non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = ListDatasetsHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestRunEvaluationHandler:\n    \"\"\"Tests for RunEvaluationHandler.\"\"\"\n\n    def test_run_evaluation_success(self):\n        \"\"\"Test successful evaluation run.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_run_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        \n        # Create test cases\n        test_cases = [\n            TestCase(\n                id=uuid.uuid4().hex,\n                question=\"Test question 1\",\n                expected_answer=\"Expected answer 1\",\n                source_chunk_ids=[\n                    uuid.uuid4().hex, \n                    uuid.uuid4().hex\n                ],\n            ),\n            TestCase(\n                id=uuid.uuid4().hex,\n                question=\"Test question 2\",\n                expected_answer=\"Expected answer 2\",\n                source_chunk_ids=[\n                    uuid.uuid4().hex\n                ],\n            ),\n        ]\n        \n        # Create dataset\n        dataset = Dataset(\n            id=uuid.uuid4().hex,\n            notebook_id=\"notebook123\",\n            name=\"Test Dataset\",\n            test_cases=tuple(test_cases),\n            status=EvaluationStatus.COMPLETED,\n            created_at=datetime.datetime.now(),\n            completed_at=datetime.datetime.now(),\n        )\n        \n        mock_dataset_repo.find_by_id.return_value = dataset\n        \n        # Mock retrieval results\n        def mock_retrieve(notebook_id, query, max_chunks):\n            return [MagicMock() for _ in range(2)]\n            \n        mock_retrieval.retrieve.side_effect = mock_retrieve\n        \n        # Mock run save\n        saved_run = None\n        def save_run(run):\n            nonlocal saved_run\n            saved_run = run\n            return run\n            \n        mock_run_repo.save.side_effect = save_run\n        \n        handler = RunEvaluationHandler(\n            dataset_repository=mock_dataset_repo,\n            run_repository=mock_run_repo,\n            retrieval_service=mock_retrieval,\n        )\n        \n        cmd = RunEvaluation(\n            metrics=[MetricType.PRECISION, MetricType.RECALL],\n            k_values=[1, 3, 5],\n            name=\"Test Run\",\n        )\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(dataset.id, cmd))\n\n        # Assert\n        mock_dataset_repo.find_by_id.assert_called_once_with(dataset.id)\n        assert mock_retrieval.retrieve.call_count == len(test_cases)\n        mock_run_repo.save.assert_called_once()\n        \n        assert result.id is not None\n        assert result.dataset_id == dataset.id\n        assert result.name == \"Test Run\"\n        assert len(result.test_results) == len(test_cases)\n        assert result.metrics is not None\n\n    def test_run_evaluation_dataset_not_found(self):\n        \"\"\"Test running evaluation on non-existent dataset raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_run_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        \n        mock_dataset_repo.find_by_id.return_value = None\n        \n        handler = RunEvaluationHandler(\n            dataset_repository=mock_dataset_repo,\n            run_repository=mock_run_repo,\n            retrieval_service=mock_retrieval,\n        )\n        \n        cmd = RunEvaluation(\n            metrics=[MetricType.PRECISION, MetricType.RECALL],\n            k_values=[1, 3, 5],\n            name=\"Test Run\",\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_run_evaluation_dataset_not_ready(self):\n        \"\"\"Test running evaluation on non-ready dataset raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_run_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        \n        # Create dataset with PENDING status\n        dataset = Dataset(\n            id=uuid.uuid4().hex,\n            notebook_id=\"notebook123\",\n            name=\"Test Dataset\",\n            test_cases=(),\n            status=EvaluationStatus.PENDING,\n            created_at=datetime.datetime.now(),\n            completed_at=None,\n        )\n        \n        mock_dataset_repo.find_by_id.return_value = dataset\n        \n        handler = RunEvaluationHandler(\n            dataset_repository=mock_dataset_repo,\n            run_repository=mock_run_repo,\n            retrieval_service=mock_retrieval,\n        )\n        \n        cmd = RunEvaluation(\n            metrics=[MetricType.PRECISION, MetricType.RECALL],\n            k_values=[1, 3, 5],\n            name=\"Test Run\",\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.ValidationError):\n            pytest.asyncio.run(handler.handle(dataset.id, cmd))\n\n\nclass TestGetRunHandler:\n    \"\"\"Tests for GetRunHandler.\"\"\"\n\n    def test_get_run_success(self):\n        \"\"\"Test successful run retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        run_id = uuid.uuid4().hex\n        \n        # Create run with test results\n        test_results = [\n            TestResult(\n                test_case_id=uuid.uuid4().hex,\n                retrieved_chunk_ids=[uuid.uuid4().hex],\n                relevant_chunk_ids=[uuid.uuid4().hex],\n                metrics={\n                    \"precision@1\": 1.0,\n                    \"recall@1\": 0.5,\n                },\n            ),\n        ]\n        \n        run = Run(\n            id=run_id,\n            dataset_id=uuid.uuid4().hex,\n            name=\"Test Run\",\n            metrics={\n                \"avg_precision@1\": 1.0,\n                \"avg_recall@1\": 0.5,\n            },\n            test_results=tuple(test_results),\n            created_at=datetime.datetime.now(),\n        )\n        \n        mock_repo.find_by_id.return_value = run\n        handler = GetRunHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(run_id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(run_id)\n        assert result.id == run_id\n        assert result.name == \"Test Run\"\n        assert len(result.test_results) == len(test_results)\n        assert result.metrics is not None\n\n    def test_get_run_not_found(self):\n        \"\"\"Test run not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetRunHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the evaluation handler module\n2. Verify tests for all happy path scenarios\n3. Verify tests for all error cases (not found, invalid state, no chunks)\n4. Confirm that tests verify metric calculations are aggregated correctly\n5. Validate that tests properly mock repositories, generators, and retrieval service",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "8",
        "title": "Implement Query Handler Unit Tests",
        "description": "Add comprehensive unit tests for the QueryNotebookHandler with mock retrieval service and RAG agent.",
        "details": "Create tests/query/test_handlers.py with tests for the query handler:\n\n```python\n\"\"\"Tests for Query handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.notebook.domain.model import Notebook\nfrom src.query.handler.handlers import QueryNotebookHandler\nfrom src.query.schema.command import QueryNotebook\nfrom src.query.schema.response import QueryResponse\nfrom src.chunk.domain.model import RetrievedChunk\n\n\nclass TestQueryNotebookHandler:\n    \"\"\"Tests for QueryNotebookHandler.\"\"\"\n\n    def test_query_notebook_success(self):\n        \"\"\"Test successful notebook query with results.\"\"\"\n        # Arrange\n        mock_notebook_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        # Create mock retrieved chunks\n        retrieved_chunks = [\n            RetrievedChunk(\n                id=\"chunk1\", \n                document_id=\"doc1\", \n                document_title=\"Doc 1\",\n                content=\"Content 1\", \n                score=0.9,\n            ),\n            RetrievedChunk(\n                id=\"chunk2\", \n                document_id=\"doc2\", \n                document_title=\"Doc 2\",\n                content=\"Content 2\", \n                score=0.8,\n            ),\n        ]\n        mock_retrieval.retrieve.return_value = retrieved_chunks\n        \n        # Create mock RAG answer\n        mock_answer = MagicMock()\n        mock_answer.answer = \"This is the answer based on retrieved chunks.\"\n        mock_answer.citations = [MagicMock(), MagicMock()]\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = QueryNotebookHandler(\n            notebook_repository=mock_notebook_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = QueryNotebook(question=\"What is the meaning of life?\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_retrieval.retrieve.assert_called_once_with(\n            notebook_id=notebook.id,\n            query=cmd.question,\n            max_chunks=10,\n        )\n        mock_rag_agent.answer.assert_called_once_with(\n            question=cmd.question,\n            retrieved_chunks=retrieved_chunks,\n            conversation_history=[],\n        )\n        \n        assert isinstance(result, QueryResponse)\n        assert result.answer == mock_answer.answer\n        assert len(result.citations) == len(mock_answer.citations)\n\n    def test_query_notebook_not_found(self):\n        \"\"\"Test query with non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_notebook_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = QueryNotebookHandler(\n            notebook_repository=mock_notebook_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = QueryNotebook(question=\"What is the meaning of life?\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_query_notebook_no_results(self):\n        \"\"\"Test query with no retrieved results still gets an answer.\"\"\"\n        # Arrange\n        mock_notebook_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        # Return empty retrieval results\n        mock_retrieval.retrieve.return_value = []\n        \n        # Create mock RAG answer for no results case\n        mock_answer = MagicMock()\n        mock_answer.answer = \"I couldn't find relevant information in the notebook.\"\n        mock_answer.citations = []\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = QueryNotebookHandler(\n            notebook_repository=mock_notebook_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = QueryNotebook(question=\"What is the meaning of life?\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_retrieval.retrieve.assert_called_once()\n        mock_rag_agent.answer.assert_called_once_with(\n            question=cmd.question,\n            retrieved_chunks=[],  # Empty results\n            conversation_history=[],\n        )\n        \n        assert result.answer == mock_answer.answer\n        assert len(result.citations) == 0\n\n    def test_query_notebook_with_max_chunks_param(self):\n        \"\"\"Test query with custom max_chunks parameter.\"\"\"\n        # Arrange\n        mock_notebook_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        # Return some retrieval results\n        mock_retrieval.retrieve.return_value = [MagicMock()]\n        \n        # Create mock RAG answer\n        mock_answer = MagicMock()\n        mock_answer.answer = \"This is the answer.\"\n        mock_answer.citations = []\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = QueryNotebookHandler(\n            notebook_repository=mock_notebook_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = QueryNotebook(\n            question=\"What is the meaning of life?\",\n            max_chunks=5,  # Custom max_chunks\n        )\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_retrieval.retrieve.assert_called_once_with(\n            notebook_id=notebook.id,\n            query=cmd.question,\n            max_chunks=5,  # Should use the custom value\n        )\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the query handler module\n2. Verify test for the happy path (question -> retrieval -> RAG answer)\n3. Verify test for notebook not found error\n4. Verify test for empty retrieval results\n5. Validate that tests properly mock retrieval service and RAG agent\n6. Verify test for custom max_chunks parameter",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "9",
        "title": "Implement Retrieval Service Unit Tests",
        "description": "Add comprehensive unit tests for the RetrievalService which handles embedding generation, vector search, and document enrichment.",
        "details": "Create tests/query/test_retrieval.py with tests for the retrieval service:\n\n```python\n\"\"\"Tests for RetrievalService.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src.query.service.retrieval import RetrievalService\nfrom src.chunk.domain.model import Chunk, RetrievedChunk\nfrom src.document.domain.model import Document\n\n\nclass TestRetrievalService:\n    \"\"\"Tests for RetrievalService.\"\"\"\n\n    def test_retrieve_success(self):\n        \"\"\"Test successful retrieval flow.\"\"\"\n        # Arrange\n        mock_chunk_repo = AsyncMock()\n        mock_doc_repo = AsyncMock()\n        mock_embedding_provider = AsyncMock()\n        \n        # Mock embedding generation\n        query_embedding = [0.1, 0.2, 0.3]\n        mock_embedding_provider.generate.return_value = query_embedding\n        \n        # Mock vector search results\n        chunk1 = MagicMock(spec=Chunk)\n        chunk1.id = \"chunk1\"\n        chunk1.document_id = \"doc1\"\n        chunk1.content = \"Content 1\"\n        \n        chunk2 = MagicMock(spec=Chunk)\n        chunk2.id = \"chunk2\"\n        chunk2.document_id = \"doc2\"\n        chunk2.content = \"Content 2\"\n        \n        vector_search_results = [\n            (chunk1, 0.9),\n            (chunk2, 0.8),\n        ]\n        mock_chunk_repo.vector_search.return_value = vector_search_results\n        \n        # Mock document retrieval for enrichment\n        doc1 = MagicMock(spec=Document)\n        doc1.id = \"doc1\"\n        doc1.title = \"Document 1\"\n        \n        doc2 = MagicMock(spec=Document)\n        doc2.id = \"doc2\"\n        doc2.title = \"Document 2\"\n        \n        mock_doc_repo.find_by_ids.return_value = [doc1, doc2]\n        \n        service = RetrievalService(\n            chunk_repository=mock_chunk_repo,\n            document_repository=mock_doc_repo,\n            embedding_provider=mock_embedding_provider,\n        )\n\n        # Act\n        result = pytest.asyncio.run(service.retrieve(\n            notebook_id=\"notebook123\",\n            query=\"What is the meaning of life?\",\n            max_chunks=10,\n        ))\n\n        # Assert\n        mock_embedding_provider.generate.assert_called_once_with(\"What is the meaning of life?\")\n        mock_chunk_repo.vector_search.assert_called_once_with(\n            notebook_id=\"notebook123\",\n            embedding=query_embedding,\n            limit=10,\n        )\n        mock_doc_repo.find_by_ids.assert_called_once_with([\"doc1\", \"doc2\"])\n        \n        # Verify enriched results\n        assert len(result) == 2\n        assert isinstance(result[0], RetrievedChunk)\n        assert result[0].id == \"chunk1\"\n        assert result[0].document_id == \"doc1\"\n        assert result[0].document_title == \"Document 1\"\n        assert result[0].content == \"Content 1\"\n        assert result[0].score == 0.9\n        \n        assert isinstance(result[1], RetrievedChunk)\n        assert result[1].id == \"chunk2\"\n        assert result[1].document_id == \"doc2\"\n        assert result[1].document_title == \"Document 2\"\n        assert result[1].content == \"Content 2\"\n        assert result[1].score == 0.8\n\n    def test_retrieve_no_results(self):\n        \"\"\"Test retrieval with no search results.\"\"\"\n        # Arrange\n        mock_chunk_repo = AsyncMock()\n        mock_doc_repo = AsyncMock()\n        mock_embedding_provider = AsyncMock()\n        \n        # Mock embedding generation\n        query_embedding = [0.1, 0.2, 0.3]\n        mock_embedding_provider.generate.return_value = query_embedding\n        \n        # Mock empty vector search results\n        mock_chunk_repo.vector_search.return_value = []\n        \n        service = RetrievalService(\n            chunk_repository=mock_chunk_repo,\n            document_repository=mock_doc_repo,\n            embedding_provider=mock_embedding_provider,\n        )\n\n        # Act\n        result = pytest.asyncio.run(service.retrieve(\n            notebook_id=\"notebook123\",\n            query=\"What is the meaning of life?\",\n            max_chunks=10,\n        ))\n\n        # Assert\n        mock_embedding_provider.generate.assert_called_once()\n        mock_chunk_repo.vector_search.assert_called_once()\n        mock_doc_repo.find_by_ids.assert_not_called()  # No documents to find\n        \n        # Verify empty results\n        assert len(result) == 0\n\n    def test_retrieve_missing_documents(self):\n        \"\"\"Test retrieval with missing documents during enrichment.\"\"\"\n        # Arrange\n        mock_chunk_repo = AsyncMock()\n        mock_doc_repo = AsyncMock()\n        mock_embedding_provider = AsyncMock()\n        \n        # Mock embedding generation\n        query_embedding = [0.1, 0.2, 0.3]\n        mock_embedding_provider.generate.return_value = query_embedding\n        \n        # Mock vector search results\n        chunk1 = MagicMock(spec=Chunk)\n        chunk1.id = \"chunk1\"\n        chunk1.document_id = \"doc1\"\n        chunk1.content = \"Content 1\"\n        \n        chunk2 = MagicMock(spec=Chunk)\n        chunk2.id = \"chunk2\"\n        chunk2.document_id = \"doc2\"\n        chunk2.content = \"Content 2\"\n        \n        vector_search_results = [\n            (chunk1, 0.9),\n            (chunk2, 0.8),\n        ]\n        mock_chunk_repo.vector_search.return_value = vector_search_results\n        \n        # Mock document retrieval with missing document\n        doc1 = MagicMock(spec=Document)\n        doc1.id = \"doc1\"\n        doc1.title = \"Document 1\"\n        \n        # doc2 is missing\n        mock_doc_repo.find_by_ids.return_value = [doc1]\n        \n        service = RetrievalService(\n            chunk_repository=mock_chunk_repo,\n            document_repository=mock_doc_repo,\n            embedding_provider=mock_embedding_provider,\n        )\n\n        # Act\n        result = pytest.asyncio.run(service.retrieve(\n            notebook_id=\"notebook123\",\n            query=\"What is the meaning of life?\",\n            max_chunks=10,\n        ))\n\n        # Assert\n        mock_doc_repo.find_by_ids.assert_called_once_with([\"doc1\", \"doc2\"])\n        \n        # Verify enriched results - should have doc1 info but use fallback for doc2\n        assert len(result) == 2\n        assert result[0].document_title == \"Document 1\"  # From doc1\n        assert result[1].document_title is None  # Fallback for missing doc2\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the retrieval service\n2. Verify test for happy path (query -> embedding -> search -> enrichment)\n3. Verify test for empty results\n4. Verify test for missing documents in enrichment\n5. Validate that tests properly mock embedding provider and repositories\n6. Check that immutable models are correctly used",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "10",
        "title": "Implement Integration Tests for Core Workflows",
        "description": "Add integration tests that test the full workflow from API to database for core features using testcontainers for PostgreSQL.",
        "details": "1. Create tests/integration/conftest.py with PostgreSQL testcontainer setup:\n\n```python\n\"\"\"Integration test fixtures.\"\"\"\n\nimport asyncio\nimport pytest\nimport os\nfrom typing import AsyncGenerator\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker\nimport alembic.config\nfrom testcontainers.postgres import PostgresContainer\n\nfrom src.database import Base\n\n\n@pytest.fixture(scope=\"session\")\ndef postgres_container():\n    \"\"\"Start a PostgreSQL container with pgvector extension.\"\"\"\n    postgres = PostgresContainer(\n        \"ankane/pgvector:latest\",\n        port=5432,\n        user=\"postgres\",\n        password=\"postgres\",\n    )\n    postgres.start()\n    \n    # Set environment variables for connection\n    os.environ[\"DATABASE_URL\"] = postgres.get_connection_url().replace(\"psycopg2\", \"postgresql+asyncpg\")\n    \n    yield postgres\n    postgres.stop()\n\n\n@pytest.fixture(scope=\"session\")\ndef event_loop(postgres_container):\n    \"\"\"Create an event loop for the session.\"\"\"\n    loop = asyncio.new_event_loop()\n    yield loop\n    loop.close()\n\n\n@pytest.fixture(scope=\"session\")\nasync def integration_engine(postgres_container):\n    \"\"\"Create database engine for integration tests.\"\"\"\n    # Create engine from environment variable\n    engine = create_async_engine(os.environ[\"DATABASE_URL\"], echo=False)\n    \n    # Run migrations\n    alembic_args = [\"--raiseerr\", \"upgrade\", \"head\"]\n    alembic.config.main(argv=alembic_args)\n    \n    return engine\n\n\n@pytest.fixture(scope=\"session\")\nasync def integration_session_factory(integration_engine):\n    \"\"\"Create session factory for integration tests.\"\"\"\n    return async_sessionmaker(integration_engine, expire_on_commit=False)\n\n\n@pytest.fixture\nasync def integration_session(\n    integration_session_factory\n) -> AsyncGenerator[AsyncSession, None]:\n    \"\"\"Create a new database session for a test.\"\"\"\n    async with integration_session_factory() as session:\n        yield session\n        await session.rollback()\n```\n\n2. Create integration test for notebook flow (create, add source, query):\n\n```python\n\"\"\"Integration test for notebook -> source -> query workflow.\"\"\"\n\nimport pytest\nimport uuid\nimport asyncio\nfrom fastapi.testclient import TestClient\n\nfrom src.main import app\n\n\n@pytest.mark.integration\ndef test_notebook_source_query_flow(integration_session):\n    \"\"\"Test end-to-end flow: create notebook -> add source -> query.\"\"\"\n    client = TestClient(app)\n    \n    # Step 1: Create notebook\n    notebook_name = f\"Test Notebook {uuid.uuid4().hex[:8]}\"\n    create_response = client.post(\n        \"/api/v1/notebooks\",\n        json={\"name\": notebook_name},\n    )\n    assert create_response.status_code == 201\n    notebook_id = create_response.json()[\"id\"]\n    \n    # Step 2: Add source (use a small, stable example URL)\n    source_url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n    add_source_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/sources\",\n        json={\"url\": source_url, \"title\": \"Python Wikipedia\"},\n    )\n    assert add_source_response.status_code == 201\n    document_id = add_source_response.json()[\"id\"]\n    \n    # Wait for background processing (in real tests, we'd mock this)\n    # This is just for illustration - in practice we'd need a more robust approach\n    asyncio.sleep(2)  \n    \n    # Step 3: Query the notebook\n    query_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/query\",\n        json={\"question\": \"What is Python?\"},\n    )\n    assert query_response.status_code == 200\n    result = query_response.json()\n    assert \"answer\" in result\n    assert isinstance(result[\"answer\"], str)\n    assert len(result[\"answer\"]) > 0\n    \n    # Step 4: Verify source can be deleted\n    delete_response = client.delete(\n        f\"/api/v1/notebooks/{notebook_id}/sources/{document_id}\",\n    )\n    assert delete_response.status_code == 204\n    \n    # Verify document is gone\n    get_doc_response = client.get(f\"/api/v1/documents/{document_id}\")\n    assert get_doc_response.status_code == 404\n```\n\n3. Create integration test for conversation flow:\n\n```python\n\"\"\"Integration test for conversation workflow.\"\"\"\n\nimport pytest\nimport uuid\nimport asyncio\nfrom fastapi.testclient import TestClient\n\nfrom src.main import app\n\n\n@pytest.mark.integration\ndef test_conversation_flow(integration_session):\n    \"\"\"Test end-to-end flow: create notebook -> create conversation -> send messages.\"\"\"\n    client = TestClient(app)\n    \n    # Step 1: Create notebook\n    notebook_name = f\"Test Notebook {uuid.uuid4().hex[:8]}\"\n    create_notebook_response = client.post(\n        \"/api/v1/notebooks\",\n        json={\"name\": notebook_name},\n    )\n    assert create_notebook_response.status_code == 201\n    notebook_id = create_notebook_response.json()[\"id\"]\n    \n    # Step 2: Create conversation\n    conversation_title = f\"Test Conversation {uuid.uuid4().hex[:8]}\"\n    create_conversation_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/conversations\",\n        json={\"title\": conversation_title},\n    )\n    assert create_conversation_response.status_code == 201\n    conversation_id = create_conversation_response.json()[\"id\"]\n    \n    # Step 3: Send message to conversation\n    message_content = \"Hello, how are you?\"\n    send_message_response = client.post(\n        f\"/api/v1/conversations/{conversation_id}/messages\",\n        json={\"content\": message_content},\n    )\n    assert send_message_response.status_code == 200\n    result = send_message_response.json()\n    assert \"content\" in result\n    assert isinstance(result[\"content\"], str)\n    assert len(result[\"content\"]) > 0\n    \n    # Step 4: Get conversation to verify message was saved\n    get_conversation_response = client.get(\n        f\"/api/v1/conversations/{conversation_id}\",\n    )\n    assert get_conversation_response.status_code == 200\n    conversation = get_conversation_response.json()\n    assert conversation[\"title\"] == conversation_title\n    assert len(conversation[\"messages\"]) >= 2  # User message + AI response\n    \n    # Verify user message content\n    user_messages = [msg for msg in conversation[\"messages\"] if msg[\"role\"] == \"user\"]\n    assert any(msg[\"content\"] == message_content for msg in user_messages)\n```\n\n4. Create integration test for evaluation flow:\n\n```python\n\"\"\"Integration test for evaluation workflow.\"\"\"\n\nimport pytest\nimport uuid\nimport asyncio\nfrom fastapi.testclient import TestClient\n\nfrom src.main import app\nfrom src.evaluation.domain.metric import MetricType\n\n\n@pytest.mark.integration\ndef test_evaluation_flow(integration_session):\n    \"\"\"Test end-to-end flow: create notebook -> add source -> generate dataset -> run evaluation.\"\"\"\n    client = TestClient(app)\n    \n    # Step 1: Create notebook\n    notebook_name = f\"Test Notebook {uuid.uuid4().hex[:8]}\"\n    create_response = client.post(\n        \"/api/v1/notebooks\",\n        json={\"name\": notebook_name},\n    )\n    assert create_response.status_code == 201\n    notebook_id = create_response.json()[\"id\"]\n    \n    # Step 2: Add source (use a small, stable example URL)\n    source_url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n    add_source_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/sources\",\n        json={\"url\": source_url, \"title\": \"Python Wikipedia\"},\n    )\n    assert add_source_response.status_code == 201\n    \n    # Wait for background processing (in real tests, we'd mock this)\n    asyncio.sleep(2)  \n    \n    # Step 3: Generate evaluation dataset\n    dataset_name = f\"Test Dataset {uuid.uuid4().hex[:8]}\"\n    generate_dataset_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/evaluation/datasets\",\n        json={\"name\": dataset_name, \"num_cases\": 3},  # Small number for test\n    )\n    assert generate_dataset_response.status_code == 201\n    dataset_id = generate_dataset_response.json()[\"id\"]\n    \n    # Wait for dataset generation (in real tests, we'd mock this)\n    # This is just for illustration - in practice we'd need a more robust approach\n    # like polling the dataset status until it's ready\n    asyncio.sleep(5)  \n    \n    # Step 4: Run evaluation\n    run_name = f\"Test Run {uuid.uuid4().hex[:8]}\"\n    run_evaluation_response = client.post(\n        f\"/api/v1/evaluation/datasets/{dataset_id}/runs\",\n        json={\n            \"name\": run_name,\n            \"metrics\": [MetricType.PRECISION.value, MetricType.RECALL.value],\n            \"k_values\": [1, 3, 5],\n        },\n    )\n    assert run_evaluation_response.status_code == 201\n    run_id = run_evaluation_response.json()[\"id\"]\n    \n    # Step 5: Get evaluation run\n    get_run_response = client.get(f\"/api/v1/evaluation/runs/{run_id}\")\n    assert get_run_response.status_code == 200\n    run = get_run_response.json()\n    assert run[\"name\"] == run_name\n    assert run[\"dataset_id\"] == dataset_id\n    assert \"metrics\" in run\n    assert run[\"test_results\"] is not None\n```",
        "testStrategy": "1. Set up a CI pipeline that runs these integration tests using containerized PostgreSQL\n2. Verify notebook CRUD -> add source -> query flow works end-to-end\n3. Verify conversation create -> send message flow works end-to-end\n4. Verify evaluation dataset generation -> run evaluation flow works end-to-end\n5. Ensure tests use testcontainers.postgresql for PostgreSQL\n6. Tests should be marked with @pytest.mark.integration to allow separate running\n7. Ensure these tests are clearly separated from unit tests",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "11",
        "title": "Implement Chunk Handler and API Tests",
        "description": "Add comprehensive unit tests for chunk handlers and chunk-related API endpoints.",
        "details": "1. Create tests/chunk/test_handlers.py with tests for the chunk handlers:\n\n```python\n\"\"\"Tests for Chunk handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.chunk.domain.model import Chunk\nfrom src.chunk.handler.handlers import GetChunkHandler, ListChunksByDocumentHandler\n\n\nclass TestGetChunkHandler:\n    \"\"\"Tests for GetChunkHandler.\"\"\"\n\n    def test_get_chunk_success(self):\n        \"\"\"Test successful chunk retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        chunk = Chunk(\n            id=\"chunk123\",\n            document_id=\"doc123\",\n            notebook_id=\"notebook123\",\n            content=\"Chunk content\",\n            embedding=None,\n            metadata={},\n        )\n        mock_repo.find_by_id.return_value = chunk\n        handler = GetChunkHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(chunk.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(chunk.id)\n        assert result.id == chunk.id\n        assert result.document_id == chunk.document_id\n        assert result.content == chunk.content\n\n    def test_get_chunk_not_found(self):\n        \"\"\"Test chunk not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetChunkHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListChunksByDocumentHandler:\n    \"\"\"Tests for ListChunksByDocumentHandler.\"\"\"\n\n    def test_list_chunks_success(self):\n        \"\"\"Test successful chunk listing.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        chunks = [\n            Chunk(\n                id=\"chunk1\",\n                document_id=\"doc123\",\n                notebook_id=\"notebook123\",\n                content=\"Chunk 1 content\",\n                embedding=None,\n                metadata={},\n            ),\n            Chunk(\n                id=\"chunk2\",\n                document_id=\"doc123\",\n                notebook_id=\"notebook123\",\n                content=\"Chunk 2 content\",\n                embedding=None,\n                metadata={},\n            ),\n        ]\n        mock_repo.list_by_document.return_value = chunks\n        handler = ListChunksByDocumentHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(\"doc123\"))\n\n        # Assert\n        mock_repo.list_by_document.assert_called_once_with(\"doc123\")\n        assert len(result) == 2\n        assert result[0].id == \"chunk1\"\n        assert result[1].id == \"chunk2\"\n        assert result[0].content == \"Chunk 1 content\"\n        assert result[1].content == \"Chunk 2 content\"\n\n    def test_list_chunks_empty(self):\n        \"\"\"Test listing chunks for document with no chunks.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.list_by_document.return_value = []\n        handler = ListChunksByDocumentHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(\"doc123\"))\n\n        # Assert\n        mock_repo.list_by_document.assert_called_once_with(\"doc123\")\n        assert len(result) == 0\n```\n\n2. Create tests/chunk/test_api.py with tests for chunk API endpoints:\n\n```python\n\"\"\"Tests for Chunk API endpoints.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch\nfrom fastapi.testclient import TestClient\n\nfrom src.main import app\nfrom src.chunk.domain.model import Chunk\nfrom src.chunk.handler.handlers import GetChunkHandler, ListChunksByDocumentHandler\n\n\n@patch(\"src.dependency.container.ApplicationContainer.chunk.handler.get_chunk_handler\")\ndef test_get_chunk(mock_handler, client=TestClient(app)):\n    \"\"\"Test GET /chunks/{chunk_id} endpoint.\"\"\"\n    # Arrange\n    chunk_id = \"chunk123\"\n    mock_get_chunk_handler = AsyncMock()\n    mock_handler.return_value = mock_get_chunk_handler\n    \n    from src.chunk.schema.response import ChunkDetail\n    mock_response = ChunkDetail(\n        id=chunk_id,\n        document_id=\"doc123\",\n        notebook_id=\"notebook123\",\n        content=\"Chunk content\",\n        metadata={},\n    )\n    mock_get_chunk_handler.handle.return_value = mock_response\n    \n    # Act\n    response = client.get(f\"/api/v1/chunks/{chunk_id}\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"id\"] == chunk_id\n    assert data[\"content\"] == \"Chunk content\"\n    mock_get_chunk_handler.handle.assert_called_once_with(chunk_id)\n\n\n@patch(\"src.dependency.container.ApplicationContainer.chunk.handler.get_chunk_handler\")\ndef test_get_chunk_not_found(mock_handler, client=TestClient(app)):\n    \"\"\"Test GET /chunks/{chunk_id} with non-existent chunk.\"\"\"\n    # Arrange\n    chunk_id = \"nonexistent\"\n    mock_get_chunk_handler = AsyncMock()\n    mock_handler.return_value = mock_get_chunk_handler\n    \n    from src import exceptions\n    mock_get_chunk_handler.handle.side_effect = exceptions.NotFoundError(f\"Chunk not found: {chunk_id}\")\n    \n    # Act\n    response = client.get(f\"/api/v1/chunks/{chunk_id}\")\n    \n    # Assert\n    assert response.status_code == 404\n    mock_get_chunk_handler.handle.assert_called_once_with(chunk_id)\n\n\n@patch(\"src.dependency.container.ApplicationContainer.chunk.handler.list_chunks_by_document_handler\")\ndef test_list_chunks_by_document(mock_handler, client=TestClient(app)):\n    \"\"\"Test GET /documents/{document_id}/chunks endpoint.\"\"\"\n    # Arrange\n    document_id = \"doc123\"\n    mock_list_handler = AsyncMock()\n    mock_handler.return_value = mock_list_handler\n    \n    from src.chunk.schema.response import ChunkDetail\n    mock_response = [\n        ChunkDetail(\n            id=\"chunk1\",\n            document_id=document_id,\n            notebook_id=\"notebook123\",\n            content=\"Chunk 1 content\",\n            metadata={},\n        ),\n        ChunkDetail(\n            id=\"chunk2\",\n            document_id=document_id,\n            notebook_id=\"notebook123\",\n            content=\"Chunk 2 content\",\n            metadata={},\n        ),\n    ]\n    mock_list_handler.handle.return_value = mock_response\n    \n    # Act\n    response = client.get(f\"/api/v1/documents/{document_id}/chunks\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert len(data) == 2\n    assert data[0][\"id\"] == \"chunk1\"\n    assert data[1][\"id\"] == \"chunk2\"\n    assert data[0][\"content\"] == \"Chunk 1 content\"\n    assert data[1][\"content\"] == \"Chunk 2 content\"\n    mock_list_handler.handle.assert_called_once_with(document_id)\n```\n\n3. Create tests/chunk/test_repository.py with tests for chunk repository operations with embeddings:\n\n```python\n\"\"\"Tests for Chunk repository.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src.chunk.adapter.repository import ChunkRepository\nfrom src.chunk.domain.model import Chunk\n\n\ndef test_save_chunk_with_embedding():\n    \"\"\"Test saving chunk with embedding.\"\"\"\n    # Arrange\n    mock_session = AsyncMock()\n    repository = ChunkRepository(mock_session)\n    \n    # Create chunk with embedding\n    chunk = Chunk(\n        id=\"chunk123\",\n        document_id=\"doc123\",\n        notebook_id=\"notebook123\",\n        content=\"Chunk content\",\n        embedding=[0.1, 0.2, 0.3],  # Embedding vector\n        metadata={},\n    )\n    \n    # Mock add and commit methods\n    mock_session.add = MagicMock()\n    \n    # Act\n    result = pytest.asyncio.run(repository.save(chunk))\n    \n    # Assert\n    mock_session.add.assert_called_once()\n    mock_session.commit.assert_called_once()\n    assert result == chunk  # Repository returns the saved chunk\n\n\ndef test_vector_search():\n    \"\"\"Test vector similarity search.\"\"\"\n    # This test would require a more complex setup with a mock for execute\n    # and the SQLAlchemy query results, which is beyond the scope of this example\n    # In a real test, we'd mock the database response for vector search\n    pass\n\n\ndef test_delete_by_document():\n    \"\"\"Test deleting chunks by document ID.\"\"\"\n    # Arrange\n    mock_session = AsyncMock()\n    repository = ChunkRepository(mock_session)\n    document_id = \"doc123\"\n    \n    # Mock execute method to return delete count\n    mock_result = MagicMock()\n    mock_result.rowcount = 5  # 5 chunks deleted\n    mock_session.execute.return_value = mock_result\n    \n    # Act\n    delete_count = pytest.asyncio.run(repository.delete_by_document(document_id))\n    \n    # Assert\n    mock_session.execute.assert_called_once()\n    mock_session.commit.assert_called_once()\n    assert delete_count == 5  # Repository returns number of deleted chunks\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 80% coverage for the chunk handler module\n2. Verify tests for listing chunks by document\n3. Verify tests for searching similar chunks\n4. Verify tests for chunk with embedding operations\n5. Ensure API tests mock the handlers correctly and validate response formats\n6. Test repository operations with embeddings",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "12",
        "title": "Implement CLI Command Tests",
        "description": "Add tests for CLI commands to ensure they correctly invoke handlers and format output.",
        "details": "Create tests/cli/test_commands.py with tests for CLI commands:\n\n```python\n\"\"\"Tests for CLI commands.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\nfrom typer.testing import CliRunner\n\nfrom src.cli.commands.notebook import app as notebook_app\nfrom src.cli.commands.conversation import app as conversation_app\nfrom src.cli.commands.query import app as query_app\nfrom src.notebook.domain.model import Notebook\nfrom src.document.domain.model import Document\nfrom src.conversation.domain.model import Conversation, Message, MessageRole\n\n\nrunner = CliRunner()\n\n\n@pytest.fixture\ndef mock_session_context():\n    \"\"\"Mock session context for CLI tests.\"\"\"\n    mock_session = AsyncMock()\n    context_manager = MagicMock()\n    context_manager.__aenter__.return_value = mock_session\n    context_manager.__aexit__.return_value = None\n    \n    with patch(\"src.cli.utils.get_session_context\") as mock_get_session:\n        mock_get_session.return_value = context_manager\n        yield mock_session\n\n\nclass TestNotebookCommands:\n    \"\"\"Tests for notebook CLI commands.\"\"\"\n\n    def test_create_notebook(self, mock_session_context):\n        \"\"\"Test notebook create command.\"\"\"\n        # Arrange\n        notebook_name = \"Test Notebook\"\n        notebook_description = \"Test description\"\n        \n        # Mock repository save method\n        from src.notebook.adapter.repository import NotebookRepository\n        with patch.object(NotebookRepository, \"save\", new_callable=AsyncMock) as mock_save:\n            mock_save.side_effect = lambda notebook: notebook\n            \n            # Act\n            result = runner.invoke(notebook_app, [\n                \"create\", \n                notebook_name, \n                \"--description\", notebook_description\n            ])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"Notebook created:\" in result.stdout\n            assert notebook_name in result.stdout\n            assert notebook_description in result.stdout\n            mock_save.assert_called_once()\n            saved_notebook = mock_save.call_args[0][0]\n            assert saved_notebook.name == notebook_name\n            assert saved_notebook.description == notebook_description\n\n    def test_list_notebooks(self, mock_session_context):\n        \"\"\"Test notebook list command.\"\"\"\n        # Arrange\n        notebooks = [\n            Notebook.create(name=\"Notebook 1\"),\n            Notebook.create(name=\"Notebook 2\"),\n        ]\n        \n        # Mock repository list method\n        from src.notebook.adapter.repository import NotebookRepository\n        with patch.object(NotebookRepository, \"list\", new_callable=AsyncMock) as mock_list:\n            from src.common import PaginatedResult\n            mock_result = PaginatedResult(\n                items=notebooks,\n                total=len(notebooks),\n                page=1,\n                size=10,\n            )\n            mock_list.return_value = mock_result\n            \n            # Act\n            result = runner.invoke(notebook_app, [\"list\"])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"Notebook 1\" in result.stdout\n            assert \"Notebook 2\" in result.stdout\n            mock_list.assert_called_once()\n\n\nclass TestSourceCommands:\n    \"\"\"Tests for source CLI commands.\"\"\"\n\n    def test_add_source(self, mock_session_context):\n        \"\"\"Test source add command.\"\"\"\n        # Arrange\n        notebook_id = \"notebook123\"\n        source_url = \"https://example.com\"\n        source_title = \"Example Website\"\n        \n        # Mock repositories\n        from src.notebook.adapter.repository import NotebookRepository\n        from src.document.adapter.repository import DocumentRepository\n        \n        with patch.object(NotebookRepository, \"find_by_id\", new_callable=AsyncMock) as mock_find_notebook, \\\n             patch.object(DocumentRepository, \"find_by_notebook_and_url\", new_callable=AsyncMock) as mock_find_doc, \\\n             patch.object(DocumentRepository, \"save\", new_callable=AsyncMock) as mock_save_doc:\n            \n            # Mock notebook exists\n            notebook = Notebook.create(name=\"Test Notebook\")\n            mock_find_notebook.return_value = notebook\n            \n            # Mock document doesn't exist yet\n            mock_find_doc.return_value = None\n            \n            # Mock document save\n            doc = Document.create(notebook_id=notebook_id, url=source_url, title=source_title)\n            mock_save_doc.return_value = doc\n            \n            # Act\n            result = runner.invoke(notebook_app, [\n                \"source\", \"add\", \n                notebook_id, \n                source_url,\n                \"--title\", source_title\n            ])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"Source added:\" in result.stdout\n            assert doc.id in result.stdout\n            assert source_title in result.stdout\n            mock_find_notebook.assert_called_once_with(notebook_id)\n            mock_find_doc.assert_called_once()\n            mock_save_doc.assert_called_once()\n\n    def test_list_sources(self, mock_session_context):\n        \"\"\"Test source list command.\"\"\"\n        # Arrange\n        notebook_id = \"notebook123\"\n        documents = [\n            Document.create(notebook_id=notebook_id, url=\"https://example1.com\", title=\"Example 1\"),\n            Document.create(notebook_id=notebook_id, url=\"https://example2.com\", title=\"Example 2\"),\n        ]\n        \n        # Mock repositories\n        from src.notebook.adapter.repository import NotebookRepository\n        from src.document.adapter.repository import DocumentRepository\n        \n        with patch.object(NotebookRepository, \"find_by_id\", new_callable=AsyncMock) as mock_find_notebook, \\\n             patch.object(DocumentRepository, \"list_by_notebook\", new_callable=AsyncMock) as mock_list_docs:\n            \n            # Mock notebook exists\n            notebook = Notebook.create(name=\"Test Notebook\")\n            mock_find_notebook.return_value = notebook\n            \n            # Mock document listing\n            from src.common import PaginatedResult\n            mock_result = PaginatedResult(\n                items=documents,\n                total=len(documents),\n                page=1,\n                size=10,\n            )\n            mock_list_docs.return_value = mock_result\n            \n            # Act\n            result = runner.invoke(notebook_app, [\"source\", \"list\", notebook_id])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"Example 1\" in result.stdout\n            assert \"Example 2\" in result.stdout\n            assert \"https://example1.com\" in result.stdout\n            assert \"https://example2.com\" in result.stdout\n            mock_find_notebook.assert_called_once_with(notebook_id)\n            mock_list_docs.assert_called_once()\n\n\nclass TestQueryCommands:\n    \"\"\"Tests for query CLI commands.\"\"\"\n\n    def test_query_notebook(self, mock_session_context):\n        \"\"\"Test query notebook command.\"\"\"\n        # Arrange\n        notebook_id = \"notebook123\"\n        query_text = \"What is the meaning of life?\"\n        \n        # Mock repositories and services\n        from src.notebook.adapter.repository import NotebookRepository\n        from src.chunk.adapter.repository import ChunkRepository\n        from src.document.adapter.repository import DocumentRepository\n        from src.query.adapter.pydantic_ai.agent import RAGAgent\n        \n        with patch.object(NotebookRepository, \"find_by_id\", new_callable=AsyncMock) as mock_find_notebook, \\\n             patch.object(ChunkRepository, \"vector_search\", new_callable=AsyncMock) as mock_vector_search, \\\n             patch.object(DocumentRepository, \"find_by_ids\", new_callable=AsyncMock) as mock_find_docs, \\\n             patch.object(RAGAgent, \"answer\", new_callable=AsyncMock) as mock_answer:\n            \n            # Mock notebook exists\n            notebook = Notebook.create(name=\"Test Notebook\")\n            mock_find_notebook.return_value = notebook\n            \n            # Mock vector search\n            chunk1 = MagicMock()\n            chunk1.id = \"chunk1\"\n            chunk1.document_id = \"doc1\"\n            chunk1.content = \"Content 1\"\n            \n            chunk2 = MagicMock()\n            chunk2.id = \"chunk2\"\n            chunk2.document_id = \"doc2\"\n            chunk2.content = \"Content 2\"\n            \n            mock_vector_search.return_value = [(chunk1, 0.9), (chunk2, 0.8)]\n            \n            # Mock document retrieval\n            doc1 = MagicMock()\n            doc1.id = \"doc1\"\n            doc1.title = \"Document 1\"\n            \n            doc2 = MagicMock()\n            doc2.id = \"doc2\"\n            doc2.title = \"Document 2\"\n            \n            mock_find_docs.return_value = [doc1, doc2]\n            \n            # Mock RAG answer\n            from src.query.schema.response import QueryResponse, Citation\n            mock_response = QueryResponse(\n                answer=\"This is the answer to your question.\",\n                citations=[\n                    Citation(\n                        citation_index=1,\n                        chunk_id=\"chunk1\",\n                        document_id=\"doc1\",\n                        document_title=\"Document 1\",\n                        text_snippet=\"Content 1\",\n                    )\n                ],\n            )\n            mock_answer.return_value = mock_response\n            \n            # Act\n            result = runner.invoke(query_app, [\"notebook\", notebook_id, query_text])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"This is the answer to your question.\" in result.stdout\n            assert \"Document 1\" in result.stdout  # Citation should be shown\n            mock_find_notebook.assert_called_once_with(notebook_id)\n            mock_vector_search.assert_called_once()\n            mock_find_docs.assert_called_once()\n            mock_answer.assert_called_once()\n\n\nclass TestEvaluationCommands:\n    \"\"\"Tests for evaluation CLI commands.\"\"\"\n    \n    # Add tests for evaluation CLI commands here\n    # Following the same pattern as above\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure all CLI commands have test coverage\n2. Verify tests for notebook CLI commands (create, list)\n3. Verify tests for source CLI commands (add, list, delete)\n4. Verify tests for query CLI command\n5. Verify tests for evaluation CLI commands\n6. Use typer.testing.CliRunner to invoke CLI commands\n7. Mock database session and repositories to test command logic without database access",
        "priority": "low",
        "dependencies": [
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "13",
        "title": "Implement Crawl Domain Model",
        "description": "Create the core domain entities for URL crawling feature: CrawlJob as the aggregate root to track crawling operations, CrawlStatus enum to represent the state of a crawl job, and DiscoveredUrl value object to represent each discovered URL and its status.",
        "details": "Implement the following domain models:\n\n1. CrawlStatus enum:\n   - Values: PENDING, IN_PROGRESS, COMPLETED, FAILED, CANCELLED\n   - Methods for state transition validation:\n     - is_processable (PENDING  IN_PROGRESS)\n     - is_terminal (COMPLETED, FAILED)\n     - can_cancel (PENDING, IN_PROGRESS)\n\n2. DiscoveredUrlStatus enum:\n   - Values: PENDING, INGESTED, SKIPPED, FAILED\n   - Method to check if URL can be processed\n\n3. DiscoveredUrl value object:\n   - Immutable (frozen pydantic BaseModel)\n   - Fields: url, depth, status, document_id (optional)\n   - Factory method for creation\n   - State transition methods:\n     - mark_ingested(document_id)  new instance with INGESTED status\n     - mark_skipped()  new instance with SKIPPED status\n     - mark_failed(error)  new instance with FAILED status\n\n4. CrawlJob entity:\n   - Immutable (frozen pydantic BaseModel)\n   - Fields as per PRD: id, notebook_id, seed_url, domain, max_depth, max_pages, url_include_pattern, url_exclude_pattern, status, total_discovered, total_ingested, error_message, created_at, updated_at\n   - Factory method: create(notebook_id, seed_url, max_depth, max_pages, url_include_pattern, url_exclude_pattern)\n   - Domain validation (max_depth > 0, max_pages > 0)\n   - State transition methods:\n     - mark_in_progress()  new instance with IN_PROGRESS status\n     - mark_completed()  new instance with COMPLETED status\n     - mark_failed(error_message)  new instance with FAILED status\n     - mark_cancelled()  new instance with CANCELLED status\n     - increment_discovered()  new instance with total_discovered + 1\n     - increment_ingested()  new instance with total_ingested + 1\n\nFollow the existing pattern of immutable domain entities with state transition methods seen in Document.",
        "testStrategy": "1. Unit tests for all domain entities:\n   - Test enum state transitions and validations\n   - Test CrawlJob factory method creates valid instances\n   - Test CrawlJob domain validation rejects invalid inputs (max_depth < 1, max_pages < 1)\n   - Test DiscoveredUrl state transitions produce new instances with correct state\n   - Test CrawlJob state transitions produce new instances with correct state\n   - Test increment methods correctly update counters\n   - Test validation of URL patterns\n   - Test equality for value objects\n\n2. Use Arrange-Act-Assert pattern in tests:\n   - Arrange: Create initial entity state\n   - Act: Call state transition method\n   - Assert: Verify new state and immutability of original\n\n3. Example test case:\n   def test_crawl_job_can_be_marked_in_progress():\n       # Arrange\n       job = CrawlJob.create(notebook_id=\"123\", seed_url=\"https://example.com\")\n       \n       # Act\n       updated_job = job.mark_in_progress()\n       \n       # Assert\n       assert job.status == CrawlStatus.PENDING  # Original unchanged\n       assert updated_job.status == CrawlStatus.IN_PROGRESS",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:39:43.627Z"
      },
      {
        "id": "14",
        "title": "Implement Crawl Persistence",
        "description": "Create the database schema (ORM models) and repository for persisting CrawlJob entities and DiscoveredUrl value objects, following the same patterns as existing DocumentRepository.",
        "details": "1. Create SQLAlchemy ORM models in `src/infrastructure/models/crawl.py`:\n   - CrawlJobSchema:\n     - id: String(32), PK\n     - notebook_id: String(32), FKnotebooks.id CASCADE\n     - seed_url: Text\n     - domain: String(255)\n     - max_depth: Integer\n     - max_pages: Integer\n     - url_include_pattern: Text, nullable\n     - url_exclude_pattern: Text, nullable\n     - status: String(20), indexed\n     - total_discovered: Integer, default 0\n     - total_ingested: Integer, default 0\n     - error_message: Text, nullable\n     - created_at, updated_at: DateTime(timezone=True)\n\n   - DiscoveredUrlSchema:\n     - id: String(32), PK\n     - crawl_job_id: String(32), FKcrawl_jobs.id CASCADE\n     - url: Text\n     - depth: Integer\n     - status: String(20)\n     - document_id: String(32), FKdocuments.id, nullable\n     - error_message: Text, nullable\n     - created_at, updated_at: DateTime(timezone=True)\n     - Unique constraint: (crawl_job_id, url)\n\n2. Implement CrawlJobMapper in `src/crawl/domain/mapper.py`:\n   - to_entity(record)  CrawlJob\n   - to_record(entity)  CrawlJobSchema\n\n3. Implement DiscoveredUrlMapper:\n   - to_entity(record)  DiscoveredUrl\n   - to_record(entity)  DiscoveredUrlSchema\n\n4. Implement CrawlJobRepository in `src/crawl/adapter/repository.py`:\n   - find_by_id(id)  CrawlJob | None\n   - find_by_notebook_id_and_url(notebook_id, url)  CrawlJob | None\n   - save(entity)  CrawlJob\n   - save_batch(entities)  list[CrawlJob]\n   - update_discovered_url(crawl_id, url, update_data)  DiscoveredUrl\n   - list_by_notebook(notebook_id, query)  PaginationSchema[CrawlJob]\n   - find_discovered_urls(crawl_id, status=None)  list[DiscoveredUrl]\n   - delete(id)  bool\n\n5. Create Alembic migration script for the new tables\n\nFollowing the SQLAlchemy patterns used in the existing codebase, including:\n- Using UUIDs for primary keys\n- Foreign key relationships with CASCADE delete\n- Indexes on commonly queried fields\n- Proper timestamp handling\n- Support for pagination",
        "testStrategy": "1. Unit tests for repository operations:\n   - Test CRUD operations for CrawlJob and DiscoveredUrl\n   - Test find operations with various filters\n   - Test pagination for list operations\n   - Test cascading deletes (when a crawl job is deleted, all its discovered URLs are deleted)\n   - Test unique constraint on (crawl_job_id, url)\n\n2. Test mappers:\n   - Test bidirectional mapping (entityrecordentity)\n   - Test handling of all field types and nullable fields\n\n3. Test database constraints:\n   - Test foreign key constraints\n   - Test unique constraints\n   - Test default values\n\n4. Example test:\n   async def test_find_crawl_job_by_id():\n       # Arrange\n       job = CrawlJob.create(notebook_id=\"123\", seed_url=\"https://example.com\")\n       saved = await repository.save(job)\n       \n       # Act\n       retrieved = await repository.find_by_id(saved.id)\n       \n       # Assert\n       assert retrieved is not None\n       assert retrieved.id == saved.id\n       assert retrieved.notebook_id == \"123\"",
        "priority": "high",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:42:15.716Z"
      },
      {
        "id": "15",
        "title": "Implement Link Discovery Service",
        "description": "Create a service that fetches a web page and extracts all internal links (same domain). This service will handle URL discovery, normalization, and filtering based on domain and patterns.",
        "details": "1. Create a DiscoveredLink value object in `src/crawl/domain/model.py`:\n   - Immutable (frozen pydantic BaseModel)\n   - Fields: url (str), anchor_text (str | None)\n\n2. Implement LinkDiscoveryService in `src/crawl/service/link_discovery.py`:\n   - Method: discover_links(url: str, domain: str)  list[DiscoveredLink]\n   - Implementation:\n     a. Use existing ContentExtractorPort to fetch the page content or use httpx directly\n     b. Parse HTML to extract all <a href=\"...\"> tags and their text content\n     c. Normalize URLs (convert relative to absolute, remove fragments)\n     d. Filter to same domain only (match against base domain)\n     e. Return list of DiscoveredLink objects\n\n3. Add pattern filtering to LinkDiscoveryService:\n   - Method: filter_urls(links: list[DiscoveredLink], include_pattern: str | None, exclude_pattern: str | None)  list[DiscoveredLink]\n   - Implementation:\n     a. If include_pattern is provided, only keep URLs matching the regex pattern\n     b. If exclude_pattern is provided, exclude URLs matching the regex pattern\n     c. Return filtered list of links\n\n4. Add domain extraction utility:\n   - Method: extract_domain(url: str)  str\n   - Implementation: Parse URL and extract domain for scope limiting\n\n5. Add URL normalization utility:\n   - Method: normalize_url(url: str, base_url: str)  str\n   - Implementation:\n     a. Resolve relative URLs to absolute using base URL\n     b. Remove fragments (#section) from URLs\n     c. Optionally remove query parameters\n     d. Ensure protocol and domain are included",
        "testStrategy": "1. Unit tests for LinkDiscoveryService:\n   - Test link extraction from sample HTML content\n   - Test URL normalization (relativeabsolute, fragment removal)\n   - Test domain filtering (same domain only)\n   - Test pattern filtering (include/exclude regex patterns)\n\n2. Test with various HTML structures:\n   - Regular <a href> links\n   - Relative URLs (same path, root relative, etc.)\n   - Links with fragments and query parameters\n   - Malformed URLs\n   - Non-HTTP schemes (mailto:, javascript:, etc.)\n\n3. Test domain extraction utility:\n   - Test with various URL formats\n   - Test subdomains (should match parent domain)\n   - Test URLs with and without www prefix\n\n4. Test with mock ContentExtractorPort:\n   - Mock HTTP responses to test integration without network calls\n   - Test error handling for failed requests\n\n5. Example test:\n   async def test_extract_internal_links():\n       # Arrange\n       html = \"\"\"\n       <html><body>\n         <a href=\"https://example.com/page1\">Page 1</a>\n         <a href=\"/page2\">Page 2</a>\n         <a href=\"https://other-site.com/page3\">External</a>\n       </body></html>\n       \"\"\"\n       service = LinkDiscoveryService()\n       mock_extractor = Mock()\n       mock_extractor.extract.return_value = html\n       service._extractor = mock_extractor\n       \n       # Act\n       links = await service.discover_links(\"https://example.com\", \"example.com\")\n       \n       # Assert\n       assert len(links) == 2  # Only internal links\n       assert any(link.url == \"https://example.com/page1\" for link in links)\n       assert any(link.url == \"https://example.com/page2\" for link in links)",
        "priority": "medium",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:44:01.011Z"
      },
      {
        "id": "16",
        "title": "Implement Crawl Execution Service",
        "description": "Create the core crawl orchestration service that performs BFS traversal of pages, discovers links, and creates documents for each discovered URL, coordinating with the existing ingestion pipeline.",
        "details": "1. Implement CrawlService in `src/crawl/service/crawl_service.py`:\n   - Fields:\n     - _document_repository: DocumentRepository\n     - _crawl_repository: CrawlJobRepository\n     - _link_discovery: LinkDiscoveryService\n     - _ingestion_service: BackgroundIngestionService\n\n   - Method: `async def execute(self, crawl_job_id: str)  CrawlJob`:\n     - Implementation:\n       a. Load the crawl job from repository\n       b. Use BFS algorithm to traverse pages starting from seed URL:\n          - Queue = [(seed_url, 0)] (URL and depth)\n          - While queue is not empty and within limits:\n            * Dequeue (url, depth)\n            * Skip if already visited or depth > max_depth\n            * Add to visited URLs\n            * Discover links on the page\n            * Filter links by domain and patterns\n            * For each link: add to queue if not visited, depth+1 <= max_depth\n            * Create document and trigger ingestion\n       c. Update CrawlJob status and counters\n       d. Return updated CrawlJob\n\n   - Method: `async def _create_document(self, notebook_id: str, url: str, crawl_job_id: str, depth: int)  str | None`:\n     - Create Document entity\n     - Save to repository\n     - Trigger background ingestion\n     - Update DiscoveredUrl status and document_id\n     - Return document ID if successful\n\n   - Method: `async def _process_url(self, url: str, crawl_job: CrawlJob, depth: int, visited: set[str])  None`:\n     - Process a single URL\n     - Create document, trigger ingestion\n     - Update crawl job counters\n     - Discover and process new links if depth < max_depth\n\n2. Implement BackgroundCrawlService in `src/crawl/service/background_crawl_service.py`:\n   - Similar to BackgroundIngestionService, but for crawling\n   - Track active crawl tasks\n   - Start crawl jobs in the background\n   - Method: `def trigger_crawl(self, crawl_job: CrawlJob)  None`:\n     - Start an asyncio task for the crawl\n   - Method: `async def _process_with_cleanup(self, crawl_job_id: str)  None`:\n     - Process the crawl and clean up afterward\n\n3. Add deduplication checking:\n   - Check if URL already exists in the notebook (reuse find_by_notebook_and_url)\n   - Skip if already exists to avoid duplicate ingestion",
        "testStrategy": "1. Unit tests for CrawlService:\n   - Test BFS traversal algorithm with mock repositories and services\n   - Test depth limiting and max pages limiting\n   - Test URL pattern filtering\n   - Test error handling and recovery (individual page failures)\n   - Test document creation and integration with ingestion service\n   - Test deduplication of URLs\n\n2. Test BackgroundCrawlService:\n   - Test task creation and tracking\n   - Test error handling in background tasks\n   - Test cleanup after task completion or failure\n\n3. Integration tests:\n   - Test end-to-end crawl process with mock HTTP responses\n   - Test integration with document ingestion pipeline\n   - Test concurrent crawls for different notebooks\n\n4. Edge case testing:\n   - Test with cycles in the link graph\n   - Test with very large numbers of links\n   - Test with rate limiting considerations\n   - Test recovery from temporary failures\n\n5. Example test:\n   async def test_crawl_respects_depth_limit():\n       # Arrange\n       crawl_job = CrawlJob.create(\n           notebook_id=\"123\",\n           seed_url=\"https://example.com\",\n           max_depth=2,\n           max_pages=10\n       )\n       mock_repo = Mock()\n       mock_repo.find_by_id.return_value = crawl_job\n       mock_link_discovery = Mock()\n       mock_link_discovery.discover_links.side_effect = [\n           # Seed URL (depth 0) has 2 links\n           [DiscoveredLink(url=\"https://example.com/page1\"), \n            DiscoveredLink(url=\"https://example.com/page2\")],\n           # Page1 (depth 1) has 2 links\n           [DiscoveredLink(url=\"https://example.com/page1-1\"), \n            DiscoveredLink(url=\"https://example.com/page1-2\")],\n           # Page2 (depth 1) has 2 links\n           [DiscoveredLink(url=\"https://example.com/page2-1\"), \n            DiscoveredLink(url=\"https://example.com/page2-2\")],\n           # The rest are at depth 2, should not be crawled\n           [], [], [], []\n       ]\n       service = CrawlService(\n           document_repository=Mock(),\n           crawl_repository=mock_repo,\n           link_discovery=mock_link_discovery,\n           ingestion_service=Mock()\n       )\n       \n       # Act\n       await service.execute(\"crawl123\")\n       \n       # Assert\n       # Should only create documents for seed + 4 pages (2 at depth 1 + their 2 pages each at depth 2)\n       assert service._document_repository.save.call_count == 5\n       # Should only discover links from 3 pages (seed + 2 at depth 1)\n       assert mock_link_discovery.discover_links.call_count == 3",
        "priority": "high",
        "dependencies": [
          "13",
          "14",
          "15"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:45:46.231Z"
      },
      {
        "id": "17",
        "title": "Implement Crawl Command Handlers",
        "description": "Create the command handlers, schemas (command/response/query), following the existing DDD handler pattern for crawl operations like starting a crawl, getting status, and cancelling a crawl.",
        "details": "1. Create schema definitions in `src/crawl/schema/`:\n   \n   a. Command schemas (command.py):\n      - StartCrawl:\n        - url: pydantic.HttpUrl (seed URL)\n        - max_depth: int = 2\n        - max_pages: int = 50\n        - url_include_pattern: str | None = None\n        - url_exclude_pattern: str | None = None\n      - CancelCrawl: (empty, ID from path)\n   \n   b. Response schemas (response.py):\n      - CrawlJobId:\n        - id: str\n      - DiscoveredUrlDetail:\n        - url: str\n        - depth: int\n        - status: str\n        - document_id: str | None\n      - CrawlJobDetail:\n        - id, notebook_id, seed_url, domain\n        - max_depth, max_pages\n        - url_include_pattern, url_exclude_pattern\n        - status, total_discovered, total_ingested\n        - error_message\n        - created_at, updated_at\n        - discovered_urls: list[DiscoveredUrlDetail] (optional)\n   \n   c. Query schemas (query.py):\n      - ListCrawlJobs(pagination.ListQuery):\n        - notebook_id: str\n\n2. Implement handlers in `src/crawl/handler/handlers.py`:\n   \n   a. StartCrawlHandler:\n      - Dependencies: notebook_repository, crawl_repository, background_crawl_service\n      - Method: async def handle(self, notebook_id: str, cmd: command.StartCrawl)  response.CrawlJobId\n      - Implementation:\n        - Validate notebook exists\n        - Extract domain from URL\n        - Create CrawlJob in PENDING status\n        - Save to repository\n        - Trigger background crawl\n        - Return CrawlJobId\n   \n   b. GetCrawlJobHandler:\n      - Dependencies: crawl_repository\n      - Method: async def handle(self, crawl_job_id: str, include_urls: bool = False)  response.CrawlJobDetail\n      - Implementation:\n        - Find crawl job by ID\n        - If include_urls, load discovered URLs\n        - Return CrawlJobDetail\n   \n   c. ListCrawlJobsHandler:\n      - Dependencies: notebook_repository, crawl_repository\n      - Method: async def handle(self, qry: query.ListCrawlJobs)  pagination.PaginationSchema[response.CrawlJobDetail]\n      - Implementation:\n        - Validate notebook exists\n        - List crawl jobs with pagination\n        - Return paginated response\n   \n   d. CancelCrawlHandler:\n      - Dependencies: crawl_repository, background_crawl_service\n      - Method: async def handle(self, crawl_job_id: str)  None\n      - Implementation:\n        - Find crawl job by ID\n        - If in processable state, mark as CANCELLED\n        - Try to cancel background task\n        - Return None (HTTP 204)",
        "testStrategy": "1. Unit tests for command handlers:\n   - Test StartCrawlHandler creates valid crawl job and triggers background task\n   - Test GetCrawlJobHandler returns correct detail response\n   - Test ListCrawlJobsHandler returns paginated results\n   - Test CancelCrawlHandler cancels a running crawl\n\n2. Test validation logic:\n   - Test notebook existence validation\n   - Test URL validation\n   - Test invalid parameters (negative max_depth, etc.)\n   - Test error responses for not found resources\n\n3. Test with mock repositories and services:\n   - Use dependency injection for testability\n   - Mock repository responses\n   - Verify correct repository method calls\n   - Test state transitions\n\n4. Example test:\n   async def test_start_crawl_creates_job_and_triggers_background_process():\n       # Arrange\n       notebook_repo = Mock()\n       notebook_repo.find_by_id.return_value = Notebook(id=\"notebook123\", name=\"Test\")\n       \n       crawl_repo = Mock()\n       crawl_repo.save.side_effect = lambda job: job\n       \n       background_service = Mock()\n       \n       handler = StartCrawlHandler(\n           notebook_repository=notebook_repo,\n           crawl_repository=crawl_repo,\n           background_crawl_service=background_service\n       )\n       \n       cmd = command.StartCrawl(\n           url=\"https://example.com\",\n           max_depth=3,\n           max_pages=100\n       )\n       \n       # Act\n       result = await handler.handle(\"notebook123\", cmd)\n       \n       # Assert\n       assert result is not None\n       assert result.id is not None\n       assert crawl_repo.save.call_count == 1\n       saved_job = crawl_repo.save.call_args[0][0]\n       assert saved_job.notebook_id == \"notebook123\"\n       assert saved_job.seed_url == \"https://example.com\"\n       assert saved_job.max_depth == 3\n       assert saved_job.max_pages == 100\n       assert saved_job.status == CrawlStatus.PENDING\n       assert background_service.trigger_crawl.call_count == 1",
        "priority": "medium",
        "dependencies": [
          "13",
          "14",
          "16"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:47:35.516Z"
      },
      {
        "id": "18",
        "title": "Implement Crawl API Endpoints",
        "description": "Create FastAPI API endpoints for the crawl feature, following existing patterns for dependency injection, response models, and error handling.",
        "details": "1. Create API endpoints in `src/crawl/entrypoint/api.py`:\n   \n   a. POST /api/v1/notebooks/{notebook_id}/crawl\n      - Summary: \"Start crawling URLs from a seed URL\"\n      - Request body: StartCrawl schema\n      - Response: CrawlJobId\n      - Status codes: 201 Created, 404 Not Found, 422 Validation Error\n\n   b. GET /api/v1/notebooks/{notebook_id}/crawl\n      - Summary: \"List crawl jobs for a notebook\"\n      - Query parameters: page, size\n      - Response: PaginationSchema[CrawlJobDetail]\n      - Status codes: 200 OK, 404 Not Found\n\n   c. GET /api/v1/crawl/{crawl_job_id}\n      - Summary: \"Get crawl job details\"\n      - Query parameters: include_urls (bool, default=False)\n      - Response: CrawlJobDetail\n      - Status codes: 200 OK, 404 Not Found\n\n   d. POST /api/v1/crawl/{crawl_job_id}/cancel\n      - Summary: \"Cancel a crawl job\"\n      - Response: None (204 No Content)\n      - Status codes: 204 No Content, 404 Not Found\n\n2. Add API documentation:\n   - Add detailed descriptions for all endpoints\n   - Document all response models\n   - Document error responses\n\n3. Add FastAPI dependencies in `src/crawl/dependency.py`:\n   - Define dependencies for handlers\n   - Use dependency injection pattern\n\n4. Register routers in the main FastAPI app:\n   - Update `src/__main__.py` to include crawl router\n\n5. Add proper error handling:\n   - Convert domain exceptions to HTTP responses\n   - Provide meaningful error messages",
        "testStrategy": "1. Test API endpoints with pytest and async test client:\n   - Test each endpoint's happy path\n   - Test error responses for invalid inputs\n   - Test not found responses\n   - Test validation errors\n\n2. Test dependency injection:\n   - Verify handlers are correctly instantiated with dependencies\n   - Test FastAPI dependency overrides for testing\n\n3. Test HTTP status codes:\n   - 201 Created for successful creation\n   - 200 OK for successful retrieval\n   - 204 No Content for successful cancellation\n   - 404 Not Found for missing resources\n   - 422 Validation Error for invalid inputs\n\n4. Test request validation:\n   - Test URL format validation\n   - Test numeric parameter constraints\n   - Test required vs. optional fields\n\n5. Example test:\n   async def test_start_crawl_endpoint():\n       # Arrange\n       app = FastAPI()\n       app.include_router(crawl_router)\n       client = AsyncClient(app=app, base_url=\"http://test\")\n       \n       # Override dependencies\n       mock_handler = Mock()\n       mock_handler.handle.return_value = CrawlJobId(id=\"crawl123\")\n       app.dependency_overrides[get_start_crawl_handler] = lambda: mock_handler\n       \n       # Act\n       response = await client.post(\n           \"/api/v1/notebooks/notebook123/crawl\",\n           json={\"url\": \"https://example.com\", \"max_depth\": 2, \"max_pages\": 50}\n       )\n       \n       # Assert\n       assert response.status_code == 201\n       assert response.json() == {\"id\": \"crawl123\"}\n       mock_handler.handle.assert_called_once()\n       call_args = mock_handler.handle.call_args\n       assert call_args[0][0] == \"notebook123\"  # notebook_id\n       assert call_args[0][1].url == \"https://example.com\"",
        "priority": "medium",
        "dependencies": [
          "17"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:49:12.996Z"
      },
      {
        "id": "19",
        "title": "Implement Crawl CLI Commands",
        "description": "Add CLI commands for the crawl feature using Typer, including commands to start, monitor, list, and cancel crawls.",
        "details": "1. Create CLI commands in `src/cli/commands/crawl.py`:\n   \n   a. Command: `ntlm source crawl <notebook_id> <url> [--depth N] [--max-pages N] [--include PATTERN] [--exclude PATTERN]`\n      - Implementation:\n        - Create StartCrawl command\n        - Call StartCrawlHandler\n        - Display crawl job ID and monitoring instructions\n        - Optional: Track progress in real-time\n\n   b. Command: `ntlm crawl status <crawl_job_id>`\n      - Implementation:\n        - Call GetCrawlJobHandler with include_urls=True\n        - Display crawl status, progress, and discovered URLs\n        - Format output with rich tables and styling\n\n   c. Command: `ntlm crawl list <notebook_id>`\n      - Implementation:\n        - Call ListCrawlJobsHandler\n        - Display list of crawl jobs with status and progress\n        - Format as rich table\n\n   d. Command: `ntlm crawl cancel <crawl_job_id>`\n      - Implementation:\n        - Call CancelCrawlHandler\n        - Display confirmation message\n        - Handle errors (not found, already completed)\n\n2. Add rich terminal output:\n   - Use rich progress bars for status display\n   - Use rich tables for listing crawl jobs and URLs\n   - Use colored output for different statuses\n\n3. Add error handling:\n   - Handle common errors (not found, validation)\n   - Display user-friendly error messages\n   - Add retry options for transient errors\n\n4. Register CLI commands in main Typer app:\n   - Update `src/cli/__init__.py` to include crawl commands",
        "testStrategy": "1. Test CLI commands with Typer testing utilities:\n   - Test command invocation with various arguments\n   - Test output formatting\n   - Test error handling and messages\n\n2. Test integration with handlers:\n   - Verify correct handler calls\n   - Test data passing between CLI and handlers\n   - Mock handler responses for testing\n\n3. Test user experience:\n   - Test progress display updates\n   - Test formatting of tables and status info\n   - Test error messages are user-friendly\n\n4. Example test:\n   def test_crawl_status_command():\n       # Arrange\n       runner = CliRunner()\n       mock_handler = Mock()\n       mock_handler.handle.return_value = CrawlJobDetail(\n           id=\"crawl123\",\n           notebook_id=\"notebook456\",\n           seed_url=\"https://example.com\",\n           status=\"completed\",\n           total_discovered=10,\n           total_ingested=8,\n           # ... other fields\n       )\n       \n       # Patch dependency to return mock handler\n       with patch(\"src.cli.commands.crawl.get_handler\", return_value=mock_handler):\n           # Act\n           result = runner.invoke(app, [\"crawl\", \"status\", \"crawl123\"])\n           \n           # Assert\n           assert result.exit_code == 0\n           assert \"Crawl Status: completed\" in result.stdout\n           assert \"Total Discovered: 10\" in result.stdout\n           assert \"Total Ingested: 8\" in result.stdout",
        "priority": "low",
        "dependencies": [
          "17"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:49:13.610Z"
      },
      {
        "id": "20",
        "title": "Implement Dependency Injection Container and Integration",
        "description": "Create the dependency injection container for the crawl module and integrate it into the main application, wiring all components together.",
        "details": "1. Create dependency container in `src/crawl/dependency.py`:\n   \n   a. Define container with dependencies:\n   ```python\n   from dependency_injector import containers, providers\n   \n   class CrawlContainer(containers.DeclarativeContainer):\n       \"\"\"Crawl module dependency container.\"\"\"\n       \n       config = providers.Configuration()\n       db = providers.Dependency()\n       document_container = providers.Dependency()\n       \n       # Repositories\n       crawl_repository = providers.Factory(\n           CrawlJobRepository,\n           session=db.provided.session,\n       )\n       \n       # Services\n       link_discovery_service = providers.Factory(\n           LinkDiscoveryService,\n           content_extractor=document_container.content_extractor,\n       )\n       \n       crawl_service = providers.Factory(\n           CrawlService,\n           document_repository=document_container.document_repository,\n           crawl_repository=crawl_repository,\n           link_discovery=link_discovery_service,\n           ingestion_service=document_container.background_ingestion_service,\n       )\n       \n       background_crawl_service = providers.Singleton(\n           BackgroundCrawlService,\n           crawl_service=crawl_service,\n       )\n       \n       # Handlers\n       start_crawl_handler = providers.Factory(\n           StartCrawlHandler,\n           notebook_repository=document_container.notebook_repository,\n           crawl_repository=crawl_repository,\n           background_crawl_service=background_crawl_service,\n       )\n       \n       get_crawl_job_handler = providers.Factory(\n           GetCrawlJobHandler,\n           crawl_repository=crawl_repository,\n       )\n       \n       list_crawl_jobs_handler = providers.Factory(\n           ListCrawlJobsHandler,\n           notebook_repository=document_container.notebook_repository,\n           crawl_repository=crawl_repository,\n       )\n       \n       cancel_crawl_handler = providers.Factory(\n           CancelCrawlHandler,\n           crawl_repository=crawl_repository,\n           background_crawl_service=background_crawl_service,\n       )\n   ```\n\n2. Configure FastAPI dependencies:\n   ```python\n   def get_start_crawl_handler() -> StartCrawlHandler:\n       return crawl_container.start_crawl_handler()\n   \n   def get_get_crawl_job_handler() -> GetCrawlJobHandler:\n       return crawl_container.get_crawl_job_handler()\n   \n   def get_list_crawl_jobs_handler() -> ListCrawlJobsHandler:\n       return crawl_container.list_crawl_jobs_handler()\n   \n   def get_cancel_crawl_handler() -> CancelCrawlHandler:\n       return crawl_container.cancel_crawl_handler()\n   ```\n\n3. Integrate with main application container in `src/dependency/__init__.py`:\n   - Add CrawlContainer to main app container\n   - Wire dependencies between containers\n\n4. Initialize background crawl service with app lifecycle in `src/__main__.py`:\n   ```python\n   @app.on_event(\"startup\")\n   async def startup_crawl_service():\n       # Initialize crawl service\n   \n   @app.on_event(\"shutdown\")\n   async def shutdown_crawl_service():\n       # Cleanup any running crawl tasks\n   ```",
        "testStrategy": "1. Test container configuration:\n   - Test all dependencies are correctly wired\n   - Test container instantiation and resolution\n   - Test overriding dependencies for testing\n\n2. Test FastAPI dependency functions:\n   - Test dependency resolution in API endpoints\n   - Verify correct handlers are injected\n\n3. Test integration with app lifecycle:\n   - Test startup and shutdown events\n   - Verify background services are properly initialized\n   - Test cleanup on shutdown\n\n4. Test component interactions:\n   - Test end-to-end interaction between components\n   - Verify message passing between components\n   - Test correct dependency resolution for nested dependencies\n\n5. Example test:\n   def test_crawl_container_wiring():\n       # Arrange\n       container = CrawlContainer()\n       container.db = Mock()\n       container.document_container = Mock()\n       \n       # Configure mocks\n       db_session = Mock()\n       container.db.provided.session = db_session\n       container.document_container.content_extractor = Mock()\n       container.document_container.document_repository = Mock()\n       container.document_container.notebook_repository = Mock()\n       container.document_container.background_ingestion_service = Mock()\n       \n       # Act - resolve services and handlers\n       crawl_service = container.crawl_service()\n       start_handler = container.start_crawl_handler()\n       \n       # Assert\n       assert crawl_service._document_repository is container.document_container.document_repository\n       assert crawl_service._crawl_repository is not None\n       assert start_handler._notebook_repository is container.document_container.notebook_repository",
        "priority": "medium",
        "dependencies": [
          "16",
          "17",
          "18",
          "19"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:50:45.394Z"
      },
      {
        "id": "21",
        "title": "Add QuestionDifficulty enum and extend TestCase model",
        "description": "Create QuestionDifficulty StrEnum with values FACTUAL, ANALYTICAL, INFERENTIAL, PARAPHRASED and add optional difficulty field to TestCase entity",
        "details": "Add QuestionDifficulty StrEnum to src/evaluation/domain/model.py:\n\n```python\nclass QuestionDifficulty(enum.StrEnum):\n    \"\"\"Question difficulty classification.\"\"\"\n    FACTUAL = \"factual\"\n    ANALYTICAL = \"analytical\"\n    INFERENTIAL = \"inferential\"\n    PARAPHRASED = \"paraphrased\"\n```\n\nUpdate TestCase frozen model to include optional difficulty field with None as default:\n\n```python\nclass TestCase(pydantic.BaseModel):\n    model_config = pydantic.ConfigDict(frozen=True, extra=\"forbid\")\n    id: str\n    question: str\n    ground_truth_chunk_ids: tuple[str, ...]\n    source_chunk_id: str\n    difficulty: QuestionDifficulty | None = None\n    created_at: datetime.datetime\n```\n\nUpdate TestCase.create() factory method to accept optional difficulty parameter:\n\n```python\n@classmethod\ndef create(\n    cls,\n    question: str,\n    ground_truth_chunk_ids: tuple[str, ...],\n    source_chunk_id: str,\n    difficulty: QuestionDifficulty | None = None,\n) -> Self:\n    return cls(\n        id=uuid.uuid4().hex,\n        question=question,\n        ground_truth_chunk_ids=ground_truth_chunk_ids,\n        source_chunk_id=source_chunk_id,\n        difficulty=difficulty,\n        created_at=common_types.utc_now(),\n    )\n```",
        "testStrategy": "Unit tests in tests/evaluation/test_model.py verifying:\n1. QuestionDifficulty enum has all four values\n2. TestCase can be created with difficulty=None (backward compatibility)\n3. TestCase can be created with each QuestionDifficulty value\n4. TestCase.create() factory accepts optional difficulty parameter\n5. Frozen model immutability is maintained",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:04:06.521Z"
      },
      {
        "id": "22",
        "title": "Add difficulty column to evaluation_test_cases table",
        "description": "Create Alembic migration adding nullable difficulty column to evaluation_test_cases table and update ORM model",
        "details": "Create new Alembic migration file:\n\n```bash\nalembic revision -m \"add_difficulty_to_test_cases\"\n```\n\nIn the generated migration file, add:\n\n```python\ndef upgrade() -> None:\n    op.add_column(\n        'evaluation_test_cases',\n        sa.Column('difficulty', sa.String(20), nullable=True)\n    )\n\ndef downgrade() -> None:\n    op.drop_column('evaluation_test_cases', 'difficulty')\n```\n\nUpdate EvaluationTestCaseSchema in src/infrastructure/models/evaluation.py:\n\n```python\ndifficulty: sqlalchemy.orm.Mapped[str | None] = sqlalchemy.orm.mapped_column(\n    sqlalchemy.String(20), nullable=True\n)\n```\n\nUpdate DatasetMapper in src/evaluation/domain/mapper.py to handle difficulty serialization/deserialization:\n\n```python\n# In test_case_to_record:\ndifficulty=entity.difficulty.value if entity.difficulty else None\n\n# In _test_case_from_record:\ndifficulty=QuestionDifficulty(record.difficulty) if record.difficulty else None\n```",
        "testStrategy": "1. Run migration: alembic upgrade head\n2. Verify column exists in database schema with correct type and nullable constraint\n3. Test mapper round-trip: entity -> record -> entity preserves difficulty value\n4. Test backward compatibility: existing records with difficulty=NULL map correctly to None\n5. Integration test: save and load TestCase with difficulty through repository",
        "priority": "high",
        "dependencies": [
          "21"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:07:54.927Z"
      },
      {
        "id": "23",
        "title": "Update SyntheticTestGenerator for difficulty classification",
        "description": "Modify LLM prompts and parsing logic to generate and classify question difficulty",
        "details": "Update SYSTEM_PROMPT in src/evaluation/adapter/generator.py:\n\n```python\nSYSTEM_PROMPT = \"\"\"You are a test data generator for a retrieval evaluation system.\nYour task is to generate diverse, realistic questions that can be answered from the given passage.\n\nRules:\n- Questions must be self-contained (do not reference \"the passage\", \"the text\", \"the above\", etc.)\n- Do not generate yes/no questions\n- Generate diverse question types with varying difficulty:\n  * FACTUAL: Direct information recall from the passage\n  * ANALYTICAL: Requires analyzing or comparing information\n  * INFERENTIAL: Requires drawing conclusions beyond explicit text\n  * PARAPHRASED: Rewording of passage content\n- Questions should require information specifically from the passage to answer\n- Return valid JSON only\"\"\"\n```\n\nUpdate USER_PROMPT_TEMPLATE to request difficulty classification:\n\n```python\nUSER_PROMPT_TEMPLATE = \"\"\"Based on the following passage, generate exactly {count} questions that can be answered using the information in this passage.\n\nPassage:\n{content}\n\nReturn your response as a JSON object with this exact format:\n{{\"questions\": [{{\"text\": \"question 1\", \"difficulty\": \"factual\"}}, {{\"text\": \"question 2\", \"difficulty\": \"analytical\"}}, ...]}}\"\"\"\n```\n\nUpdate _parse_questions to extract difficulty:\n\n```python\ndef _parse_questions(self, output: str, expected_count: int) -> list[tuple[str, model.QuestionDifficulty | None]]:\n    try:\n        # ... existing JSON parsing ...\n        questions = data.get(\"questions\", [])\n        result = []\n        for q in questions:\n            if isinstance(q, dict):\n                text = q.get(\"text\", \"\")\n                diff_str = q.get(\"difficulty\")\n                difficulty = None\n                if diff_str:\n                    try:\n                        difficulty = model.QuestionDifficulty(diff_str.lower())\n                    except ValueError:\n                        logger.warning(\"Invalid difficulty: %s\", diff_str)\n                if text:\n                    result.append((text.strip(), difficulty))\n        return result\n    except (json.JSONDecodeError, AttributeError):\n        logger.warning(\"Failed to parse LLM output as JSON: %s\", output[:200])\n    return []\n```\n\nUpdate generate_test_cases to pass difficulty to TestCase.create():\n\n```python\nfor question_text, difficulty in questions:\n    test_case = model.TestCase.create(\n        question=question_text,\n        ground_truth_chunk_ids=(chunk.id,),\n        source_chunk_id=chunk.id,\n        difficulty=difficulty,\n    )\n    test_cases.append(test_case)\n```",
        "testStrategy": "1. Unit test _parse_questions with mock LLM output containing difficulty field\n2. Unit test _parse_questions handles missing difficulty gracefully (backward compatibility)\n3. Unit test _parse_questions handles invalid difficulty values\n4. Integration test: generate_test_cases produces TestCase entities with difficulty values\n5. Test all four QuestionDifficulty values are properly parsed",
        "priority": "high",
        "dependencies": [
          "21",
          "22"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:11:55.979Z"
      },
      {
        "id": "24",
        "title": "Add difficulty to response schemas",
        "description": "Extend TestCaseResponse and create DifficultyMetrics response models",
        "details": "Update TestCaseResponse in src/evaluation/schema/response.py:\n\n```python\nclass TestCaseResponse(pydantic.BaseModel):\n    id: str\n    question: str\n    ground_truth_chunk_ids: list[str]\n    source_chunk_id: str\n    difficulty: str | None  # String for API compatibility\n    created_at: datetime.datetime\n\n    @classmethod\n    def from_entity(cls, entity: model.TestCase) -> Self:\n        return cls(\n            id=entity.id,\n            question=entity.question,\n            ground_truth_chunk_ids=list(entity.ground_truth_chunk_ids),\n            source_chunk_id=entity.source_chunk_id,\n            difficulty=entity.difficulty.value if entity.difficulty else None,\n            created_at=entity.created_at,\n        )\n```\n\nAdd DifficultyMetrics response model:\n\n```python\nclass DifficultyMetrics(pydantic.BaseModel):\n    \"\"\"Per-difficulty aggregate metrics.\"\"\"\n    difficulty: str\n    test_case_count: int\n    precision_at_k: float\n    recall_at_k: float\n    hit_rate_at_k: float\n    mrr: float\n```\n\nExtend RunDetail to include metrics_by_difficulty:\n\n```python\nclass RunDetail(pydantic.BaseModel):\n    # ... existing fields ...\n    metrics_by_difficulty: list[DifficultyMetrics] | None = None\n```",
        "testStrategy": "1. Unit test TestCaseResponse.from_entity() with difficulty=None\n2. Unit test TestCaseResponse.from_entity() with each QuestionDifficulty value\n3. Unit test DifficultyMetrics model validation\n4. Verify JSON serialization produces correct format\n5. Test backward compatibility: responses without metrics_by_difficulty",
        "priority": "medium",
        "dependencies": [
          "21",
          "22",
          "23"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:14:00.796Z"
      },
      {
        "id": "25",
        "title": "Add difficulty breakdown to GetRunHandler",
        "description": "Compute per-difficulty metrics in GetRunHandler and populate RunDetail response",
        "details": "Update GetRunHandler in src/evaluation/handler/handlers.py:\n\n```python\nclass GetRunHandler:\n    def __init__(\n        self,\n        run_repository: evaluation_repository_module.RunRepository,\n        dataset_repository: evaluation_repository_module.DatasetRepository,\n    ) -> None:\n        self._run_repository = run_repository\n        self._dataset_repository = dataset_repository\n\n    async def handle(self, run_id: str) -> response.RunDetail:\n        run = await self._run_repository.find_by_id(run_id)\n        if run is None:\n            raise exceptions.NotFoundError(f\"Run not found: {run_id}\")\n        \n        # Load dataset to get test case difficulties\n        dataset = await self._dataset_repository.find_by_id(run.dataset_id)\n        if dataset is None:\n            raise exceptions.NotFoundError(f\"Dataset not found: {run.dataset_id}\")\n        \n        # Build difficulty map\n        difficulty_map = {tc.id: tc.difficulty for tc in dataset.test_cases}\n        \n        # Group results by difficulty\n        from collections import defaultdict\n        results_by_difficulty = defaultdict(list)\n        for result in run.results:\n            difficulty = difficulty_map.get(result.test_case_id)\n            if difficulty:\n                results_by_difficulty[difficulty].append(result)\n        \n        # Compute per-difficulty metrics\n        metrics_by_difficulty = []\n        for difficulty, results in results_by_difficulty.items():\n            precisions = [r.precision for r in results]\n            recalls = [r.recall for r in results]\n            hits = [r.hit for r in results]\n            reciprocal_ranks = [r.reciprocal_rank for r in results]\n            \n            mean_p, mean_r, hit_rate, mrr = metric_module.aggregate_metrics(\n                precisions, recalls, hits, reciprocal_ranks\n            )\n            \n            metrics_by_difficulty.append(\n                response.DifficultyMetrics(\n                    difficulty=difficulty.value,\n                    test_case_count=len(results),\n                    precision_at_k=mean_p,\n                    recall_at_k=mean_r,\n                    hit_rate_at_k=hit_rate,\n                    mrr=mrr,\n                )\n            )\n        \n        # Build RunDetail with difficulty breakdown\n        base_response = response.RunDetail.from_entity(run)\n        return base_response.model_copy(\n            update={\"metrics_by_difficulty\": metrics_by_difficulty or None}\n        )\n```\n\nUpdate EvaluationHandlerContainer in src/evaluation/dependency.py:\n\n```python\nget_run_handler = providers.Factory(\n    handlers.GetRunHandler,\n    run_repository=adapter.run_repository,\n    dataset_repository=adapter.dataset_repository,\n)\n```",
        "testStrategy": "1. Unit test GetRunHandler.handle() with test cases of varying difficulties\n2. Verify metrics_by_difficulty contains correct DifficultyMetrics for each difficulty\n3. Test with runs where test cases have no difficulty (backward compatibility)\n4. Integration test: full flow from API to handler to response\n5. Verify CLI results command displays difficulty breakdown correctly",
        "priority": "medium",
        "dependencies": [
          "24"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create DifficultyMetrics response model",
            "description": "Add DifficultyMetrics response model to src/evaluation/schema/response.py to represent per-difficulty aggregated metrics",
            "dependencies": [],
            "details": "Create a new Pydantic model class DifficultyMetrics in src/evaluation/schema/response.py with fields: difficulty (str), test_case_count (int), precision_at_k (float), recall_at_k (float), hit_rate_at_k (float), and mrr (float). This model will be used to encapsulate metrics for a specific difficulty level.",
            "status": "pending",
            "testStrategy": "Unit test DifficultyMetrics model validation: 1) Test instantiation with all required fields, 2) Test JSON serialization produces correct format, 3) Test field types are validated correctly",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Add metrics_by_difficulty field to RunDetail",
            "description": "Extend RunDetail response schema to include optional metrics_by_difficulty field",
            "dependencies": [
              1
            ],
            "details": "Update RunDetail class in src/evaluation/schema/response.py to add metrics_by_difficulty: list[DifficultyMetrics] | None = None field. This field will be None for runs without difficulty breakdown or for backward compatibility with existing runs.",
            "status": "pending",
            "testStrategy": "Unit test RunDetail.from_entity(): 1) Test with metrics_by_difficulty=None, 2) Test serialization produces correct JSON structure with new field",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add dataset_repository dependency to GetRunHandler",
            "description": "Update GetRunHandler constructor to accept DatasetRepository dependency",
            "dependencies": [],
            "details": "Modify GetRunHandler.__init__ in src/evaluation/handler/handlers.py to accept dataset_repository: evaluation_repository_module.DatasetRepository parameter and store it as self._dataset_repository. This is needed to load the dataset and access test case difficulties.",
            "status": "pending",
            "testStrategy": "Unit test GetRunHandler instantiation: 1) Test constructor accepts both run_repository and dataset_repository, 2) Test dependencies are stored correctly",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement difficulty map building logic",
            "description": "Add logic to GetRunHandler.handle() to load dataset and build difficulty map from test cases",
            "dependencies": [
              3
            ],
            "details": "In GetRunHandler.handle() method: 1) Load dataset using self._dataset_repository.find_by_id(run.dataset_id), 2) Raise NotFoundError if dataset is None, 3) Build difficulty_map as dict[str, QuestionDifficulty] = {tc.id: tc.difficulty for tc in dataset.test_cases} to map test case IDs to their difficulty levels.",
            "status": "pending",
            "testStrategy": "Unit test handle() method: 1) Test dataset is loaded correctly, 2) Test NotFoundError is raised when dataset not found, 3) Test difficulty_map is built correctly from test cases",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Group results by difficulty",
            "description": "Implement logic to group test case results by difficulty level",
            "dependencies": [
              4
            ],
            "details": "In GetRunHandler.handle(): 1) Import defaultdict from collections, 2) Create results_by_difficulty = defaultdict(list), 3) Iterate through run.results and group by difficulty: for result in run.results: difficulty = difficulty_map.get(result.test_case_id); if difficulty: results_by_difficulty[difficulty].append(result). This handles cases where test cases may not have difficulty set.",
            "status": "pending",
            "testStrategy": "Unit test grouping logic: 1) Test results are correctly grouped by difficulty, 2) Test results without difficulty are excluded, 3) Test empty results list, 4) Test mixed difficulties",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Compute per-difficulty metrics using aggregate_metrics",
            "description": "Calculate aggregated metrics for each difficulty level using the existing aggregate_metrics function",
            "dependencies": [
              5,
              1
            ],
            "details": "In GetRunHandler.handle(): For each difficulty group in results_by_difficulty: 1) Extract lists of precisions, recalls, hits, and reciprocal_ranks from results, 2) Call metric_module.aggregate_metrics(precisions, recalls, hits, reciprocal_ranks) to get mean_p, mean_r, hit_rate, mrr, 3) Create response.DifficultyMetrics instance with difficulty=difficulty.value, test_case_count=len(results), and computed metrics.",
            "status": "pending",
            "testStrategy": "Unit test metrics computation: 1) Test aggregate_metrics is called with correct parameters, 2) Test DifficultyMetrics objects are created with correct values, 3) Test multiple difficulty levels, 4) Test single difficulty level",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Populate RunDetail with difficulty breakdown",
            "description": "Update GetRunHandler.handle() to return RunDetail with metrics_by_difficulty populated",
            "dependencies": [
              6,
              2
            ],
            "details": "In GetRunHandler.handle(): 1) Collect all DifficultyMetrics objects into metrics_by_difficulty list, 2) Build base RunDetail using response.RunDetail.from_entity(run), 3) Return base_response.model_copy(update={\"metrics_by_difficulty\": metrics_by_difficulty or None}) to include the breakdown while maintaining other fields. Set to None if list is empty for backward compatibility.",
            "status": "pending",
            "testStrategy": "Integration test handle(): 1) Test RunDetail includes metrics_by_difficulty when run has results with difficulties, 2) Test metrics_by_difficulty is None when no difficulties are present, 3) Test all existing RunDetail fields are preserved",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Update EvaluationHandlerContainer dependency injection",
            "description": "Modify get_run_handler provider in dependency container to inject dataset_repository",
            "dependencies": [
              3
            ],
            "details": "In src/evaluation/dependency.py, update the get_run_handler provider definition: Change from providers.Factory(handlers.GetRunHandler, run_repository=adapter.run_repository) to providers.Factory(handlers.GetRunHandler, run_repository=adapter.run_repository, dataset_repository=adapter.dataset_repository).",
            "status": "pending",
            "testStrategy": "Integration test: 1) Test container wires GetRunHandler correctly, 2) Test both repositories are injected, 3) Test handler can be instantiated from container",
            "parentId": "undefined"
          },
          {
            "id": 9,
            "title": "Write unit tests for GetRunHandler.handle() with difficulty breakdown",
            "description": "Create comprehensive unit tests for the enhanced GetRunHandler.handle() method",
            "dependencies": [
              7
            ],
            "details": "Create or update tests in tests/evaluation/ (following existing test structure in test_model.py): 1) Test handle() with test cases of varying difficulties returns correct DifficultyMetrics for each, 2) Test metrics_by_difficulty contains correct number of entries, 3) Test with runs where test cases have difficulty=None (backward compatibility), 4) Test correct error handling when dataset not found, 5) Mock run_repository and dataset_repository dependencies.",
            "status": "pending",
            "testStrategy": "Run pytest on new test file: 1) Verify all test cases pass, 2) Verify test coverage includes happy path and edge cases, 3) Verify mocking is correct",
            "parentId": "undefined"
          },
          {
            "id": 10,
            "title": "Write integration test for full API to handler flow",
            "description": "Create integration test verifying end-to-end flow from API through handler to response",
            "dependencies": [
              7,
              8
            ],
            "details": "Create integration test that: 1) Creates a dataset with test cases having different difficulty levels, 2) Creates a run with results for those test cases, 3) Calls GetRunHandler.handle() with real repositories, 4) Verifies returned RunDetail has correctly computed metrics_by_difficulty, 5) Verifies each DifficultyMetrics entry has correct difficulty value, test_case_count, and metric values.",
            "status": "pending",
            "testStrategy": "Run pytest with integration test flag: 1) Verify test uses real database transaction, 2) Verify all metrics computed correctly, 3) Verify response serialization works end-to-end",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-23T11:18:05.922Z"
      },
      {
        "id": "26",
        "title": "Add generation-related domain models",
        "description": "Add EvaluationType enum and generation metrics models to domain layer",
        "details": "Add to src/evaluation/domain/model.py:\n\n```python\nclass EvaluationType(enum.StrEnum):\n    \"\"\"Type of evaluation to run.\"\"\"\n    RETRIEVAL_ONLY = \"retrieval_only\"\n    FULL_RAG = \"full_rag\"\n\nclass GenerationCaseMetrics(pydantic.BaseModel):\n    \"\"\"Per-case generation quality metrics.\"\"\"\n    model_config = pydantic.ConfigDict(frozen=True, extra=\"forbid\")\n    faithfulness: float  # 0.0-1.0\n    answer_relevancy: float  # 0.0-1.0\n\nclass GenerationMetrics(pydantic.BaseModel):\n    \"\"\"Aggregated generation quality metrics.\"\"\"\n    model_config = pydantic.ConfigDict(frozen=True, extra=\"forbid\")\n    mean_faithfulness: float\n    mean_answer_relevancy: float\n```\n\nExtend TestCaseResult:\n\n```python\nclass TestCaseResult(pydantic.BaseModel):\n    model_config = pydantic.ConfigDict(frozen=True, extra=\"forbid\")\n    id: str\n    test_case_id: str\n    retrieved_chunk_ids: tuple[str, ...]\n    retrieved_scores: tuple[float, ...]\n    precision: float\n    recall: float\n    hit: bool\n    reciprocal_rank: float\n    # Generation fields\n    generated_answer: str | None = None\n    faithfulness: float | None = None\n    answer_relevancy: float | None = None\n\n    @classmethod\n    def create(\n        cls,\n        test_case_id: str,\n        retrieved_chunk_ids: tuple[str, ...],\n        retrieved_scores: tuple[float, ...],\n        metrics: CaseMetrics,\n        generation_metrics: GenerationCaseMetrics | None = None,\n        generated_answer: str | None = None,\n    ) -> Self:\n        return cls(\n            id=uuid.uuid4().hex,\n            test_case_id=test_case_id,\n            retrieved_chunk_ids=retrieved_chunk_ids,\n            retrieved_scores=retrieved_scores,\n            precision=metrics.precision,\n            recall=metrics.recall,\n            hit=metrics.hit,\n            reciprocal_rank=metrics.reciprocal_rank,\n            generated_answer=generated_answer,\n            faithfulness=generation_metrics.faithfulness if generation_metrics else None,\n            answer_relevancy=generation_metrics.answer_relevancy if generation_metrics else None,\n        )\n```\n\nExtend EvaluationRun:\n\n```python\nclass EvaluationRun(pydantic.BaseModel):\n    model_config = pydantic.ConfigDict(frozen=True, extra=\"forbid\")\n    # ... existing fields ...\n    evaluation_type: EvaluationType = EvaluationType.RETRIEVAL_ONLY\n    mean_faithfulness: float | None = None\n    mean_answer_relevancy: float | None = None\n\n    @classmethod\n    def create(cls, dataset_id: str, k: int = 5, evaluation_type: EvaluationType = EvaluationType.RETRIEVAL_ONLY) -> Self:\n        now = common_types.utc_now()\n        return cls(\n            id=uuid.uuid4().hex,\n            dataset_id=dataset_id,\n            status=RunStatus.PENDING,\n            k=k,\n            evaluation_type=evaluation_type,\n            created_at=now,\n            updated_at=now,\n        )\n\n    def mark_completed(\n        self,\n        metrics: RetrievalMetrics,\n        results: tuple[TestCaseResult, ...],\n        generation_metrics: GenerationMetrics | None = None,\n    ) -> Self:\n        update_dict = {\n            \"status\": RunStatus.COMPLETED,\n            \"precision_at_k\": metrics.precision_at_k,\n            \"recall_at_k\": metrics.recall_at_k,\n            \"hit_rate_at_k\": metrics.hit_rate_at_k,\n            \"mrr\": metrics.mrr,\n            \"results\": results,\n            \"updated_at\": common_types.utc_now(),\n        }\n        if generation_metrics:\n            update_dict[\"mean_faithfulness\"] = generation_metrics.mean_faithfulness\n            update_dict[\"mean_answer_relevancy\"] = generation_metrics.mean_answer_relevancy\n        return self.model_copy(update=update_dict)\n```",
        "testStrategy": "1. Unit test EvaluationType enum has correct values\n2. Unit test GenerationCaseMetrics creation and immutability\n3. Unit test GenerationMetrics creation and immutability\n4. Unit test TestCaseResult.create() with and without generation_metrics\n5. Unit test EvaluationRun.create() with evaluation_type parameter\n6. Unit test EvaluationRun.mark_completed() with and without generation_metrics\n7. Verify backward compatibility: RETRIEVAL_ONLY is default",
        "priority": "high",
        "dependencies": [
          "25"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:21:03.549Z"
      },
      {
        "id": "27",
        "title": "Add generation columns to database",
        "description": "Create Alembic migration adding evaluation_type and generation metrics columns",
        "details": "Create new Alembic migration:\n\n```bash\nalembic revision -m \"add_generation_evaluation_columns\"\n```\n\nMigration upgrade:\n\n```python\ndef upgrade() -> None:\n    # Add to evaluation_runs\n    op.add_column(\n        'evaluation_runs',\n        sa.Column('evaluation_type', sa.String(20), nullable=False, server_default='retrieval_only')\n    )\n    op.add_column(\n        'evaluation_runs',\n        sa.Column('mean_faithfulness', sa.Float, nullable=True)\n    )\n    op.add_column(\n        'evaluation_runs',\n        sa.Column('mean_answer_relevancy', sa.Float, nullable=True)\n    )\n    \n    # Add to evaluation_test_case_results\n    op.add_column(\n        'evaluation_test_case_results',\n        sa.Column('generated_answer', sa.Text, nullable=True)\n    )\n    op.add_column(\n        'evaluation_test_case_results',\n        sa.Column('faithfulness', sa.Float, nullable=True)\n    )\n    op.add_column(\n        'evaluation_test_case_results',\n        sa.Column('answer_relevancy', sa.Float, nullable=True)\n    )\n\ndef downgrade() -> None:\n    op.drop_column('evaluation_test_case_results', 'answer_relevancy')\n    op.drop_column('evaluation_test_case_results', 'faithfulness')\n    op.drop_column('evaluation_test_case_results', 'generated_answer')\n    op.drop_column('evaluation_runs', 'mean_answer_relevancy')\n    op.drop_column('evaluation_runs', 'mean_faithfulness')\n    op.drop_column('evaluation_runs', 'evaluation_type')\n```\n\nUpdate ORM models in src/infrastructure/models/evaluation.py:\n\n```python\nclass EvaluationRunSchema(database_module.Base):\n    # ... existing fields ...\n    evaluation_type: sqlalchemy.orm.Mapped[str] = sqlalchemy.orm.mapped_column(\n        sqlalchemy.String(20), nullable=False, default=\"retrieval_only\"\n    )\n    mean_faithfulness: sqlalchemy.orm.Mapped[float | None] = sqlalchemy.orm.mapped_column(\n        sqlalchemy.Float, nullable=True\n    )\n    mean_answer_relevancy: sqlalchemy.orm.Mapped[float | None] = sqlalchemy.orm.mapped_column(\n        sqlalchemy.Float, nullable=True\n    )\n\nclass EvaluationTestCaseResultSchema(database_module.Base):\n    # ... existing fields ...\n    generated_answer: sqlalchemy.orm.Mapped[str | None] = sqlalchemy.orm.mapped_column(\n        sqlalchemy.Text, nullable=True\n    )\n    faithfulness: sqlalchemy.orm.Mapped[float | None] = sqlalchemy.orm.mapped_column(\n        sqlalchemy.Float, nullable=True\n    )\n    answer_relevancy: sqlalchemy.orm.Mapped[float | None] = sqlalchemy.orm.mapped_column(\n        sqlalchemy.Float, nullable=True\n    )\n```\n\nUpdate mappers in src/evaluation/domain/mapper.py to handle new fields in serialization/deserialization.",
        "testStrategy": "1. Run migration: alembic upgrade head\n2. Verify all columns exist with correct types and constraints\n3. Test mapper round-trip for generation fields\n4. Integration test: save and load EvaluationRun with generation metrics\n5. Test backward compatibility: existing runs load correctly with default evaluation_type",
        "priority": "high",
        "dependencies": [
          "26"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:25:19.632Z"
      },
      {
        "id": "28",
        "title": "Implement LLMJudge adapter",
        "description": "Create LLMJudge class using PydanticAI for faithfulness and answer relevancy scoring",
        "details": "Create src/evaluation/adapter/judge.py:\n\n```python\nimport json\nimport logging\nimport pydantic_ai\nfrom src.chunk.domain import model as chunk_model\n\nlogger = logging.getLogger(__name__)\n\nFAITHFULNESS_SYSTEM_PROMPT = \"\"\"You are an evaluation agent that assesses whether a generated answer is grounded in the provided context chunks.\n\nYour task: Score faithfulness on a scale of 0.0 to 1.0:\n- 1.0: Answer is fully grounded in the context, no hallucinations\n- 0.5: Answer is partially grounded, contains some unsupported claims\n- 0.0: Answer contradicts context or is entirely hallucinated\n\nReturn only valid JSON: {\"score\": <float>, \"reasoning\": \"<brief explanation>\"}\"\"\"\n\nFAITHFULNESS_USER_PROMPT = \"\"\"Question: {question}\n\nGenerated Answer: {answer}\n\nContext Chunks:\n{context}\n\nScore the faithfulness of the answer based on the context.\"\"\"\n\nRELEVANCY_SYSTEM_PROMPT = \"\"\"You are an evaluation agent that assesses whether a generated answer is relevant to the question.\n\nYour task: Score answer relevancy on a scale of 0.0 to 1.0:\n- 1.0: Answer directly and completely addresses the question\n- 0.5: Answer is partially relevant but incomplete or tangential\n- 0.0: Answer does not address the question\n\nReturn only valid JSON: {\"score\": <float>, \"reasoning\": \"<brief explanation>\"}\"\"\"\n\nRELEVANCY_USER_PROMPT = \"\"\"Question: {question}\n\nGenerated Answer: {answer}\n\nScore the relevancy of the answer to the question.\"\"\"\n\nclass LLMJudge:\n    \"\"\"LLM-as-Judge for evaluating generation quality.\"\"\"\n\n    def __init__(self, eval_model: str = \"openai:gpt-4o-mini\") -> None:\n        self._faithfulness_agent = pydantic_ai.Agent(\n            model=eval_model,\n            system_prompt=FAITHFULNESS_SYSTEM_PROMPT,\n        )\n        self._relevancy_agent = pydantic_ai.Agent(\n            model=eval_model,\n            system_prompt=RELEVANCY_SYSTEM_PROMPT,\n        )\n\n    async def score_faithfulness(\n        self,\n        question: str,\n        answer: str,\n        context_chunks: list[chunk_model.Chunk],\n    ) -> float:\n        \"\"\"Score answer faithfulness (grounding in context).\"\"\"\n        context_text = \"\\n\\n\".join(\n            f\"[{i+1}] {chunk.content}\" for i, chunk in enumerate(context_chunks)\n        )\n        prompt = FAITHFULNESS_USER_PROMPT.format(\n            question=question,\n            answer=answer,\n            context=context_text,\n        )\n        \n        try:\n            result = await self._faithfulness_agent.run(prompt)\n            return self._parse_score(result.output)\n        except Exception as exc:\n            logger.warning(\"Failed to score faithfulness: %s\", exc)\n            return 0.0\n\n    async def score_answer_relevancy(\n        self,\n        question: str,\n        answer: str,\n    ) -> float:\n        \"\"\"Score answer relevancy to question.\"\"\"\n        prompt = RELEVANCY_USER_PROMPT.format(\n            question=question,\n            answer=answer,\n        )\n        \n        try:\n            result = await self._relevancy_agent.run(prompt)\n            return self._parse_score(result.output)\n        except Exception as exc:\n            logger.warning(\"Failed to score relevancy: %s\", exc)\n            return 0.0\n\n    def _parse_score(self, output: str) -> float:\n        \"\"\"Parse LLM output to extract score.\"\"\"\n        try:\n            cleaned = output.strip()\n            if cleaned.startswith(\"```\"):\n                lines = cleaned.split(\"\\n\")\n                cleaned = \"\\n\".join(lines[1:-1]) if len(lines) > 2 else cleaned\n            \n            data = json.loads(cleaned)\n            score = float(data.get(\"score\", 0.0))\n            return max(0.0, min(1.0, score))  # Clamp to [0, 1]\n        except (json.JSONDecodeError, ValueError, TypeError):\n            logger.warning(\"Failed to parse score from output: %s\", output[:200])\n            return 0.0\n```\n\nAdd aggregate_generation_metrics to src/evaluation/domain/metric.py:\n\n```python\ndef aggregate_generation_metrics(\n    faithfulness_scores: list[float],\n    relevancy_scores: list[float],\n) -> tuple[float, float]:\n    \"\"\"Calculate aggregate generation metrics.\n    \n    Returns:\n        Tuple of (mean_faithfulness, mean_answer_relevancy).\n    \"\"\"\n    if not faithfulness_scores:\n        return (0.0, 0.0)\n    \n    mean_faithfulness = sum(faithfulness_scores) / len(faithfulness_scores)\n    mean_relevancy = sum(relevancy_scores) / len(relevancy_scores)\n    \n    return (mean_faithfulness, mean_relevancy)\n```",
        "testStrategy": "1. Unit test LLMJudge._parse_score with valid JSON output\n2. Unit test _parse_score with markdown-wrapped JSON\n3. Unit test _parse_score handles invalid JSON gracefully\n4. Unit test _parse_score clamps values to [0.0, 1.0]\n5. Integration test score_faithfulness with mocked PydanticAI agent\n6. Integration test score_answer_relevancy with mocked agent\n7. Unit test aggregate_generation_metrics function",
        "priority": "high",
        "dependencies": [
          "26",
          "27"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:26:54.328Z"
      },
      {
        "id": "29",
        "title": "Extend RunEvaluationHandler for full RAG evaluation",
        "description": "Add RAGAgent and LLMJudge dependencies to RunEvaluationHandler and implement full RAG evaluation flow",
        "details": "Update RunEvaluationHandler in src/evaluation/handler/handlers.py:\n\n```python\nfrom src.query.adapter.pydantic_ai import agent as rag_agent_module\nfrom src.evaluation.adapter import judge as judge_module\n\nclass RunEvaluationHandler:\n    def __init__(\n        self,\n        dataset_repository: evaluation_repository_module.DatasetRepository,\n        run_repository: evaluation_repository_module.RunRepository,\n        retrieval_service: retrieval.RetrievalService,\n        chunk_repository: chunk_repository_module.ChunkRepository,\n        rag_agent: rag_agent_module.RAGAgent | None = None,\n        llm_judge: judge_module.LLMJudge | None = None,\n    ) -> None:\n        self._dataset_repository = dataset_repository\n        self._run_repository = run_repository\n        self._retrieval_service = retrieval_service\n        self._chunk_repository = chunk_repository\n        self._rag_agent = rag_agent\n        self._llm_judge = llm_judge\n\n    async def handle(\n        self, dataset_id: str, cmd: command.RunEvaluation\n    ) -> response.RunDetail:\n        dataset = await self._load_runnable_dataset(dataset_id)\n\n        run = model.EvaluationRun.create(\n            dataset_id=dataset_id,\n            k=cmd.k,\n            evaluation_type=cmd.evaluation_type,\n        )\n        run = run.mark_running()\n        await self._run_repository.save(run)\n\n        try:\n            if cmd.evaluation_type == model.EvaluationType.FULL_RAG:\n                results, generation_metrics = await self._evaluate_full_rag(\n                    dataset, cmd.k\n                )\n            else:\n                results = await self._evaluate_test_cases(dataset, cmd.k)\n                generation_metrics = None\n            \n            retrieval_metrics = self._compute_aggregate_metrics(results, cmd.k)\n            run = run.mark_completed(\n                metrics=retrieval_metrics,\n                results=tuple(results),\n                generation_metrics=generation_metrics,\n            )\n            saved = await self._run_repository.save_with_results(run)\n            return response.RunDetail.from_entity(saved)\n        except Exception as exc:\n            # ... error handling ...\n\n    async def _evaluate_full_rag(\n        self, dataset: model.EvaluationDataset, k: int\n    ) -> tuple[list[model.TestCaseResult], model.GenerationMetrics]:\n        \"\"\"Evaluate with full RAG pipeline including generation.\"\"\"\n        if not self._rag_agent or not self._llm_judge:\n            raise exceptions.ValidationError(\n                \"RAGAgent and LLMJudge required for FULL_RAG evaluation\"\n            )\n        \n        results: list[model.TestCaseResult] = []\n        faithfulness_scores: list[float] = []\n        relevancy_scores: list[float] = []\n\n        for test_case in dataset.test_cases:\n            # Step 1: Retrieval\n            retrieved_chunks = await self._retrieval_service.retrieve(\n                notebook_id=dataset.notebook_id,\n                query=test_case.question,\n                max_chunks=k,\n            )\n            \n            # Step 2: Generation\n            query_answer = await self._rag_agent.answer(\n                question=test_case.question,\n                retrieved_chunks=retrieved_chunks,\n            )\n            generated_answer = query_answer.answer\n            \n            # Step 3: Judge generation quality\n            context_chunks = [rc.chunk for rc in retrieved_chunks]\n            faithfulness = await self._llm_judge.score_faithfulness(\n                question=test_case.question,\n                answer=generated_answer,\n                context_chunks=context_chunks,\n            )\n            relevancy = await self._llm_judge.score_answer_relevancy(\n                question=test_case.question,\n                answer=generated_answer,\n            )\n            \n            faithfulness_scores.append(faithfulness)\n            relevancy_scores.append(relevancy)\n            \n            # Step 4: Compute retrieval metrics\n            retrieved_ids = [rc.chunk.id for rc in retrieved_chunks]\n            retrieved_scores = [rc.score for rc in retrieved_chunks]\n            relevant_ids = set(test_case.ground_truth_chunk_ids)\n            \n            case_metrics = model.CaseMetrics(\n                precision=metric_module.precision_at_k(retrieved_ids, relevant_ids, k),\n                recall=metric_module.recall_at_k(retrieved_ids, relevant_ids, k),\n                hit=metric_module.hit_at_k(retrieved_ids, relevant_ids, k),\n                reciprocal_rank=metric_module.reciprocal_rank(retrieved_ids, relevant_ids, k),\n            )\n            \n            generation_case_metrics = model.GenerationCaseMetrics(\n                faithfulness=faithfulness,\n                answer_relevancy=relevancy,\n            )\n            \n            result = model.TestCaseResult.create(\n                test_case_id=test_case.id,\n                retrieved_chunk_ids=tuple(retrieved_ids),\n                retrieved_scores=tuple(retrieved_scores),\n                metrics=case_metrics,\n                generation_metrics=generation_case_metrics,\n                generated_answer=generated_answer,\n            )\n            results.append(result)\n        \n        # Aggregate generation metrics\n        mean_f, mean_r = metric_module.aggregate_generation_metrics(\n            faithfulness_scores, relevancy_scores\n        )\n        generation_metrics = model.GenerationMetrics(\n            mean_faithfulness=mean_f,\n            mean_answer_relevancy=mean_r,\n        )\n        \n        return results, generation_metrics\n```\n\nUpdate RunEvaluation command schema in src/evaluation/schema/command.py:\n\n```python\nclass RunEvaluation(pydantic.BaseModel):\n    k: int = 5\n    evaluation_type: model.EvaluationType = model.EvaluationType.RETRIEVAL_ONLY\n```\n\nUpdate RunDetail response in src/evaluation/schema/response.py:\n\n```python\nclass RunDetail(pydantic.BaseModel):\n    # ... existing fields ...\n    evaluation_type: str\n    mean_faithfulness: float | None = None\n    mean_answer_relevancy: float | None = None\n\n    @classmethod\n    def from_entity(cls, entity: model.EvaluationRun) -> Self:\n        # ... existing logic ...\n        return cls(\n            # ... existing fields ...\n            evaluation_type=entity.evaluation_type.value,\n            mean_faithfulness=entity.mean_faithfulness,\n            mean_answer_relevancy=entity.mean_answer_relevancy,\n            # ...\n        )\n```\n\nUpdate DI container in src/evaluation/dependency.py:\n\n```python\nfrom src.evaluation.adapter import judge as judge_module\n\nclass EvaluationAdapterContainer(containers.DeclarativeContainer):\n    # ... existing providers ...\n    llm_judge = providers.Singleton(\n        judge_module.LLMJudge,\n        eval_model=settings_module.settings.eval_model,\n    )\n\nclass EvaluationHandlerContainer(containers.DeclarativeContainer):\n    # ... existing dependencies ...\n    query_adapter = providers.DependenciesContainer()\n    \n    run_evaluation_handler = providers.Factory(\n        handlers.RunEvaluationHandler,\n        dataset_repository=adapter.dataset_repository,\n        run_repository=adapter.run_repository,\n        retrieval_service=query_service.retrieval_service,\n        chunk_repository=chunk_adapter.repository,\n        rag_agent=query_adapter.rag_agent,\n        llm_judge=adapter.llm_judge,\n    )\n```\n\nUpdate main container in src/dependency/container.py to wire query_adapter to evaluation handler.",
        "testStrategy": "1. Unit test _evaluate_full_rag with mocked dependencies\n2. Verify full RAG evaluation generates answers and scores them\n3. Integration test: RETRIEVAL_ONLY evaluation still works (backward compatibility)\n4. Integration test: FULL_RAG evaluation computes both retrieval and generation metrics\n5. Test error handling when RAGAgent/LLMJudge not available for FULL_RAG\n6. Verify RunDetail response includes generation metrics when present",
        "priority": "high",
        "dependencies": [
          "28"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:30:24.103Z"
      },
      {
        "id": "30",
        "title": "Update CLI and API for generation evaluation",
        "description": "Add --type option to CLI run command and update results display for generation metrics",
        "details": "Update CLI in src/cli/commands/evaluation.py:\n\n```python\n@app.command(\"run\")\ndef run_evaluation(\n    dataset_id: str = typer.Argument(..., help=\"Dataset ID\"),\n    k: int = typer.Option(5, \"--k\", help=\"Number of chunks to retrieve\"),\n    evaluation_type: str = typer.Option(\n        \"retrieval_only\",\n        \"--type\",\n        help=\"Evaluation type: retrieval_only or full_rag\",\n    ),\n) -> None:\n    \"\"\"Run evaluation against a dataset.\"\"\"\n    try:\n        eval_type = model.EvaluationType(evaluation_type)\n    except ValueError:\n        console.print(f\"[red]Invalid evaluation type: {evaluation_type}[/red]\")\n        raise typer.Exit(1)\n    \n    cmd = command.RunEvaluation(k=k, evaluation_type=eval_type)\n    # ... rest of implementation ...\n\n@app.command(\"results\")\ndef get_results(\n    run_id: str = typer.Argument(..., help=\"Run ID\"),\n) -> None:\n    \"\"\"Display evaluation run results.\"\"\"\n    # ... fetch run_detail ...\n    \n    # Display retrieval metrics\n    if run_detail.metrics:\n        table = Table(title=\"Retrieval Metrics\")\n        table.add_column(\"Metric\", style=\"cyan\")\n        table.add_column(\"Value\", style=\"green\")\n        table.add_row(\"Precision@k\", f\"{run_detail.metrics.precision_at_k:.4f}\")\n        table.add_row(\"Recall@k\", f\"{run_detail.metrics.recall_at_k:.4f}\")\n        table.add_row(\"Hit Rate@k\", f\"{run_detail.metrics.hit_rate_at_k:.4f}\")\n        table.add_row(\"MRR\", f\"{run_detail.metrics.mrr:.4f}\")\n        console.print(table)\n    \n    # Display generation metrics if present\n    if run_detail.evaluation_type == \"full_rag\" and run_detail.mean_faithfulness is not None:\n        gen_table = Table(title=\"Generation Metrics\")\n        gen_table.add_column(\"Metric\", style=\"cyan\")\n        gen_table.add_column(\"Value\", style=\"green\")\n        gen_table.add_row(\"Faithfulness\", f\"{run_detail.mean_faithfulness:.4f}\")\n        gen_table.add_row(\"Answer Relevancy\", f\"{run_detail.mean_answer_relevancy:.4f}\")\n        console.print(gen_table)\n    \n    # Display difficulty breakdown if present\n    if run_detail.metrics_by_difficulty:\n        diff_table = Table(title=\"Metrics by Difficulty\")\n        diff_table.add_column(\"Difficulty\", style=\"cyan\")\n        diff_table.add_column(\"Count\", style=\"yellow\")\n        diff_table.add_column(\"Precision@k\", style=\"green\")\n        diff_table.add_column(\"Recall@k\", style=\"green\")\n        diff_table.add_column(\"Hit Rate@k\", style=\"green\")\n        diff_table.add_column(\"MRR\", style=\"green\")\n        \n        for dm in run_detail.metrics_by_difficulty:\n            diff_table.add_row(\n                dm.difficulty,\n                str(dm.test_case_count),\n                f\"{dm.precision_at_k:.4f}\",\n                f\"{dm.recall_at_k:.4f}\",\n                f\"{dm.hit_rate_at_k:.4f}\",\n                f\"{dm.mrr:.4f}\",\n            )\n        console.print(diff_table)\n```\n\nUpdate API endpoint in src/evaluation/entrypoint/api.py to accept evaluation_type in request body (already handled if command schema is updated).",
        "testStrategy": "1. Manual test: run CLI with --type retrieval_only\n2. Manual test: run CLI with --type full_rag\n3. Verify CLI validates evaluation_type and rejects invalid values\n4. Verify results command displays generation metrics table when present\n5. Verify results command displays difficulty breakdown when present\n6. Integration test: API accepts evaluation_type in request body",
        "priority": "medium",
        "dependencies": [
          "29"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:32:00.521Z"
      },
      {
        "id": "31",
        "title": "Add RunRepository.list_by_ids and comparison response schemas",
        "description": "Add list_by_ids method to RunRepository and create comparison response schemas",
        "details": "Update RunRepository in src/evaluation/adapter/repository.py:\n\n```python\nclass RunRepository:\n    # ... existing methods ...\n    \n    async def list_by_ids(self, run_ids: list[str]) -> list[model.EvaluationRun]:\n        \"\"\"Find multiple runs by their IDs.\"\"\"\n        stmt = (\n            sqlalchemy.select(evaluation_schema.EvaluationRunSchema)\n            .where(evaluation_schema.EvaluationRunSchema.id.in_(run_ids))\n            .order_by(evaluation_schema.EvaluationRunSchema.created_at.asc())\n        )\n        result = await self._session.execute(stmt)\n        records = result.scalars().all()\n        return [self._mapper.to_entity(record) for record in records]\n```\n\nCreate comparison schemas in src/evaluation/schema/response.py:\n\n```python\nclass RunComparisonMetrics(pydantic.BaseModel):\n    \"\"\"Per-run aggregate metrics for comparison.\"\"\"\n    run_id: str\n    created_at: datetime.datetime\n    precision_at_k: float\n    recall_at_k: float\n    hit_rate_at_k: float\n    mrr: float\n    mean_faithfulness: float | None = None\n    mean_answer_relevancy: float | None = None\n\nclass TestCaseComparisonEntry(pydantic.BaseModel):\n    \"\"\"Per-run metrics for a single test case.\"\"\"\n    run_id: str\n    precision: float\n    recall: float\n    hit: bool\n    reciprocal_rank: float\n    faithfulness: float | None = None\n    answer_relevancy: float | None = None\n    generated_answer: str | None = None\n\nclass TestCaseComparison(pydantic.BaseModel):\n    \"\"\"Comparison of a test case across multiple runs.\"\"\"\n    test_case_id: str\n    question: str\n    difficulty: str | None\n    entries: list[TestCaseComparisonEntry]\n\nclass RunComparisonResponse(pydantic.BaseModel):\n    \"\"\"Complete run comparison response.\"\"\"\n    dataset_id: str\n    k: int\n    run_count: int\n    aggregate_metrics: list[RunComparisonMetrics]\n    test_case_comparisons: list[TestCaseComparison]\n```\n\nCreate CompareRuns command schema in src/evaluation/schema/command.py:\n\n```python\nclass CompareRuns(pydantic.BaseModel):\n    \"\"\"Command to compare multiple evaluation runs.\"\"\"\n    run_ids: list[str] = pydantic.Field(..., min_length=2, max_length=10)\n```",
        "testStrategy": "1. Unit test RunRepository.list_by_ids with valid run IDs\n2. Unit test list_by_ids with empty list returns empty result\n3. Unit test list_by_ids with non-existent IDs returns only found runs\n4. Unit test comparison response schema validation\n5. Unit test CompareRuns command validation (min 2, max 10 runs)\n6. Verify list_by_ids preserves order by created_at",
        "priority": "medium",
        "dependencies": [
          "29",
          "30"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:33:37.193Z"
      },
      {
        "id": "32",
        "title": "Implement CompareRunsHandler",
        "description": "Create CompareRunsHandler to build cross-run comparison with difficulty labels and generation metrics",
        "details": "Create CompareRunsHandler in src/evaluation/handler/handlers.py:\n\n```python\nclass CompareRunsHandler:\n    \"\"\"Handler for comparing multiple evaluation runs.\"\"\"\n\n    def __init__(\n        self,\n        run_repository: evaluation_repository_module.RunRepository,\n        dataset_repository: evaluation_repository_module.DatasetRepository,\n    ) -> None:\n        self._run_repository = run_repository\n        self._dataset_repository = dataset_repository\n\n    async def handle(self, cmd: command.CompareRuns) -> response.RunComparisonResponse:\n        \"\"\"Compare multiple evaluation runs.\"\"\"\n        # Load all runs\n        runs = await self._run_repository.list_by_ids(cmd.run_ids)\n        \n        if len(runs) != len(cmd.run_ids):\n            found_ids = {r.id for r in runs}\n            missing = set(cmd.run_ids) - found_ids\n            raise exceptions.NotFoundError(f\"Runs not found: {missing}\")\n        \n        # Validate all runs belong to same dataset\n        dataset_ids = {r.dataset_id for r in runs}\n        if len(dataset_ids) > 1:\n            raise exceptions.ValidationError(\n                \"All runs must belong to the same dataset\"\n            )\n        \n        # Validate all runs are COMPLETED\n        incomplete = [r.id for r in runs if r.status != model.RunStatus.COMPLETED]\n        if incomplete:\n            raise exceptions.ValidationError(\n                f\"All runs must be COMPLETED. Incomplete runs: {incomplete}\"\n            )\n        \n        # Validate all runs use same k\n        k_values = {r.k for r in runs}\n        if len(k_values) > 1:\n            raise exceptions.ValidationError(\n                \"All runs must use the same k value\"\n            )\n        \n        dataset_id = runs[0].dataset_id\n        k = runs[0].k\n        \n        # Load dataset for test case metadata\n        dataset = await self._dataset_repository.find_by_id(dataset_id)\n        if dataset is None:\n            raise exceptions.NotFoundError(f\"Dataset not found: {dataset_id}\")\n        \n        # Build aggregate metrics per run\n        aggregate_metrics = self._build_aggregate_metrics(runs)\n        \n        # Build per-test-case comparison\n        test_case_comparisons = self._build_test_case_comparisons(\n            runs, dataset\n        )\n        \n        return response.RunComparisonResponse(\n            dataset_id=dataset_id,\n            k=k,\n            run_count=len(runs),\n            aggregate_metrics=aggregate_metrics,\n            test_case_comparisons=test_case_comparisons,\n        )\n\n    def _build_aggregate_metrics(\n        self, runs: list[model.EvaluationRun]\n    ) -> list[response.RunComparisonMetrics]:\n        \"\"\"Build aggregate metrics for each run.\"\"\"\n        return [\n            response.RunComparisonMetrics(\n                run_id=run.id,\n                created_at=run.created_at,\n                precision_at_k=run.precision_at_k or 0.0,\n                recall_at_k=run.recall_at_k or 0.0,\n                hit_rate_at_k=run.hit_rate_at_k or 0.0,\n                mrr=run.mrr or 0.0,\n                mean_faithfulness=run.mean_faithfulness,\n                mean_answer_relevancy=run.mean_answer_relevancy,\n            )\n            for run in runs\n        ]\n\n    def _build_test_case_comparisons(\n        self,\n        runs: list[model.EvaluationRun],\n        dataset: model.EvaluationDataset,\n    ) -> list[response.TestCaseComparison]:\n        \"\"\"Build per-test-case cross-run comparison.\"\"\"\n        # Build test case metadata map\n        tc_map = {\n            tc.id: tc for tc in dataset.test_cases\n        }\n        \n        # Build result map: test_case_id -> {run_id -> result}\n        result_map: dict[str, dict[str, model.TestCaseResult]] = {}\n        for run in runs:\n            for result in run.results:\n                if result.test_case_id not in result_map:\n                    result_map[result.test_case_id] = {}\n                result_map[result.test_case_id][run.id] = result\n        \n        # Build comparisons\n        comparisons: list[response.TestCaseComparison] = []\n        for tc_id, run_results in result_map.items():\n            test_case = tc_map.get(tc_id)\n            if not test_case:\n                continue\n            \n            entries = [\n                response.TestCaseComparisonEntry(\n                    run_id=run.id,\n                    precision=run_results[run.id].precision,\n                    recall=run_results[run.id].recall,\n                    hit=run_results[run.id].hit,\n                    reciprocal_rank=run_results[run.id].reciprocal_rank,\n                    faithfulness=run_results[run.id].faithfulness,\n                    answer_relevancy=run_results[run.id].answer_relevancy,\n                    generated_answer=run_results[run.id].generated_answer,\n                )\n                for run in runs\n                if run.id in run_results\n            ]\n            \n            comparisons.append(\n                response.TestCaseComparison(\n                    test_case_id=tc_id,\n                    question=test_case.question,\n                    difficulty=test_case.difficulty.value if test_case.difficulty else None,\n                    entries=entries,\n                )\n            )\n        \n        return comparisons\n```",
        "testStrategy": "1. Unit test handle() validates all runs exist\n2. Unit test handle() validates runs belong to same dataset\n3. Unit test handle() validates all runs are COMPLETED\n4. Unit test handle() validates all runs use same k\n5. Unit test _build_aggregate_metrics returns correct RunComparisonMetrics\n6. Unit test _build_test_case_comparisons includes difficulty labels\n7. Unit test _build_test_case_comparisons includes generation metrics when present\n8. Integration test: full comparison flow with multiple runs",
        "priority": "medium",
        "dependencies": [
          "31"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:36:59.518Z"
      },
      {
        "id": "33",
        "title": "Add comparison API endpoint and CLI command",
        "description": "Add POST /evaluation/compare endpoint and compare CLI command with Rich table output",
        "details": "Add endpoint to src/evaluation/entrypoint/api.py:\n\n```python\n@router.post(\n    \"/compare\",\n    response_model=response.RunComparisonResponse,\n    status_code=status.HTTP_200_OK,\n)\nasync def compare_runs(\n    cmd: command.CompareRuns,\n    handler: handlers.CompareRunsHandler = fastapi.Depends(\n        dependencies.get_compare_runs_handler\n    ),\n) -> response.RunComparisonResponse:\n    \"\"\"Compare multiple evaluation runs.\"\"\"\n    return await handler.handle(cmd)\n```\n\nAdd CLI command to src/cli/commands/evaluation.py:\n\n```python\n@app.command(\"compare\")\ndef compare_runs(\n    run_ids: list[str] = typer.Argument(\n        ..., help=\"Run IDs to compare (2-10 runs)\"\n    ),\n) -> None:\n    \"\"\"Compare multiple evaluation runs side-by-side.\"\"\"\n    if len(run_ids) < 2 or len(run_ids) > 10:\n        console.print(\"[red]Must provide 2-10 run IDs[/red]\")\n        raise typer.Exit(1)\n    \n    cmd = command.CompareRuns(run_ids=run_ids)\n    # ... fetch comparison ...\n    \n    # Display aggregate metrics comparison\n    agg_table = Table(title=\"Run Comparison - Aggregate Metrics\")\n    agg_table.add_column(\"Run ID\", style=\"cyan\")\n    agg_table.add_column(\"Created At\", style=\"yellow\")\n    agg_table.add_column(\"Precision@k\", style=\"green\")\n    agg_table.add_column(\"Recall@k\", style=\"green\")\n    agg_table.add_column(\"Hit Rate@k\", style=\"green\")\n    agg_table.add_column(\"MRR\", style=\"green\")\n    agg_table.add_column(\"Faithfulness\", style=\"blue\")\n    agg_table.add_column(\"Relevancy\", style=\"blue\")\n    \n    for metrics in comparison.aggregate_metrics:\n        agg_table.add_row(\n            metrics.run_id[:8],\n            metrics.created_at.strftime(\"%Y-%m-%d %H:%M\"),\n            f\"{metrics.precision_at_k:.4f}\",\n            f\"{metrics.recall_at_k:.4f}\",\n            f\"{metrics.hit_rate_at_k:.4f}\",\n            f\"{metrics.mrr:.4f}\",\n            f\"{metrics.mean_faithfulness:.4f}\" if metrics.mean_faithfulness else \"N/A\",\n            f\"{metrics.mean_answer_relevancy:.4f}\" if metrics.mean_answer_relevancy else \"N/A\",\n        )\n    console.print(agg_table)\n    \n    # Display per-test-case comparison summary\n    console.print(f\"\\n[bold]Test Cases:[/bold] {len(comparison.test_case_comparisons)}\")\n    console.print(f\"[bold]Dataset ID:[/bold] {comparison.dataset_id}\")\n    console.print(f\"[bold]k:[/bold] {comparison.k}\")\n```\n\nUpdate DI container in src/evaluation/dependency.py:\n\n```python\nclass EvaluationHandlerContainer(containers.DeclarativeContainer):\n    # ... existing handlers ...\n    \n    compare_runs_handler = providers.Factory(\n        handlers.CompareRunsHandler,\n        run_repository=adapter.run_repository,\n        dataset_repository=adapter.dataset_repository,\n    )\n```\n\nUpdate dependencies module in src/evaluation/entrypoint/api.py:\n\n```python\nasync def get_compare_runs_handler(\n    session: AsyncSession = Depends(dependencies.get_db_session),\n) -> handlers.CompareRunsHandler:\n    container = dependencies.get_evaluation_container()\n    return container.handler.compare_runs_handler(db_session=session)\n```",
        "testStrategy": "1. Manual test: CLI compare command with 2 runs\n2. Manual test: CLI compare command with invalid run count (< 2 or > 10)\n3. Verify Rich table displays all aggregate metrics correctly\n4. Integration test: POST /evaluation/compare endpoint\n5. Test error cases: runs from different datasets, incomplete runs, different k values\n6. Verify generation metrics displayed when present",
        "priority": "medium",
        "dependencies": [
          "32"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:37:56.684Z"
      },
      {
        "id": "34",
        "title": "Write Retrieval Evaluation documentation",
        "description": "Create comprehensive Korean documentation for the retrieval evaluation system in docs/evaluation/retrieval-evaluation.md",
        "details": "Create docs/evaluation/retrieval-evaluation.md documenting the fully implemented retrieval evaluation system.\n\nDocument structure:\n1.  (Overview): Explain what retrieval evaluation measures and its importance for RAG systems. Describe how it validates whether the right chunks are being retrieved for user queries.\n\n2.   (Core Concepts): Document the evaluation pipeline:\n   - Dataset generation from chunks via LLM (SyntheticTestGenerator)\n   - Test case creation with ground truth chunk IDs\n   - Retrieval execution using RetrievalService\n   - Metric computation using metric functions\n   Explain the relationship between source chunks, generated questions, and ground truth chunk IDs.\n\n3.   (Metrics Explained): For each metric, provide:\n   - Precision@k: Formula: (relevant chunks in top-k) / k. Range: 0.0-1.0. Example: \"Precision@5 = 0.4 means 2 out of 5 retrieved chunks were relevant\". Mathematical formula: P@k = |{relevant}  {retrieved[:k]}| / k\n   - Recall@k: Formula: (relevant chunks in top-k) / (total relevant chunks). Range: 0.0-1.0. Example: \"Recall@5 = 0.6 with 3 ground truth chunks means 1.8  2 chunks were found\". Mathematical formula: R@k = |{relevant}  {retrieved[:k]}| / |{relevant}|\n   - Hit Rate@k: Binary metric (0.0 or 1.0). Example: \"Hit Rate@5 = 1.0 means at least one relevant chunk appeared in top-5\". Formula: H@k = 1 if |{relevant}  {retrieved[:k]}| > 0 else 0\n   - MRR (Mean Reciprocal Rank): Average of 1/rank of first relevant chunk. Range: 0.0-1.0. Example: \"MRR = 0.5 means first relevant chunk appeared at position 2 on average\". Formula: MRR = mean(1/rank_first_relevant)\n\n4.   (Usage Guide):\n   - API examples for all 5 endpoints (using actual endpoint paths from api.py):\n     * POST /notebooks/{notebook_id}/evaluation/datasets (generate dataset)\n     * GET /notebooks/{notebook_id}/evaluation/datasets (list datasets)\n     * GET /evaluation/datasets/{dataset_id} (get dataset detail)\n     * POST /evaluation/datasets/{dataset_id}/runs (run evaluation)\n     * GET /evaluation/runs/{run_id} (get run results)\n   - CLI examples for all 4 commands (using actual command names from evaluation.py):\n     * eval generate <notebook_id> --name <name> --questions 2 --max-chunks 50\n     * eval list <notebook_id>\n     * eval run <dataset_id> --k 5\n     * eval results <run_id>\n   Include sample JSON request/response and CLI output examples.\n\n5.   (Parameter Tuning):\n   - k value: Explain how k affects precision/recall tradeoff (higher k  lower precision, higher recall)\n   - questions_per_chunk: More questions  better coverage but higher LLM cost (default: 2)\n   - max_chunks_sample: Limits dataset size for faster generation (default: 50)\n\n6.   (Result Interpretation):\n   - Score ranges: >0.8 (excellent), 0.6-0.8 (good), 0.4-0.6 (acceptable), <0.4 (needs improvement)\n   - Common patterns: High precision but low recall  k too small; Low precision  embedding quality issues; Low MRR  relevant chunks ranked poorly\n   - Actionable suggestions: Improve chunk size, adjust embedding model, tune retrieval algorithm, increase k value\n\nReference existing implementation in:\n- src/evaluation/domain/model.py (entities)\n- src/evaluation/domain/metric.py (metric functions)\n- src/evaluation/handler/handlers.py (business logic)\n- src/evaluation/entrypoint/api.py (API endpoints)\n- src/cli/commands/evaluation.py (CLI commands)\n\nEnsure all code examples use actual field names from schemas (command.py, response.py).",
        "testStrategy": "1. Review documentation for completeness by checking all 6 sections are present\n2. Verify all API endpoint paths match src/evaluation/entrypoint/api.py\n3. Verify all CLI command names and options match src/cli/commands/evaluation.py\n4. Verify metric formulas match implementations in src/evaluation/domain/metric.py\n5. Verify JSON schema examples match src/evaluation/schema/response.py field names\n6. Manual review: Korean grammar and technical accuracy\n7. Test example API requests against running application to ensure they work\n8. Test example CLI commands to ensure they produce expected output",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:06:09.384Z"
      },
      {
        "id": "35",
        "title": "Write Question Difficulty Classification documentation",
        "description": "Create comprehensive Korean documentation for the question difficulty classification feature in docs/evaluation/question-difficulty.md",
        "details": "Create docs/evaluation/question-difficulty.md documenting the question difficulty classification system (depends on Task 25 completion).\n\nDocument structure:\n1.  (Overview): Explain why difficulty classification enables granular evaluation analysis. Allows identifying which question types the retrieval system handles well vs poorly.\n\n2.   (Difficulty Levels): Define each QuestionDifficulty enum level with concrete examples:\n   - FACTUAL: Direct fact retrieval. Example: \"What is the capital of France?\"  Answer found verbatim in chunk\n   - ANALYTICAL: Requires synthesizing information. Example: \"Compare the performance of algorithm A and B\"  Answer requires combining multiple facts\n   - INFERENTIAL: Requires reasoning beyond stated facts. Example: \"Why did the stock price increase?\"  Answer requires inference from context\n   - PARAPHRASED: Same meaning, different wording. Example: Question uses \"automobile\" when chunk says \"car\"  Tests semantic understanding\n   Explain that difficulty is assigned by LLM during synthetic test generation in SyntheticTestGenerator.\n\n3.    (How Classification Works):\n   - Explain that LLM assigns difficulty during dataset generation based on question characteristics\n   - The difficulty field is stored in TestCase.difficulty (added in Task 24)\n   - Reference the prompt engineering used in SyntheticTestGenerator to classify difficulty\n\n4.   (Metrics by Difficulty):\n   - Explain how results are grouped by difficulty in GetRunHandler\n   - Aggregate metrics (Precision@k, Recall@k, Hit Rate@k, MRR) computed per difficulty group\n   - Allows comparing retrieval performance across question types\n\n5.   (Usage Guide):\n   - Show how difficulty appears in dataset detail API response:\n     ```json\n     {\n       \"test_cases\": [\n         {\n           \"id\": \"abc123\",\n           \"question\": \"What is...\",\n           \"difficulty\": \"FACTUAL\",\n           ...\n         }\n       ]\n     }\n     ```\n   - Show how metrics_by_difficulty appears in run detail response:\n     ```json\n     {\n       \"metrics_by_difficulty\": [\n         {\n           \"difficulty\": \"FACTUAL\",\n           \"test_case_count\": 25,\n           \"precision_at_k\": 0.85,\n           \"recall_at_k\": 0.72,\n           \"hit_rate_at_k\": 0.96,\n           \"mrr\": 0.68\n         },\n         ...\n       ]\n     }\n     ```\n   - Show CLI results command output with difficulty breakdown table (from Task 30 implementation)\n\n6.   (Result Interpretation):\n   - Low FACTUAL scores  Basic retrieval problem, check embedding quality or chunk splitting\n   - Low ANALYTICAL scores  System struggles with multi-chunk questions, consider improving ranking\n   - Low INFERENTIAL scores  Expected (hardest type), may not need action unless critical for use case\n   - Low PARAPHRASED scores  Embedding model struggles with semantic similarity, consider fine-tuning or different model\n   - Provide guidance on which difficulty levels matter most for different RAG use cases\n\nReference Task 25 implementation for GetRunHandler difficulty breakdown logic.\nReference response.DifficultyMetrics schema structure.\nNote that this feature requires Task 24 (difficulty field) and Task 25 (difficulty breakdown) to be completed.",
        "testStrategy": "1. Verify documentation references correct QuestionDifficulty enum values from Task 24\n2. Verify DifficultyMetrics JSON structure matches response.DifficultyMetrics from Task 25\n3. Verify difficulty breakdown logic description matches GetRunHandler implementation from Task 25\n4. Review Korean technical terminology for difficulty levels\n5. Validate example JSON responses are valid according to schema\n6. Cross-reference with retrieval-evaluation.md for consistent terminology\n7. After Task 25 completion: Test example API calls return expected difficulty breakdown\n8. After Task 30 completion: Test CLI results command shows difficulty table",
        "priority": "medium",
        "dependencies": [
          "25"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:21:03.553Z"
      },
      {
        "id": "36",
        "title": "Write Generation Quality Evaluation documentation",
        "description": "Create comprehensive Korean documentation for the LLM-as-Judge generation quality evaluation in docs/evaluation/generation-evaluation.md",
        "details": "Create docs/evaluation/generation-evaluation.md documenting the generation quality evaluation system (depends on Task 30 completion).\n\nDocument structure:\n1.  (Overview): Explain why evaluating answer generation quality matters beyond just retrieval. RAG systems must not only retrieve the right chunks but also generate accurate, relevant answers. Generation evaluation validates the LLM's ability to synthesize retrieved information.\n\n2.   (Evaluation Types):\n   - RETRIEVAL_ONLY: Evaluates only retrieval quality (default, existing behavior)\n   - FULL_RAG: Evaluates both retrieval and answer generation quality\n   - Explain when to use each: RETRIEVAL_ONLY for fast iteration on retrieval, FULL_RAG for end-to-end system validation\n   - Reference EvaluationType enum from domain model\n\n3.   (Generation Metrics):\n   - Faithfulness: Measures factual accuracy (answer is grounded in retrieved context)\n     * Concept: LLM-as-Judge evaluates if answer statements can be verified from retrieved chunks\n     * Scoring: 0.0-1.0 scale, LLM assigns score based on faithfulness rubric\n     * Example: \"Faithfulness = 0.9 means 90% of answer statements are supported by retrieved chunks\"\n     * Implementation: LLM receives (question, retrieved_chunks, generated_answer) and judges faithfulness\n   - Answer Relevancy: Measures how well answer addresses the question\n     * Concept: LLM-as-Judge evaluates if answer directly answers the question\n     * Scoring: 0.0-1.0 scale, LLM assigns score based on relevancy rubric\n     * Example: \"Answer Relevancy = 0.8 means answer mostly addresses the question with minor tangents\"\n     * Implementation: LLM receives (question, generated_answer) and judges relevancy\n   Reference GenerationMetrics domain model and LLMJudge implementation from Task 29.\n\n4.  RAG  (Full RAG Pipeline):\n   - Step-by-step flow diagram:\n     1. Question  RetrievalService retrieves top-k chunks\n     2. (Question + Retrieved chunks)  AnswerGenerator generates answer\n     3. (Question + Retrieved chunks + Generated answer)  LLM Judge scores Faithfulness\n     4. (Question + Generated answer)  LLM Judge scores Answer Relevancy\n     5. Aggregate metrics computed across all test cases\n   - Explain that FULL_RAG runs the complete pipeline for each test case\n   - Reference AnswerGenerator and LLMJudge from Task 29 implementation\n\n5.   (Usage Guide):\n   - API example with evaluation_type=full_rag:\n     ```bash\n     POST /evaluation/datasets/{dataset_id}/runs\n     {\n       \"k\": 5,\n       \"evaluation_type\": \"full_rag\"\n     }\n     ```\n   - CLI run command with --type full_rag option:\n     ```bash\n     eval run <dataset_id> --k 5 --type full_rag\n     ```\n   - CLI results output showing both retrieval and generation metrics:\n     ```\n     Retrieval Metrics:\n       Precision@5: 0.7200\n       Recall@5: 0.6800\n       ...\n     \n     Generation Metrics:\n       Faithfulness: 0.8500\n       Answer Relevancy: 0.7800\n     ```\n   Reference updated RunEvaluation command schema and CLI from Task 30.\n\n6.   (Result Interpretation):\n   - Faithfulness vs Relevancy tradeoffs:\n     * High relevancy but low faithfulness  Hallucination risk (LLM inventing facts)\n     * High faithfulness but low relevancy  Off-topic or overly cautious answers\n     * Both low  Retrieval problem (wrong chunks) or generation problem (poor prompts)\n   - Common patterns:\n     * Low faithfulness with good retrieval  Improve generation prompts, add grounding instructions\n     * Low relevancy with good retrieval  Improve answer generation prompt to focus on question\n     * Both metrics low with poor retrieval  Fix retrieval first before tuning generation\n   - Actionable suggestions:\n     * For low faithfulness: Add \"Only use information from provided context\" instruction, use stricter prompts\n     * For low relevancy: Add \"Directly answer the question\" instruction, avoid verbose preambles\n     * For both low: Reevaluate chunk quality, retrieval algorithm, and generation prompt together\n\n7.   (Cost Considerations):\n   - FULL_RAG uses additional LLM calls per test case:\n     * 1 call for answer generation (AnswerGenerator)\n     * 1 call for faithfulness judgment (LLMJudge)\n     * 1 call for answer relevancy judgment (LLMJudge)\n   - Total: 3 additional LLM calls per test case vs RETRIEVAL_ONLY\n   - Cost estimation example: 100 test cases with GPT-4  ~300 judge calls  estimate $X (provide rough calculation based on token costs)\n   - Recommendation: Use FULL_RAG selectively for final validation, use RETRIEVAL_ONLY for rapid iteration\n\nReference Task 29 implementation for AnswerGenerator, LLMJudge, and GenerationMetrics.\nReference Task 30 implementation for CLI --type option and results display.",
        "testStrategy": "1. Verify EvaluationType enum values match domain model from Task 29\n2. Verify GenerationMetrics field names match response schema from Task 29\n3. Verify API request/response examples match updated command/response schemas from Task 30\n4. Verify CLI command examples match actual CLI implementation from Task 30\n5. Review pipeline flow diagram for accuracy against Task 29 implementation\n6. Validate cost estimation calculations are reasonable\n7. Cross-reference terminology with retrieval-evaluation.md\n8. After Task 30 completion: Test API and CLI examples produce expected results\n9. Manual review: Korean technical accuracy for LLM-as-Judge concepts",
        "priority": "medium",
        "dependencies": [
          "30"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:38:25.678Z"
      },
      {
        "id": "37",
        "title": "Write Run Comparison documentation",
        "description": "Create comprehensive Korean documentation for the run comparison feature in docs/evaluation/run-comparison.md",
        "details": "Create docs/evaluation/run-comparison.md documenting the run comparison system (depends on Task 33 completion).\n\nDocument structure:\n1.  (Overview): Explain why comparing evaluation runs is critical for iterative RAG improvement. Enables A/B testing different configurations to measure impact of changes. Without comparison, engineers cannot objectively validate improvements.\n\n2.   (Use Cases): Provide concrete A/B testing scenarios:\n   - Different k values: Compare k=3 vs k=5 vs k=10 to find optimal retrieval count\n   - Embedding model changes: Test OpenAI text-embedding-3-small vs text-embedding-3-large\n   - Chunking strategy changes: Compare fixed-size chunks vs semantic chunking\n   - Prompt engineering: Test different answer generation prompts (FULL_RAG evaluation)\n   - Retrieval algorithm tuning: Compare hybrid search vs pure vector search\n   Explain that comparison enables data-driven decisions for each change.\n\n3.   (Comparison Mechanics):\n   - Explain validation requirements:\n     * All runs must share the same dataset_id (ensures same test questions)\n     * All runs must use the same k value (enables fair metric comparison)\n     * All runs must be COMPLETED status\n   - Explain alignment: Metrics compared per-test-case across runs (same question  compare metrics)\n   - Reference CompareRunsHandler validation logic from Task 32\n\n4.   (Usage Guide):\n   - API example with run_ids in POST /evaluation/compare:\n     ```bash\n     POST /evaluation/compare\n     {\n       \"run_ids\": [\"run_abc123\", \"run_def456\", \"run_ghi789\"]\n     }\n     ```\n     Response:\n     ```json\n     {\n       \"dataset_id\": \"dataset_xyz\",\n       \"k\": 5,\n       \"aggregate_metrics\": [\n         {\n           \"run_id\": \"run_abc123\",\n           \"created_at\": \"2025-01-15T10:00:00Z\",\n           \"precision_at_k\": 0.72,\n           \"recall_at_k\": 0.68,\n           \"hit_rate_at_k\": 0.95,\n           \"mrr\": 0.65,\n           \"mean_faithfulness\": 0.85,\n           \"mean_answer_relevancy\": 0.78\n         },\n         ...\n       ],\n       \"test_case_comparisons\": [\n         {\n           \"test_case_id\": \"tc_123\",\n           \"question\": \"What is...\",\n           \"metrics\": [\n             {\"run_id\": \"run_abc123\", \"precision\": 0.8, \"recall\": 0.6, ...},\n             {\"run_id\": \"run_def456\", \"precision\": 0.6, \"recall\": 0.8, ...}\n           ]\n         },\n         ...\n       ]\n     }\n     ```\n   - CLI compare command with multiple run IDs:\n     ```bash\n     eval compare run_abc123 run_def456 run_ghi789\n     ```\n   - Show sample Rich table output with side-by-side metrics (from Task 33 CLI implementation):\n     ```\n     Run Comparison - Aggregate Metrics\n     \n      Run ID    Created At       Precision@k   Recall@k   Hit Rate@k    MRR  \n     \n      run_abc1  2025-01-15 10:00 0.7200        0.6800     0.9500        0.65 \n      run_def4  2025-01-15 11:00 0.7800        0.7200     0.9600        0.70 \n     \n     ```\n   Reference CompareRuns command schema and RunComparisonResponse from Task 32.\n\n5.    (Reading the Comparison):\n   - Aggregate metrics comparison:\n     * Identify which run performed better overall (higher precision, recall, etc.)\n     * Look for tradeoffs: e.g., Run A has higher precision but lower recall than Run B\n     * For FULL_RAG: Also compare faithfulness and relevancy across runs\n   - Per-test-case comparison:\n     * Identify which specific questions improved or regressed\n     * Find patterns: e.g., \"Run B improved PARAPHRASED questions but regressed FACTUAL\"\n     * Use test_case_comparisons to drill into individual question performance\n   - Decision making: Choose run with best aggregate metrics for production, or investigate anomalies in per-case comparisons\n\n6.   (Practical Workflow):\n   - Step-by-step guide for typical A/B test cycle:\n     1. Generate dataset once: `eval generate <notebook_id> --name baseline`\n     2. Run baseline evaluation: `eval run <dataset_id> --k 5`  Save run_id as baseline\n     3. Make system change (e.g., adjust chunking strategy)\n     4. Run evaluation with same dataset: `eval run <dataset_id> --k 5`  Save run_id as experiment\n     5. Compare runs: `eval compare <baseline_run_id> <experiment_run_id>`\n     6. Analyze results: Did metrics improve? Which test cases changed?\n     7. Iterate: Keep improvement or revert change based on data\n     8. Repeat: Make next change and compare again\n   - Best practices:\n     * Always use same dataset for fair comparison\n     * Run baseline before making any changes\n     * Test one change at a time to isolate impact\n     * Document what changed between runs (e.g., in run notes or external docs)\n     * Archive successful runs as new baselines for future comparisons\n\nReference Task 33 implementation for API endpoint, CLI command, and Rich table display.\nReference Task 32 implementation for CompareRunsHandler business logic.",
        "testStrategy": "1. Verify CompareRuns command schema matches Task 32 implementation\n2. Verify RunComparisonResponse structure matches Task 32 response schema\n3. Verify API endpoint path matches Task 33 implementation\n4. Verify CLI command name and options match Task 33 implementation\n5. Verify Rich table format matches Task 33 CLI output logic\n6. Validate example JSON request/response are valid according to schemas\n7. Cross-reference validation requirements with CompareRunsHandler from Task 32\n8. After Task 33 completion: Test API and CLI examples produce expected output\n9. Manual review: Korean technical accuracy and workflow practicality\n10. Ensure consistent terminology across all 4 documentation files",
        "priority": "medium",
        "dependencies": [
          "33"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T11:39:34.787Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-02-23T11:39:34.787Z",
      "taskCount": 37,
      "completedCount": 25,
      "tags": [
        "master"
      ]
    }
  }
}