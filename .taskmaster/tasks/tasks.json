{
  "master": {
    "tasks": [
      {
        "id": "38",
        "title": "Add cosine_similarity metric function",
        "description": "Implement pure cosine similarity calculation for embedding vector comparison",
        "details": "Create cosine_similarity(vec_a: list[float], vec_b: list[float]) -> float function in src/evaluation/domain/metric.py. Use numpy or pure Python implementation with dot product / (norm_a * norm_b). Returns value between -1.0 and 1.0. Add comprehensive unit tests covering edge cases (zero vectors, identical vectors, orthogonal vectors).",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_cosine.py with various vector pairs, edge cases, and numerical stability checks",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.021Z"
      },
      {
        "id": "39",
        "title": "Implement GroundTruthExpander adapter",
        "description": "Create adapter to expand ground truth with adjacent and semantically similar chunks",
        "details": "Create src/evaluation/adapter/ground_truth.py with GroundTruthExpander class. Implement: __init__(chunk_repository, embedding_provider), expand() method to process test cases, _expand_adjacent() for chunk_index ± 1 neighbors, _expand_semantic() using cosine similarity threshold (default 0.85). Use model_copy(update={...}) pattern for frozen TestCase instances. Load all chunks from notebook, group by document_id, expand each test case's ground_truth_chunk_ids, deduplicate results.",
        "testStrategy": "Unit tests in tests/evaluation/adapter/test_ground_truth.py verifying adjacent expansion logic, semantic similarity filtering, deduplication, and frozen model handling",
        "priority": "high",
        "dependencies": [
          "38"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "40",
        "title": "Add expand_ground_truth fields to domain models",
        "description": "Extend EvaluationDataset and GenerateDataset command with ground truth expansion options",
        "details": "Add to src/evaluation/domain/model.py EvaluationDataset: expand_ground_truth: bool = False, similarity_threshold: float = 0.85. Add to src/evaluation/schema/command.py GenerateDataset: expand_ground_truth: bool = False (default), similarity_threshold: float = Field(default=0.85, ge=0.5, le=1.0). Update DatasetSummary and DatasetDetail response schemas with expand_ground_truth field.",
        "testStrategy": "Verify field validation with pydantic, test threshold bounds, ensure backward compatibility with existing datasets",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "41",
        "title": "Create Alembic migration for ground truth expansion",
        "description": "Add database columns for ground truth expansion tracking",
        "details": "Generate Alembic migration adding to evaluation_datasets: expand_ground_truth (Boolean, default=False, nullable=False), similarity_threshold (Float, default=0.85, nullable=True). Update EvaluationDatasetSchema in src/infrastructure/models/evaluation.py with corresponding mapped_column definitions. Update DatasetMapper in src/evaluation/domain/mapper.py to handle new fields bidirectionally.",
        "testStrategy": "Test migration up/down, verify default values, check mapper round-trip conversion",
        "priority": "high",
        "dependencies": [
          "40"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "42",
        "title": "Integrate GroundTruthExpander into GenerateDatasetHandler",
        "description": "Add ground truth expansion logic to dataset generation flow",
        "details": "Modify src/evaluation/handler/handlers.py GenerateDatasetHandler: add ground_truth_expander: GroundTruthExpander | None to __init__ (optional dependency). In handle() method, after test_cases generation and before dataset.mark_completed(), check cmd.expand_ground_truth. If True, call expander.expand(test_cases, notebook_id, cmd.similarity_threshold) and use expanded result. Update DI in src/evaluation/dependency.py: add ground_truth_expander provider in EvaluationAdapterContainer, wire to generate_dataset_handler.",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_generate_dataset_expand.py with expansion on/off scenarios, verify expanded chunk IDs are persisted",
        "priority": "high",
        "dependencies": [
          "39",
          "41"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "43",
        "title": "Add CLI and API support for ground truth expansion",
        "description": "Expose ground truth expansion options in CLI and REST API",
        "details": "Update src/cli/commands/evaluation.py generate_dataset command: add --expand-ground-truth flag and --similarity-threshold float option. Update src/evaluation/entrypoint/api.py generate_dataset endpoint to accept and pass through expand_ground_truth and similarity_threshold from request body (automatically handled via GenerateDataset schema).",
        "testStrategy": "CLI integration test with expansion flags, API endpoint test verifying parameter passing",
        "priority": "medium",
        "dependencies": [
          "42"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "44",
        "title": "Add MULTI_HOP to QuestionDifficulty enum",
        "description": "Extend question difficulty classification with multi-hop category",
        "details": "Add MULTI_HOP = \"multi_hop\" to QuestionDifficulty StrEnum in src/evaluation/domain/model.py. No DB migration needed as difficulty column is String(20) nullable. Update any difficulty-related display logic in CLI/API responses to handle new value.",
        "testStrategy": "Verify enum value serialization, test difficulty filtering with MULTI_HOP",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.023Z"
      },
      {
        "id": "45",
        "title": "Implement MultiHopTestGenerator adapter",
        "description": "Create LLM-based multi-hop question generator requiring multiple chunks",
        "details": "Create src/evaluation/adapter/multi_hop_generator.py with MultiHopTestGenerator class. Use PydanticAI Agent with gpt-4o-mini. Implement: __init__(eval_model), generate_multi_hop_cases(chunks, max_cases) returns list[TestCase], _group_chunks_for_multi_hop() creates 2-3 chunk combinations (adjacent + distant within same document), _generate_multi_hop_question() with prompt: 'Generate question requiring ALL passages, not answerable from single passage. Return JSON: {question, difficulty: multi_hop}'. Set ground_truth_chunk_ids to all chunks in group.",
        "testStrategy": "Unit tests in tests/evaluation/adapter/test_multi_hop_generator.py mocking LLM responses, verify chunk grouping logic, validate output TestCase structure",
        "priority": "high",
        "dependencies": [
          "44"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "46",
        "title": "Add multi_hop configuration to GenerateDataset command",
        "description": "Extend dataset generation command with multi-hop question options",
        "details": "Add to src/evaluation/schema/command.py GenerateDataset: multi_hop_ratio: float = Field(default=0.0, ge=0.0, le=1.0, description='Multi-hop question ratio'), multi_hop_max_cases: int = Field(default=10, ge=1, le=50). Update CLI with --multi-hop-ratio and --multi-hop-max-cases options.",
        "testStrategy": "Validate ratio bounds, test multi-hop case count limits",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "47",
        "title": "Integrate MultiHopTestGenerator into GenerateDatasetHandler",
        "description": "Merge single-chunk and multi-hop questions during dataset generation",
        "details": "Update GenerateDatasetHandler: add multi_hop_generator: MultiHopTestGenerator dependency. In handle(), after single-chunk generation: if cmd.multi_hop_ratio > 0, generate multi-hop cases up to multi_hop_max_cases, merge with single-chunk cases respecting ratio, shuffle and take final set. Update DI: add multi_hop_generator Singleton provider in EvaluationAdapterContainer.",
        "testStrategy": "Handler tests verifying question ratio mixing, max case limits, proper difficulty assignment",
        "priority": "high",
        "dependencies": [
          "45",
          "46"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "48",
        "title": "Implement complete_context_rate metric for multi-hop",
        "description": "Create metric measuring retrieval of all ground truth chunks in top-k",
        "details": "Add to src/evaluation/domain/metric.py: complete_context_rate(retrieved_ids: list[str], relevant_ids: set[str], k: int) -> float. Return 1.0 if all relevant_ids present in retrieved_ids[:k], else 0.0. This is binary: perfect recall or fail. Add to DifficultyMetrics in response schema: complete_context_rate: float | None = None.",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_complete_context.py with partial/complete retrieval scenarios",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.024Z"
      },
      {
        "id": "49",
        "title": "Add complete_context_rate to multi-hop difficulty metrics",
        "description": "Calculate and display complete context rate for multi-hop test cases",
        "details": "Update GetRunHandler in src/evaluation/handler/handlers.py: when aggregating metrics by difficulty, for MULTI_HOP cases compute complete_context_rate using metric function. Store in DifficultyMetrics.complete_context_rate field. Update RunDetail response schema and CLI output to display this metric for multi-hop questions.",
        "testStrategy": "Handler test with multi-hop test cases, verify complete_context_rate calculation and display",
        "priority": "medium",
        "dependencies": [
          "48"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "50",
        "title": "Implement NDCG@k metric function",
        "description": "Add Normalized Discounted Cumulative Gain ranking quality metric",
        "details": "Add to src/evaluation/domain/metric.py: ndcg_at_k(retrieved_ids: list[str], relevant_ids: set[str], k: int) -> float. Binary relevance: rel=1 if relevant else 0. DCG = sum(rel_i / log2(i+1)) for i in 1..k. IDCG = sum(1 / log2(i+1)) for i in 1..min(k, |relevant_ids|). Return DCG/IDCG, handle IDCG=0 edge case (return 0.0).",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_ndcg_map.py with perfect/random/poor rankings",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.025Z"
      },
      {
        "id": "51",
        "title": "Implement MAP@k metric function",
        "description": "Add Mean Average Precision ranking quality metric",
        "details": "Add to src/evaluation/domain/metric.py: average_precision_at_k(retrieved_ids: list[str], relevant_ids: set[str], k: int) -> float. AP = (1/|relevant_ids|) * sum(precision@i * rel_i) for i in 1..k. Precision@i = count(relevant in top i) / i. Only sum where rel_i=1. Handle empty relevant_ids (return 0.0).",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_ndcg_map.py with various ranking quality scenarios",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.026Z"
      },
      {
        "id": "52",
        "title": "Add NDCG and MAP fields to domain models",
        "description": "Extend retrieval metrics with ranking quality measures",
        "details": "Add to src/evaluation/domain/model.py: RetrievalMetrics gets ndcg_at_k: float, map_at_k: float. TestCaseResult gets ndcg: float = 0.0, map_score: float = 0.0. CaseMetrics gets ndcg: float, map_score: float. EvaluationRun gets ndcg_at_k: float | None = None, map_at_k: float | None = None.",
        "testStrategy": "Model instantiation tests, verify frozen config, test default values",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.027Z"
      },
      {
        "id": "53",
        "title": "Create Alembic migration for NDCG and MAP",
        "description": "Add database columns for ranking metrics",
        "details": "Generate Alembic migration adding: evaluation_runs table gets ndcg_at_k (Float, nullable), map_at_k (Float, nullable). evaluation_test_case_results gets ndcg (Float, default=0.0), map_score (Float, default=0.0). Update EvaluationRunSchema and EvaluationTestCaseResultSchema in src/infrastructure/models/evaluation.py with mapped columns.",
        "testStrategy": "Migration up/down tests, verify nullable and default constraints",
        "priority": "high",
        "dependencies": [
          "52"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "54",
        "title": "Integrate NDCG and MAP into evaluation pipeline",
        "description": "Calculate and store NDCG and MAP metrics during evaluation runs",
        "details": "Update RunEvaluationHandler in src/evaluation/handler/handlers.py: in _compute_case_metrics(), call ndcg_at_k() and average_precision_at_k() to populate CaseMetrics. In _compute_aggregate_metrics(), average ndcg and map_score across cases for RetrievalMetrics. Update RunMapper in mapper.py to persist/load new fields. Update TestCaseResult.create() to accept ndcg and map_score.",
        "testStrategy": "Handler tests verifying metric calculation, aggregation, and persistence round-trip",
        "priority": "high",
        "dependencies": [
          "50",
          "51",
          "53"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "55",
        "title": "Update API responses and CLI for NDCG/MAP",
        "description": "Display ranking metrics in API responses and CLI output",
        "details": "Add to src/evaluation/schema/response.py: MetricsResponse gets ndcg_at_k, map_at_k. TestCaseResultResponse gets ndcg, map_score. RunComparisonMetrics gets ndcg_at_k, map_at_k. TestCaseComparisonEntry gets ndcg, map_score. DifficultyMetrics gets ndcg_at_k, map_at_k. Update CLI in src/cli/commands/evaluation.py to display NDCG@k and MAP@k in results tables.",
        "testStrategy": "API response schema validation, CLI output verification with test data",
        "priority": "medium",
        "dependencies": [
          "54"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "56",
        "title": "Implement retrieval score distribution metrics",
        "description": "Create functions to analyze score gaps between relevant and irrelevant chunks",
        "details": "Add to src/evaluation/domain/metric.py: score_gap(retrieved_ids, retrieved_scores, relevant_ids) returns mean score difference GT vs non-GT, None if either empty. high_confidence_rate(retrieved_ids, retrieved_scores, relevant_ids, margin=0.1) returns fraction where GT score > all non-GT scores by margin. mean_relevant_score() and mean_irrelevant_score() return mean scores for each group.",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_score_distribution.py with various score distributions",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.028Z"
      },
      {
        "id": "57",
        "title": "Add ScoreDistributionMetrics value object",
        "description": "Create domain model for score distribution analysis results",
        "details": "Add to src/evaluation/domain/model.py: ScoreDistributionMetrics(BaseModel, frozen=True) with fields: mean_score_gap: float | None, high_confidence_rate: float, mean_relevant_score: float, mean_irrelevant_score: float. Add ScoreDistributionResponse to response schema matching structure.",
        "testStrategy": "Model validation tests, frozen config verification",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.029Z"
      },
      {
        "id": "58",
        "title": "Calculate score distribution in GetRunHandler",
        "description": "Compute score distribution metrics at runtime from stored results",
        "details": "Update GetRunHandler.handle() in src/evaluation/handler/handlers.py: after loading run and dataset, for each test_case_result extract retrieved_ids, retrieved_scores, ground_truth_chunk_ids. Call score distribution metric functions per case, aggregate across run. Add score_distribution: ScoreDistributionResponse | None to RunDetail. No DB changes needed (runtime calculation).",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_get_run_score_distribution.py verifying calculation from stored data",
        "priority": "medium",
        "dependencies": [
          "56",
          "57"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "59",
        "title": "Display score distribution in CLI output",
        "description": "Add score distribution section to CLI evaluation results",
        "details": "Update src/cli/commands/evaluation.py get_run and compare_runs commands: if score_distribution present in response, display table with mean_score_gap, high_confidence_rate, mean_relevant_score, mean_irrelevant_score with appropriate formatting (2 decimal places, handle None gracefully).",
        "testStrategy": "CLI integration test with score distribution data",
        "priority": "low",
        "dependencies": [
          "58"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "60",
        "title": "Implement ChunkQualityMetrics domain models",
        "description": "Create models for chunk quality evaluation results",
        "details": "Add to src/evaluation/domain/model.py: ChunkQualityMetrics(BaseModel, frozen=True) with chunk_id: str, boundary_coherence: float (0-1), self_containment: float (0-1), information_density: float (0-1). ChunkQualityReport(BaseModel, frozen=True) with notebook_id, total_chunks_analyzed: int, mean metrics for all three dimensions, low_quality_chunk_ids: tuple[str, ...], created_at: datetime.",
        "testStrategy": "Model instantiation, field validation, frozen config tests",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.030Z"
      },
      {
        "id": "61",
        "title": "Implement ChunkQualityEvaluator adapter",
        "description": "Create LLM-based chunk quality evaluator using PydanticAI",
        "details": "Create src/evaluation/adapter/chunk_quality.py with ChunkQualityEvaluator class. Use PydanticAI Agent with eval_model. Implement: __init__(eval_model), evaluate_chunk(chunk: Chunk) -> ChunkQualityMetrics calls LLM with prompt 'Evaluate chunk quality: boundary_coherence, self_containment, information_density. Return JSON: {boundary_coherence: 0-1, self_containment: 0-1, information_density: 0-1, reasoning}'. evaluate_chunks(chunks, sample_size=30) randomly samples and batches evaluation.",
        "testStrategy": "Adapter tests in tests/evaluation/adapter/test_chunk_quality.py with mocked LLM, verify sampling logic",
        "priority": "medium",
        "dependencies": [
          "60"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "62",
        "title": "Create EvaluateChunkQuality command and response schemas",
        "description": "Add command schema for chunk quality evaluation",
        "details": "Add to src/evaluation/schema/command.py: EvaluateChunkQuality(BaseModel) with notebook_id: str, sample_size: int = Field(default=30, ge=5, le=200), low_quality_threshold: float = Field(default=0.5, ge=0.0, le=1.0). Add to response.py: ChunkQualityMetricsResponse and ChunkQualityReportResponse mirroring domain models.",
        "testStrategy": "Schema validation tests for field constraints",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "63",
        "title": "Implement EvaluateChunkQualityHandler",
        "description": "Create handler for chunk quality evaluation workflow",
        "details": "Add to src/evaluation/handler/handlers.py: EvaluateChunkQualityHandler with __init__(notebook_repository, document_repository, chunk_repository, chunk_quality_evaluator). handle(cmd: EvaluateChunkQuality) -> ChunkQualityReportResponse: verify notebook exists, load chunks, sample per cmd.sample_size, evaluate with evaluator, identify low_quality chunks below threshold, build report.",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_chunk_quality_handler.py with various quality distributions",
        "priority": "medium",
        "dependencies": [
          "61",
          "62"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "64",
        "title": "Add chunk quality API endpoint and CLI command",
        "description": "Expose chunk quality evaluation via REST API and CLI",
        "details": "Add to src/evaluation/entrypoint/api.py: POST /api/v1/notebooks/{notebook_id}/evaluation/chunk-quality endpoint accepting EvaluateChunkQuality body, returns ChunkQualityReportResponse. Add to src/cli/commands/evaluation.py: 'chunk-quality <notebook_id>' command with --sample-size and --low-quality-threshold options. Update DI in dependency.py: add chunk_quality_evaluator and evaluate_chunk_quality_handler providers.",
        "testStrategy": "API integration test, CLI execution test, verify DI wiring",
        "priority": "low",
        "dependencies": [
          "63"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "65",
        "title": "Implement embedding quality metric functions",
        "description": "Create functions for embedding space quality analysis",
        "details": "Add to src/evaluation/domain/metric.py: intra_document_similarity(embeddings_by_doc: dict[str, list[list[float]]]) -> float calculates mean cosine similarity within same document. inter_document_similarity() calculates mean across different documents. separation_ratio(intra, inter) -> float returns intra/inter (higher=better). adjacent_chunk_similarity(chunks_ordered: list[tuple[str, list[float]]]) -> float for consecutive chunks. Use existing cosine_similarity helper.",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_embedding.py with synthetic embedding data",
        "priority": "medium",
        "dependencies": [
          "38"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "66",
        "title": "Create EmbeddingQualityMetrics domain model",
        "description": "Add model for embedding space quality analysis results",
        "details": "Add to src/evaluation/domain/model.py: EmbeddingQualityMetrics(BaseModel, frozen=True) with intra_document_similarity: float, inter_document_similarity: float, separation_ratio: float, adjacent_chunk_similarity: float, total_documents: int, total_chunks: int. Add matching EmbeddingQualityResponse schema.",
        "testStrategy": "Model validation tests",
        "priority": "medium",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.031Z"
      },
      {
        "id": "67",
        "title": "Implement AnalyzeEmbeddingQualityHandler",
        "description": "Create handler for embedding quality analysis",
        "details": "Add to src/evaluation/handler/handlers.py: AnalyzeEmbeddingQualityHandler with __init__(notebook_repository, chunk_repository). handle(notebook_id: str) -> EmbeddingQualityResponse: verify notebook exists, load all chunks with embeddings, group by document_id, calculate all embedding quality metrics using metric functions, return response. Implement sampling optimization for large notebooks (e.g., max 100 chunks per document for inter-doc comparison).",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_embedding_quality_handler.py with various corpus structures",
        "priority": "medium",
        "dependencies": [
          "65",
          "66"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "68",
        "title": "Add embedding quality API endpoint and CLI command",
        "description": "Expose embedding quality analysis via REST API and CLI",
        "details": "Add to src/evaluation/entrypoint/api.py: GET /api/v1/notebooks/{notebook_id}/evaluation/embedding-quality endpoint returning EmbeddingQualityResponse. Add to src/cli/commands/evaluation.py: 'embedding-quality <notebook_id>' command. Update DI: add analyze_embedding_quality_handler provider.",
        "testStrategy": "API integration test, CLI execution test",
        "priority": "low",
        "dependencies": [
          "67"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "69",
        "title": "Implement CitationMetrics domain model",
        "description": "Create model for citation accuracy metrics",
        "details": "Add to src/evaluation/domain/model.py: CitationMetrics(BaseModel, frozen=True) with citation_precision: float (correct citations / total citations), citation_recall: float (cited relevant chunks / total relevant chunks), phantom_citation_count: int (citations to non-existent sources), total_citations: int.",
        "testStrategy": "Model validation tests",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.032Z"
      },
      {
        "id": "70",
        "title": "Implement structural citation validation metrics",
        "description": "Create functions for citation precision, recall, and phantom detection",
        "details": "Add to src/evaluation/domain/metric.py: citation_precision(cited_chunk_ids: list[str], relevant_chunk_ids: set[str]) -> float returns |cited ∩ relevant| / |cited|. citation_recall(cited_chunk_ids, relevant_chunk_ids) -> float returns |cited ∩ relevant| / |relevant|. phantom_citation_count(citation_indices: list[int], retrieved_chunk_count: int) -> int counts indices > retrieved_chunk_count.",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_citation.py with various citation patterns",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.033Z"
      },
      {
        "id": "71",
        "title": "Add citation support scoring to LLMJudge",
        "description": "Implement LLM-based semantic citation validation",
        "details": "Add to src/evaluation/adapter/judge.py LLMJudge: score_citation_support(claim_with_citation: str, cited_chunk_content: str) -> float method. Create CITATION_SUPPORT_SYSTEM_PROMPT and CITATION_SUPPORT_USER_TEMPLATE constants. Add _citation_agent PydanticAI Agent. Prompt evaluates if cited chunk genuinely supports the claim, returns JSON {score: 0-1, reasoning}. Parse and return score.",
        "testStrategy": "Adapter tests in tests/evaluation/adapter/test_judge_citation.py with mocked LLM responses",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "72",
        "title": "Add citation metrics fields to domain models",
        "description": "Extend TestCaseResult and aggregate models with citation metrics",
        "details": "Add to src/evaluation/domain/model.py TestCaseResult: citation_precision: float | None, citation_recall: float | None, phantom_citation_count: int | None, citation_support_score: float | None. Add to GenerationMetrics and EvaluationRun: mean_citation_precision: float | None, mean_citation_recall: float | None, mean_phantom_citation_count: float | None. Update TestCaseResult.create() to accept citation_metrics: CitationMetrics | None parameter.",
        "testStrategy": "Model tests verifying optional citation fields",
        "priority": "high",
        "dependencies": [
          "69"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "73",
        "title": "Create Alembic migration for citation metrics",
        "description": "Add database columns for citation accuracy tracking",
        "details": "Generate Alembic migration adding to evaluation_test_case_results: citation_precision (Float, nullable), citation_recall (Float, nullable), phantom_citation_count (Integer, nullable), citation_support_score (Float, nullable). Add to evaluation_runs: mean_citation_precision (Float, nullable), mean_citation_recall (Float, nullable), mean_phantom_citation_count (Float, nullable). Update ORM schemas and mappers.",
        "testStrategy": "Migration up/down tests, mapper round-trip verification",
        "priority": "high",
        "dependencies": [
          "72"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "74",
        "title": "Integrate citation evaluation into RAG evaluation pipeline",
        "description": "Calculate and store citation metrics during FULL_RAG evaluation",
        "details": "Update RunEvaluationHandler._evaluate_single_rag() in src/evaluation/handler/handlers.py: extract citations from answer_result.citations (Citation objects with chunk_id). Extract cited_chunk_ids. Call citation_precision(), citation_recall(), phantom_citation_count(). Optionally call score_citation_support() for each citation and average. Create CitationMetrics and pass to TestCaseResult.create(). Add aggregate_citation_metrics() function to metric.py for averaging across cases. Update _compute_aggregate_metrics() to include citation metrics.",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_evaluate_citation.py with various citation scenarios",
        "priority": "high",
        "dependencies": [
          "70",
          "71",
          "73"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "75",
        "title": "Update API responses and CLI for citation metrics",
        "description": "Display citation accuracy metrics in responses and CLI output",
        "details": "Add citation fields to src/evaluation/schema/response.py: TestCaseResultResponse gets citation_precision, citation_recall, phantom_citation_count, citation_support_score. RunDetail gets mean_citation_precision, mean_citation_recall, mean_phantom_citation_count. RunComparisonMetrics and TestCaseComparisonEntry get citation fields. Update CLI to display Citation Accuracy section in results.",
        "testStrategy": "Response schema validation, CLI output verification",
        "priority": "medium",
        "dependencies": [
          "74"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "76",
        "title": "Implement ClaimVerdict and ClaimAnalysis domain models",
        "description": "Create models for structured hallucination analysis",
        "details": "Add to src/evaluation/domain/model.py: ClaimVerdict(StrEnum) with values SUPPORTED, PARTIALLY_SUPPORTED, CONTRADICTED, FABRICATED, UNVERIFIABLE. ClaimAnalysis(BaseModel, frozen=True) with claim_text: str, verdict: ClaimVerdict, supporting_chunk_indices: tuple[int, ...], reasoning: str. HallucinationAnalysis(BaseModel, frozen=True) with claims: tuple[ClaimAnalysis, ...], total_claims: int, counts per verdict type, hallucination_rate: float, faithfulness_score: float (derived).",
        "testStrategy": "Model validation tests in tests/evaluation/domain/test_hallucination_model.py",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-23T14:04:01.034Z"
      },
      {
        "id": "77",
        "title": "Add hallucination analysis to LLMJudge",
        "description": "Implement claim-level hallucination detection with LLM",
        "details": "Add to src/evaluation/adapter/judge.py LLMJudge: analyze_hallucinations(question: str, answer: str, context_chunks: list[Chunk]) -> HallucinationAnalysis method. Create HALLUCINATION_SYSTEM_PROMPT and HALLUCINATION_USER_TEMPLATE. Add _hallucination_agent. Prompt decomposes answer into atomic claims, verifies each against context, returns JSON {claims: [{claim_text, verdict, supporting_chunks, reasoning}, ...]}. Parse into HallucinationAnalysis with computed counts and rates.",
        "testStrategy": "Adapter tests in tests/evaluation/adapter/test_judge_hallucination.py with various hallucination types",
        "priority": "high",
        "dependencies": [
          "76"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "78",
        "title": "Add hallucination analysis fields to domain models",
        "description": "Extend TestCaseResult with structured hallucination metrics",
        "details": "Add to src/evaluation/domain/model.py TestCaseResult: hallucination_rate: float | None, contradiction_count: int | None, fabrication_count: int | None, total_claims: int | None, claim_analyses_json: str | None (JSON serialized ClaimAnalysis list). Add to GenerationMetrics/EvaluationRun: mean_hallucination_rate: float | None, total_contradictions: int | None, total_fabrications: int | None.",
        "testStrategy": "Model tests, JSON serialization/deserialization tests",
        "priority": "high",
        "dependencies": [
          "76"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "79",
        "title": "Create Alembic migration for hallucination analysis",
        "description": "Add database columns for claim-level hallucination tracking",
        "details": "Generate Alembic migration adding to evaluation_test_case_results: hallucination_rate (Float, nullable), contradiction_count (Integer, nullable), fabrication_count (Integer, nullable), total_claims (Integer, nullable), claim_analyses_json (Text, nullable). Add to evaluation_runs: mean_hallucination_rate (Float, nullable), total_contradictions (Integer, nullable), total_fabrications (Integer, nullable). Update ORM schemas and mappers.",
        "testStrategy": "Migration up/down tests, verify JSON text column storage",
        "priority": "high",
        "dependencies": [
          "78"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "80",
        "title": "Integrate hallucination analysis into RAG evaluation",
        "description": "Replace or augment faithfulness scoring with structured claim analysis",
        "details": "Update RunEvaluationHandler._evaluate_single_rag(): call llm_judge.analyze_hallucinations() instead of (or alongside) score_faithfulness(). Use hallucination_analysis.faithfulness_score for backward compatibility. Store hallucination_rate, contradiction_count, fabrication_count, total_claims. Serialize claims to JSON for claim_analyses_json. Aggregate in _compute_aggregate_metrics(): sum contradictions/fabrications, average hallucination_rate.",
        "testStrategy": "Handler tests verifying claim decomposition, verdict classification, aggregation logic",
        "priority": "high",
        "dependencies": [
          "77",
          "79"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "81",
        "title": "Update API responses and CLI for hallucination analysis",
        "description": "Display structured hallucination metrics in responses and CLI",
        "details": "Add to src/evaluation/schema/response.py: ClaimAnalysisResponse model, TestCaseResultResponse gets hallucination fields, RunDetail gets mean_hallucination_rate and totals. Update CLI to display Hallucination Analysis section with claim breakdown table showing verdict distribution.",
        "testStrategy": "Response schema validation, CLI formatted output test",
        "priority": "medium",
        "dependencies": [
          "80"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "82",
        "title": "Add answer completeness scoring to LLMJudge",
        "description": "Implement LLM-based answer completeness evaluation",
        "details": "Add to src/evaluation/adapter/judge.py LLMJudge: score_answer_completeness(question: str, answer: str, context_chunks: list[Chunk]) -> float method. Create COMPLETENESS_SYSTEM_PROMPT and COMPLETENESS_USER_TEMPLATE. Add _completeness_agent. Prompt evaluates if answer comprehensively uses all relevant context information, returns JSON {score: 0-1, reasoning}.",
        "testStrategy": "Adapter tests in tests/evaluation/adapter/test_judge_completeness.py with partial/complete answers",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "83",
        "title": "Add answer_completeness fields to models and DB",
        "description": "Extend models with answer completeness metric",
        "details": "Add to src/evaluation/domain/model.py TestCaseResult: answer_completeness: float | None. Add to GenerationMetrics/EvaluationRun: mean_answer_completeness: float | None. Create Alembic migration adding answer_completeness (Float, nullable) to evaluation_test_case_results and mean_answer_completeness to evaluation_runs. Update ORM schemas and mappers.",
        "testStrategy": "Migration tests, model field validation",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "84",
        "title": "Integrate answer completeness into RAG evaluation",
        "description": "Calculate and store answer completeness during evaluation",
        "details": "Update RunEvaluationHandler._evaluate_single_rag(): call llm_judge.score_answer_completeness() alongside faithfulness and relevancy. Store in TestCaseResult.answer_completeness. Update _compute_aggregate_metrics() to average completeness across cases. Update GenerationCaseMetrics to include completeness.",
        "testStrategy": "Handler tests verifying completeness calculation and aggregation",
        "priority": "medium",
        "dependencies": [
          "82",
          "83"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "85",
        "title": "Update API responses and CLI for answer completeness",
        "description": "Display answer completeness metric in responses and CLI",
        "details": "Add answer_completeness to TestCaseResultResponse, mean_answer_completeness to RunDetail and RunComparisonMetrics in src/evaluation/schema/response.py. Update CLI output to display Answer Completeness metric.",
        "testStrategy": "Response schema validation, CLI display test",
        "priority": "low",
        "dependencies": [
          "84"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "86",
        "title": "Implement answer_consistency metric function",
        "description": "Create metric for measuring answer consistency across runs",
        "details": "Add to src/evaluation/domain/metric.py: answer_consistency(embeddings: list[list[float]]) -> float. Calculates mean pairwise cosine similarity across all embedding pairs. For n answers, computes n*(n-1)/2 pairwise similarities and averages. Use existing cosine_similarity helper. Returns 0.0 for single answer.",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_consistency.py with identical/diverse answer embeddings",
        "priority": "medium",
        "dependencies": [
          "38"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "87",
        "title": "Integrate answer consistency into CompareRunsHandler",
        "description": "Calculate answer consistency when comparing FULL_RAG runs",
        "details": "Update CompareRunsHandler in src/evaluation/handler/handlers.py: add embedding_provider: EmbeddingProviderPort to __init__ (from chunk_adapter). In handle(), for FULL_RAG runs on same dataset, group test_case_results by test_case_id. For each group, collect generated_answers, embed with embedding_provider, call answer_consistency(). Calculate per-case and mean consistency. Add to response.",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_compare_consistency.py with multiple runs",
        "priority": "medium",
        "dependencies": [
          "86"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "88",
        "title": "Update comparison responses and CLI for answer consistency",
        "description": "Display answer consistency in run comparison results",
        "details": "Add to src/evaluation/schema/response.py: TestCaseComparison gets answer_consistency: float | None, RunComparisonResponse gets mean_answer_consistency: float | None. Update CLI compare_runs command to display Answer Consistency section. Update DI to inject embedding_provider into CompareRunsHandler.",
        "testStrategy": "Response schema validation, CLI display test",
        "priority": "low",
        "dependencies": [
          "87"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "89",
        "title": "Implement correlation analysis metric functions",
        "description": "Create functions for error propagation analysis",
        "details": "Add to src/evaluation/domain/metric.py: pearson_correlation(xs: list[float], ys: list[float]) -> float | None calculates Pearson r, returns None if <3 points. bucket_generation_quality(results: list[tuple[float, float, float]]) -> dict[str, tuple[float, float]] takes (recall, faithfulness, relevancy) tuples, buckets by recall (1.0='perfect', 0<r<1='partial', 0='missed'), returns mean faithfulness and relevancy per bucket.",
        "testStrategy": "Unit tests in tests/evaluation/domain/test_metric_correlation.py with synthetic data",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "90",
        "title": "Create ErrorPropagationAnalysis domain models",
        "description": "Add models for error propagation analysis results",
        "details": "Add to src/evaluation/domain/model.py: RetrievalBucketMetrics(BaseModel, frozen=True) with bucket: str, test_case_count: int, mean_faithfulness: float, mean_relevancy: float. ErrorPropagationAnalysis(BaseModel, frozen=True) with recall_faithfulness_correlation: float | None, recall_relevancy_correlation: float | None, bucket_metrics: tuple[RetrievalBucketMetrics, ...]. Add matching response schemas.",
        "testStrategy": "Model validation tests",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "91",
        "title": "Calculate error propagation in GetRunHandler",
        "description": "Analyze retrieval-generation correlation at runtime from stored results",
        "details": "Update GetRunHandler.handle(): for FULL_RAG runs, extract (recall, faithfulness, relevancy) from each test_case_result. Call pearson_correlation() for recall-faithfulness and recall-relevancy pairs. Call bucket_generation_quality() to create bucket metrics. Build ErrorPropagationAnalysis and add to RunDetail as error_propagation: ErrorPropagationResponse | None. No DB changes (runtime calculation).",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_get_run_error_propagation.py with various correlation patterns",
        "priority": "medium",
        "dependencies": [
          "89",
          "90"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "92",
        "title": "Display error propagation analysis in CLI",
        "description": "Add error propagation section to CLI FULL_RAG results",
        "details": "Update src/cli/commands/evaluation.py get_run command: if error_propagation present, display correlation coefficients and bucket metrics table showing test_case_count and mean generation quality per retrieval bucket.",
        "testStrategy": "CLI display test with error propagation data",
        "priority": "low",
        "dependencies": [
          "91"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "93",
        "title": "Create RunCostMetrics domain model and add cost tracking fields",
        "description": "Add models for token usage and cost tracking",
        "details": "Add to src/evaluation/domain/model.py: RunCostMetrics(BaseModel, frozen=True) with total_input_tokens: int, total_output_tokens: int, total_tokens: int, estimated_cost_usd: float, mean_latency_ms: float | None. Add to EvaluationRun: total_input_tokens: int | None, total_output_tokens: int | None, estimated_cost_usd: float | None. Create matching response schema.",
        "testStrategy": "Model validation tests",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "94",
        "title": "Create Alembic migration for cost tracking",
        "description": "Add database columns for token usage and cost",
        "details": "Generate Alembic migration adding to evaluation_runs: total_input_tokens (Integer, nullable), total_output_tokens (Integer, nullable), estimated_cost_usd (Float, nullable). Update EvaluationRunSchema and RunMapper.",
        "testStrategy": "Migration up/down tests",
        "priority": "medium",
        "dependencies": [
          "93"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "95",
        "title": "Integrate cost tracking into RunEvaluationHandler",
        "description": "Track token usage and calculate costs during evaluation",
        "details": "Update RunEvaluationHandler: in _evaluate_single_rag(), extract usage from PydanticAI result.usage() for each LLM call (faithfulness, relevancy, hallucination, completeness, citation). Accumulate tokens. In _evaluate_full_rag(), sum totals. Define TOKEN_COSTS dict mapping model names to (input_per_1k, output_per_1k) prices. Calculate estimated_cost_usd. Store in EvaluationRun before mark_completed().",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_cost_tracking.py verifying token accumulation and cost calculation",
        "priority": "medium",
        "dependencies": [
          "94"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "96",
        "title": "Update API responses and CLI for cost metrics",
        "description": "Display cost metrics in responses and CLI output",
        "details": "Add cost_metrics: RunCostMetricsResponse | None to RunDetail in response schema. Add estimated_cost_usd to RunComparisonMetrics. Update CLI to display Cost section with token counts and USD estimate.",
        "testStrategy": "Response schema validation, CLI display test",
        "priority": "low",
        "dependencies": [
          "95"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "97",
        "title": "Add generation_model field to models and command",
        "description": "Support specifying generation model for multi-model evaluation",
        "details": "Add to src/evaluation/schema/command.py RunEvaluation: generation_model: str | None = Field(default=None, description='Override generation model'). Add to src/evaluation/domain/model.py EvaluationRun: generation_model: str | None = None.",
        "testStrategy": "Schema validation, model field tests",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "98",
        "title": "Create Alembic migration for generation_model",
        "description": "Add database column for generation model tracking",
        "details": "Generate Alembic migration adding to evaluation_runs: generation_model (String(100), nullable). Update EvaluationRunSchema and RunMapper.",
        "testStrategy": "Migration up/down tests",
        "priority": "medium",
        "dependencies": [
          "97"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "99",
        "title": "Implement model override in RunEvaluationHandler",
        "description": "Support using alternative generation models during evaluation",
        "details": "Update RunEvaluationHandler._evaluate_full_rag(): if cmd.generation_model is specified, create temporary RAGAgent instance with that model (via factory or init parameter). Use temp agent for answer generation instead of self._rag_agent. Store model name in EvaluationRun.generation_model. Ensure proper cleanup of temp agent resources.",
        "testStrategy": "Handler tests in tests/evaluation/handler/test_multi_model.py with different model specifications",
        "priority": "medium",
        "dependencies": [
          "98"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "100",
        "title": "Update comparison and CLI for multi-model support",
        "description": "Display generation model in comparison and results",
        "details": "Add generation_model: str | None to RunComparisonMetrics in response schema. Update RunDetail to include generation_model. Update CLI compare_runs and get_run commands to display model name. Add --generation-model option to CLI run evaluation command.",
        "testStrategy": "CLI option test, display verification",
        "priority": "low",
        "dependencies": [
          "99"
        ],
        "status": "pending",
        "subtasks": []
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-02-23T14:04:01.034Z",
      "taskCount": 63,
      "completedCount": 13,
      "tags": [
        "master"
      ]
    }
  }
}