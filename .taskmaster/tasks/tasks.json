{
  "master": {
    "tasks": [
      {
        "id": "1",
        "title": "Add API Endpoint Documentation for Notebook Module",
        "description": "Update all notebook API endpoint decorators to include summary, description, and responses documentation following DDD architecture rules with HTTPStatus constants.",
        "details": "For each endpoint in src/notebook/entrypoint/api.py:\n\n1. Add or improve `summary` parameter in @router.post/get/patch/delete decorators\n2. Add or improve detailed `description` parameter \n3. Add `responses` parameter documenting all possible response scenarios with proper HTTP status codes\n4. Ensure all status codes use `http.HTTPStatus` constants rather than hardcoded numbers\n5. Document error responses (404 for not found, 400 for validation errors)\n\nEndpoints to update:\n- POST /notebooks (create_notebook)\n- GET /notebooks (list_notebooks)\n- GET /notebooks/{notebook_id} (get_notebook)\n- PATCH /notebooks/{notebook_id} (update_notebook)\n- DELETE /notebooks/{notebook_id} (delete_notebook)\n\nExample implementation:\n```python\n@router.get(\n    \"/{notebook_id}\",\n    response_model=response.NotebookDetail,\n    summary=\"Get notebook details\",\n    description=\"Retrieve detailed information about a specific notebook by its ID.\",\n    responses={\n        http.HTTPStatus.OK: {\"description\": \"Notebook details retrieved successfully\"},\n        http.HTTPStatus.NOT_FOUND: {\"description\": \"Notebook not found\"},\n    },\n)\n```",
        "testStrategy": "1. Manual review of the code to ensure all endpoints have proper documentation\n2. Verify that the FastAPI auto-generated Swagger documentation displays correct information\n3. Validate that responses include all status codes with proper descriptions\n4. Check that HTTPStatus constants are used consistently\n5. Run the API server and access the /docs endpoint to verify documentation renders correctly",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "2",
        "title": "Add API Endpoint Documentation for Document Module",
        "description": "Update document/source API endpoint decorators to include summary, description, and responses documentation with HTTPStatus constants.",
        "details": "For each endpoint in src/document/entrypoint/api.py:\n\n1. Add or improve `summary` parameter in @router.post/get decorators\n2. Add or improve detailed `description` parameter \n3. Add `responses` parameter documenting all possible response scenarios\n4. Ensure all status codes use `http.HTTPStatus` constants\n5. Document error responses (404 for not found, 400 for validation errors, 409 for duplicate URL)\n\nEndpoints to update:\n- POST /notebooks/{notebook_id}/sources (add_source)\n- GET /notebooks/{notebook_id}/sources (list_sources)\n- GET /documents/{document_id} (get_document)\n\nExample implementation:\n```python\n@router.post(\n    \"\",\n    response_model=response.DocumentId,\n    status_code=http.HTTPStatus.CREATED,\n    summary=\"Add source URL to notebook\",\n    description=\"Add a new source URL to a notebook for ingestion and analysis.\",\n    responses={\n        http.HTTPStatus.CREATED: {\"description\": \"Source created successfully\"},\n        http.HTTPStatus.NOT_FOUND: {\"description\": \"Notebook not found\"},\n        http.HTTPStatus.CONFLICT: {\"description\": \"Source URL already exists in notebook\"},\n    },\n)\n```",
        "testStrategy": "1. Manual review of the code to ensure all endpoints have proper documentation\n2. Verify that the FastAPI auto-generated Swagger documentation displays correct information\n3. Validate that responses include all status codes with proper descriptions\n4. Check that HTTPStatus constants are used consistently\n5. Run the API server and access the /docs endpoint to verify documentation renders correctly",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "3",
        "title": "Implement Delete Document/Source Feature",
        "description": "Create a complete feature for deleting source documents from notebooks, including handler, API endpoint, and CLI command with appropriate cleanup of associated chunks.",
        "details": "1. Create DeleteSourceHandler in src/document/handler/handlers.py:\n```python\nclass DeleteSourceHandler:\n    \"\"\"Handler for deleting source documents from notebooks.\"\"\"\n\n    def __init__(\n        self,\n        document_repository: document_repository_module.DocumentRepository,\n        notebook_repository: notebook_repository_module.NotebookRepository,\n        chunk_repository: chunk_repository_module.ChunkRepository,\n    ) -> None:\n        self._document_repository = document_repository\n        self._notebook_repository = notebook_repository\n        self._chunk_repository = chunk_repository\n\n    async def handle(self, notebook_id: str, document_id: str) -> None:\n        \"\"\"Delete a source document from a notebook.\"\"\"\n        # Verify notebook exists\n        notebook = await self._notebook_repository.find_by_id(notebook_id)\n        if notebook is None:\n            raise exceptions.NotFoundError(f\"Notebook not found: {notebook_id}\")\n\n        # Verify document exists and belongs to notebook\n        document = await self._document_repository.find_by_id(document_id)\n        if document is None:\n            raise exceptions.NotFoundError(f\"Document not found: {document_id}\")\n        \n        if document.notebook_id != notebook_id:\n            raise exceptions.ValidationError(\n                f\"Document {document_id} does not belong to notebook {notebook_id}\"\n            )\n\n        # Delete associated chunks first\n        await self._chunk_repository.delete_by_document(document_id)\n        \n        # Delete document\n        await self._document_repository.delete(document_id)\n```\n\n2. Add handler to src/document/handler/__init__.py exports\n\n3. Add endpoint to src/document/entrypoint/api.py:\n```python\n@router.delete(\n    \"/{document_id}\",\n    status_code=http.HTTPStatus.NO_CONTENT,\n    summary=\"Delete source document\",\n    description=\"Delete a source document and its associated chunks from a notebook.\",\n    responses={\n        http.HTTPStatus.NO_CONTENT: {\"description\": \"Document deleted successfully\"},\n        http.HTTPStatus.NOT_FOUND: {\"description\": \"Notebook or document not found\"},\n        http.HTTPStatus.BAD_REQUEST: {\"description\": \"Document does not belong to notebook\"},\n    },\n)\n@inject\nasync def delete_source(\n    notebook_id: str,\n    document_id: str,\n    handler: handlers.DeleteSourceHandler = fastapi.Depends(\n        Provide[container_module.ApplicationContainer.document.handler.delete_source_handler]\n    ),\n) -> None:\n    \"\"\"Delete a source document from a notebook.\"\"\"\n    await handler.handle(notebook_id, document_id)\n```\n\n4. Update container to register the handler\n\n5. Add CLI command in src/cli/commands/source.py:\n```python\n@app.command(\"delete\")\ndef delete_source(\n    notebook_id: str = typer.Argument(..., help=\"Notebook ID\"),\n    document_id: str = typer.Argument(..., help=\"Document ID\"),\n    force: bool = typer.Option(False, \"--force\", \"-f\", help=\"Skip confirmation\"),\n):\n    \"\"\"Delete a source document from a notebook.\"\"\"\n    asyncio.run(_delete_source(notebook_id, document_id, force))\n\n\nasync def _delete_source(notebook_id: str, document_id: str, force: bool):\n    from src.document.adapter.repository import DocumentRepository\n    from src.chunk.adapter.repository import ChunkRepository\n    from src.notebook.adapter.repository import NotebookRepository\n\n    async with get_session_context() as session:\n        # Verify notebook exists\n        notebook_repo = NotebookRepository(session)\n        notebook = await notebook_repo.find_by_id(notebook_id)\n        if notebook is None:\n            console.print(f\"[red]Notebook not found:[/red] {notebook_id}\")\n            raise typer.Exit(1)\n\n        # Verify document exists\n        document_repo = DocumentRepository(session)\n        document = await document_repo.find_by_id(document_id)\n        if document is None:\n            console.print(f\"[red]Document not found:[/red] {document_id}\")\n            raise typer.Exit(1)\n            \n        if document.notebook_id != notebook_id:\n            console.print(f\"[red]Document does not belong to notebook:[/red] {document_id}\")\n            raise typer.Exit(1)\n\n        if not force:\n            confirm = typer.confirm(\n                f\"Delete document '{document.title or document.url}'? This will also delete all associated chunks.\"\n            )\n            if not confirm:\n                raise typer.Abort()\n\n        # Delete chunks first\n        chunk_repo = ChunkRepository(session)\n        chunks = await chunk_repo.list_by_document(document_id)\n        chunk_count = len(chunks)\n        await chunk_repo.delete_by_document(document_id)\n        \n        # Delete document\n        await document_repo.delete(document_id)\n        await session.commit()\n\n        console.print(f\"[green]Document deleted:[/green] {document_id}\")\n        console.print(f\"[green]{chunk_count} chunks deleted[/green]\")\n```\n\n6. Add the delete_by_document method to ChunkRepository if not already present",
        "testStrategy": "1. Create unit tests for DeleteSourceHandler:\n   - Test happy path (successful deletion)\n   - Test not found notebook\n   - Test not found document\n   - Test document not belonging to notebook\n\n2. Create integration test for the API endpoint:\n   - Test successful deletion and verify 204 status code\n   - Test cascade deletion of chunks\n   - Test error responses for invalid IDs\n   \n3. Manual testing of CLI command with various scenarios",
        "priority": "high",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "4",
        "title": "Implement Notebook Handler Unit Tests",
        "description": "Add comprehensive unit tests for all notebook handlers using mock repositories following the Arrange-Act-Assert pattern.",
        "details": "Create tests/notebook/test_handlers.py with tests for all notebook handlers:\n\n```python\n\"\"\"Tests for Notebook handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.notebook.domain.model import Notebook\nfrom src.notebook.handler.handlers import (\n    CreateNotebookHandler,\n    GetNotebookHandler,\n    ListNotebooksHandler,\n    UpdateNotebookHandler,\n    DeleteNotebookHandler,\n)\nfrom src.notebook.schema.command import CreateNotebook, UpdateNotebook\n\n\nclass TestCreateNotebookHandler:\n    \"\"\"Tests for CreateNotebookHandler.\"\"\"\n\n    def test_create_notebook_success(self):\n        \"\"\"Test successful notebook creation.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.save.return_value = Notebook.create(name=\"Test Notebook\")\n        handler = CreateNotebookHandler(mock_repo)\n        cmd = CreateNotebook(name=\"Test Notebook\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(cmd))\n\n        # Assert\n        assert result.id is not None\n        mock_repo.save.assert_called_once()\n        saved_notebook = mock_repo.save.call_args[0][0]\n        assert saved_notebook.name == \"Test Notebook\"\n\n\nclass TestGetNotebookHandler:\n    \"\"\"Tests for GetNotebookHandler.\"\"\"\n\n    def test_get_notebook_success(self):\n        \"\"\"Test successful notebook retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_repo.find_by_id.return_value = notebook\n        handler = GetNotebookHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(notebook.id)\n        assert result.id == notebook.id\n        assert result.name == notebook.name\n\n    def test_get_notebook_not_found(self):\n        \"\"\"Test notebook not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetNotebookHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListNotebooksHandler:\n    \"\"\"Tests for ListNotebooksHandler.\"\"\"\n\n    def test_list_notebooks_success(self):\n        \"\"\"Test successful notebook listing.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        notebooks = [\n            Notebook.create(name=\"Notebook 1\"),\n            Notebook.create(name=\"Notebook 2\"),\n        ]\n        mock_result = MagicMock()\n        mock_result.items = notebooks\n        mock_result.total = len(notebooks)\n        mock_result.page = 1\n        mock_result.size = 10\n        mock_repo.list.return_value = mock_result\n        handler = ListNotebooksHandler(mock_repo)\n        from src.notebook.schema.query import ListNotebooks\n        query = ListNotebooks(page=1, size=10)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(query))\n\n        # Assert\n        mock_repo.list.assert_called_once()\n        assert result.total == 2\n        assert len(result.items) == 2\n        assert result.items[0].name == \"Notebook 1\"\n        assert result.items[1].name == \"Notebook 2\"\n        assert result.page == 1\n        assert result.size == 10\n\n\nclass TestUpdateNotebookHandler:\n    \"\"\"Tests for UpdateNotebookHandler.\"\"\"\n\n    def test_update_notebook_success(self):\n        \"\"\"Test successful notebook update.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        original = Notebook.create(name=\"Original\")\n        updated = original.update(name=\"Updated\")\n        mock_repo.find_by_id.return_value = original\n        mock_repo.save.return_value = updated\n        handler = UpdateNotebookHandler(mock_repo)\n        cmd = UpdateNotebook(name=\"Updated\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(original.id, cmd))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(original.id)\n        mock_repo.save.assert_called_once()\n        assert result.name == \"Updated\"\n        assert result.id == original.id\n\n    def test_update_notebook_not_found(self):\n        \"\"\"Test update notebook not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = UpdateNotebookHandler(mock_repo)\n        cmd = UpdateNotebook(name=\"Updated\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n\nclass TestDeleteNotebookHandler:\n    \"\"\"Tests for DeleteNotebookHandler.\"\"\"\n\n    def test_delete_notebook_success(self):\n        \"\"\"Test successful notebook deletion.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        notebook = Notebook.create(name=\"Test\")\n        mock_repo.find_by_id.return_value = notebook\n        handler = DeleteNotebookHandler(mock_repo)\n\n        # Act\n        pytest.asyncio.run(handler.handle(notebook.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_repo.delete.assert_called_once_with(notebook.id)\n\n    def test_delete_notebook_not_found(self):\n        \"\"\"Test delete notebook not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = DeleteNotebookHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the notebook handler module\n2. Verify tests for all happy path scenarios\n3. Verify tests for all error cases (not found, validation errors)\n4. Confirm that tests follow the Arrange-Act-Assert pattern\n5. Validate that tests verify the immutability of domain models",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "5",
        "title": "Implement Document Handler Unit Tests",
        "description": "Add comprehensive unit tests for all document handlers, including the new delete handler, with mock repositories.",
        "details": "Create tests/document/test_handlers.py with tests for all document handlers:\n\n```python\n\"\"\"Tests for Document handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.document.domain.model import Document\nfrom src.notebook.domain.model import Notebook\nfrom src.document.handler.handlers import (\n    AddSourceHandler,\n    GetDocumentHandler,\n    ListSourcesHandler,\n    DeleteSourceHandler,  # New handler\n)\nfrom src.document.schema.command import AddSource\n\n\nclass TestAddSourceHandler:\n    \"\"\"Tests for AddSourceHandler.\"\"\"\n\n    def test_add_source_success(self):\n        \"\"\"Test successful source addition.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_ingestion = MagicMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        document = Document.create(notebook_id=notebook.id, url=\"https://example.com\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_doc_repo.find_by_notebook_and_url.return_value = None\n        mock_doc_repo.save.return_value = document\n        \n        handler = AddSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            background_ingestion=mock_ingestion,\n        )\n        \n        cmd = AddSource(url=\"https://example.com\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_doc_repo.find_by_notebook_and_url.assert_called_once()\n        mock_doc_repo.save.assert_called_once()\n        mock_ingestion.trigger_ingestion.assert_called_once_with(document)\n        assert result.id == document.id\n\n    def test_add_source_notebook_not_found(self):\n        \"\"\"Test adding source to non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_ingestion = MagicMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = AddSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            background_ingestion=mock_ingestion,\n        )\n        \n        cmd = AddSource(url=\"https://example.com\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_add_source_duplicate_url(self):\n        \"\"\"Test adding duplicate URL raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_ingestion = MagicMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        existing_document = Document.create(notebook_id=notebook.id, url=\"https://example.com\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_doc_repo.find_by_notebook_and_url.return_value = existing_document\n        \n        handler = AddSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            background_ingestion=mock_ingestion,\n        )\n        \n        cmd = AddSource(url=\"https://example.com\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.ValidationError):\n            pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n\nclass TestGetDocumentHandler:\n    \"\"\"Tests for GetDocumentHandler.\"\"\"\n\n    def test_get_document_success(self):\n        \"\"\"Test successful document retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        document = Document.create(notebook_id=\"notebook123\", url=\"https://example.com\")\n        mock_repo.find_by_id.return_value = document\n        handler = GetDocumentHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(document.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(document.id)\n        assert result.id == document.id\n        assert result.url == document.url\n\n    def test_get_document_not_found(self):\n        \"\"\"Test document not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetDocumentHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListSourcesHandler:\n    \"\"\"Tests for ListSourcesHandler.\"\"\"\n\n    def test_list_sources_success(self):\n        \"\"\"Test successful sources listing.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        documents = [\n            Document.create(notebook_id=notebook.id, url=\"https://example1.com\"),\n            Document.create(notebook_id=notebook.id, url=\"https://example2.com\"),\n        ]\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        mock_result = MagicMock()\n        mock_result.items = documents\n        mock_result.total = len(documents)\n        mock_result.page = 1\n        mock_result.size = 10\n        \n        mock_doc_repo.list_by_notebook.return_value = mock_result\n        \n        handler = ListSourcesHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        from src.document.schema.query import ListSources\n        query = ListSources(notebook_id=notebook.id, page=1, size=10)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(query))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_doc_repo.list_by_notebook.assert_called_once_with(notebook.id, query)\n        assert result.total == 2\n        assert len(result.items) == 2\n        assert result.items[0].url == \"https://example1.com\"\n        assert result.items[1].url == \"https://example2.com\"\n\n    def test_list_sources_notebook_not_found(self):\n        \"\"\"Test listing sources for non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = ListSourcesHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        from src.document.schema.query import ListSources\n        query = ListSources(notebook_id=\"nonexistent\", page=1, size=10)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(query))\n\n\nclass TestDeleteSourceHandler:\n    \"\"\"Tests for DeleteSourceHandler.\"\"\"\n\n    def test_delete_source_success(self):\n        \"\"\"Test successful source deletion.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        document = Document.create(notebook_id=notebook.id, url=\"https://example.com\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_doc_repo.find_by_id.return_value = document\n        \n        handler = DeleteSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n        )\n\n        # Act\n        pytest.asyncio.run(handler.handle(notebook.id, document.id))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_doc_repo.find_by_id.assert_called_once_with(document.id)\n        mock_chunk_repo.delete_by_document.assert_called_once_with(document.id)\n        mock_doc_repo.delete.assert_called_once_with(document.id)\n\n    def test_delete_source_notebook_not_found(self):\n        \"\"\"Test deleting source from non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = DeleteSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", \"document123\"))\n\n    def test_delete_source_document_not_found(self):\n        \"\"\"Test deleting non-existent source raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_doc_repo.find_by_id.return_value = None\n        \n        handler = DeleteSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(notebook.id, \"nonexistent\"))\n\n    def test_delete_source_wrong_notebook(self):\n        \"\"\"Test deleting source from wrong notebook raises error.\"\"\"\n        # Arrange\n        mock_doc_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        \n        notebook1 = Notebook.create(name=\"Notebook 1\")\n        notebook2 = Notebook.create(name=\"Notebook 2\")\n        document = Document.create(notebook_id=notebook2.id, url=\"https://example.com\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook1\n        mock_doc_repo.find_by_id.return_value = document\n        \n        handler = DeleteSourceHandler(\n            document_repository=mock_doc_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.ValidationError):\n            pytest.asyncio.run(handler.handle(notebook1.id, document.id))\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the document handler module\n2. Verify tests for all happy path scenarios\n3. Verify tests for all error cases (not found, duplicate URL, validation errors)\n4. Confirm that tests follow the Arrange-Act-Assert pattern\n5. Validate that tests verify async ingestion is triggered correctly",
        "priority": "medium",
        "dependencies": [
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "6",
        "title": "Implement Conversation Handler Unit Tests",
        "description": "Add comprehensive unit tests for all conversation handlers with mock repositories and services.",
        "details": "Create tests/conversation/test_handlers.py with tests for all conversation handlers:\n\n```python\n\"\"\"Tests for Conversation handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.conversation.domain.model import Conversation, Message, MessageRole\nfrom src.notebook.domain.model import Notebook\nfrom src.conversation.handler.handlers import (\n    CreateConversationHandler,\n    GetConversationHandler,\n    ListConversationsHandler,\n    DeleteConversationHandler,\n    SendMessageHandler,\n)\nfrom src.conversation.schema.command import CreateConversation, SendMessage\nfrom src.query.service.retrieval import RetrievalService\nfrom src.query.adapter.pydantic_ai.agent import RAGAgent\n\n\nclass TestCreateConversationHandler:\n    \"\"\"Tests for CreateConversationHandler.\"\"\"\n\n    def test_create_conversation_success(self):\n        \"\"\"Test successful conversation creation.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        saved_conversation = None\n        def save_conversation(conversation):\n            nonlocal saved_conversation\n            saved_conversation = conversation\n            return conversation\n            \n        mock_conv_repo.save.side_effect = save_conversation\n        \n        handler = CreateConversationHandler(\n            conversation_repository=mock_conv_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        cmd = CreateConversation(title=\"Test Conversation\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_conv_repo.save.assert_called_once()\n        assert result.id is not None\n        assert saved_conversation.title == \"Test Conversation\"\n        assert saved_conversation.notebook_id == notebook.id\n        assert len(saved_conversation.messages) == 0\n\n    def test_create_conversation_notebook_not_found(self):\n        \"\"\"Test creating conversation in non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = CreateConversationHandler(\n            conversation_repository=mock_conv_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        cmd = CreateConversation(title=\"Test Conversation\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n\nclass TestGetConversationHandler:\n    \"\"\"Tests for GetConversationHandler.\"\"\"\n\n    def test_get_conversation_success(self):\n        \"\"\"Test successful conversation retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        conversation = Conversation(\n            id=\"conv123\",\n            notebook_id=\"notebook123\",\n            title=\"Test Conversation\",\n            messages=(),\n            created_at=None,\n            updated_at=None,\n        )\n        mock_repo.find_by_id.return_value = conversation\n        handler = GetConversationHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(conversation.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(conversation.id)\n        assert result.id == conversation.id\n        assert result.title == conversation.title\n\n    def test_get_conversation_not_found(self):\n        \"\"\"Test conversation not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetConversationHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListConversationsHandler:\n    \"\"\"Tests for ListConversationsHandler.\"\"\"\n\n    def test_list_conversations_success(self):\n        \"\"\"Test successful conversation listing.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        conversations = [\n            Conversation(id=\"conv1\", notebook_id=notebook.id, title=\"Conv 1\", messages=(), created_at=None, updated_at=None),\n            Conversation(id=\"conv2\", notebook_id=notebook.id, title=\"Conv 2\", messages=(), created_at=None, updated_at=None),\n        ]\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        mock_result = MagicMock()\n        mock_result.items = conversations\n        mock_result.total = len(conversations)\n        mock_result.page = 1\n        mock_result.size = 10\n        \n        mock_conv_repo.list_by_notebook.return_value = mock_result\n        \n        handler = ListConversationsHandler(\n            conversation_repository=mock_conv_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        from src.conversation.schema.query import ListConversations\n        query = ListConversations(notebook_id=notebook.id, page=1, size=10)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(query))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_conv_repo.list_by_notebook.assert_called_once_with(notebook.id, query)\n        assert result.total == 2\n        assert len(result.items) == 2\n        assert result.items[0].title == \"Conv 1\"\n        assert result.items[1].title == \"Conv 2\"\n\n    def test_list_conversations_notebook_not_found(self):\n        \"\"\"Test listing conversations for non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = ListConversationsHandler(\n            conversation_repository=mock_conv_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n        \n        from src.conversation.schema.query import ListConversations\n        query = ListConversations(notebook_id=\"nonexistent\", page=1, size=10)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(query))\n\n\nclass TestDeleteConversationHandler:\n    \"\"\"Tests for DeleteConversationHandler.\"\"\"\n\n    def test_delete_conversation_success(self):\n        \"\"\"Test successful conversation deletion.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        conversation = Conversation(\n            id=\"conv123\",\n            notebook_id=\"notebook123\",\n            title=\"Test Conversation\",\n            messages=(),\n            created_at=None,\n            updated_at=None,\n        )\n        mock_repo.find_by_id.return_value = conversation\n        handler = DeleteConversationHandler(mock_repo)\n\n        # Act\n        pytest.asyncio.run(handler.handle(conversation.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(conversation.id)\n        mock_repo.delete.assert_called_once_with(conversation.id)\n\n    def test_delete_conversation_not_found(self):\n        \"\"\"Test deleting non-existent conversation raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = DeleteConversationHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestSendMessageHandler:\n    \"\"\"Tests for SendMessageHandler.\"\"\"\n\n    def test_send_message_success(self):\n        \"\"\"Test successful message sending.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        # Create conversation with initial setup\n        conversation = Conversation(\n            id=\"conv123\",\n            notebook_id=\"notebook123\",\n            title=\"Test Conversation\",\n            messages=(),\n            created_at=None,\n            updated_at=None,\n        )\n        mock_conv_repo.find_by_id.return_value = conversation\n        \n        # Mock retrieved chunks\n        mock_chunks = [MagicMock(), MagicMock()]\n        mock_retrieval.retrieve.return_value = mock_chunks\n        \n        # Mock RAG agent answer\n        mock_answer = MagicMock()\n        mock_answer.answer = \"This is the AI response\"\n        mock_answer.citations = []\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = SendMessageHandler(\n            conversation_repository=mock_conv_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = SendMessage(content=\"What is the meaning of life?\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(conversation.id, cmd))\n\n        # Assert\n        mock_conv_repo.find_by_id.assert_called_once_with(conversation.id)\n        mock_retrieval.retrieve.assert_called_once()\n        mock_rag_agent.answer.assert_called_once()\n        \n        # Verify two messages were added (user + assistant)\n        assert mock_conv_repo.add_message.call_count == 2\n        \n        user_message_call = mock_conv_repo.add_message.call_args_list[0]\n        assert user_message_call[0][0] == conversation.id\n        assert user_message_call[0][1].role == MessageRole.USER\n        assert user_message_call[0][1].content == \"What is the meaning of life?\"\n        \n        assistant_message_call = mock_conv_repo.add_message.call_args_list[1]\n        assert assistant_message_call[0][0] == conversation.id\n        assert assistant_message_call[0][1].role == MessageRole.ASSISTANT\n        assert assistant_message_call[0][1].content == \"This is the AI response\"\n        \n        # Verify response\n        assert result.content == \"This is the AI response\"\n\n    def test_send_message_conversation_not_found(self):\n        \"\"\"Test sending message to non-existent conversation raises error.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        mock_conv_repo.find_by_id.return_value = None\n        \n        handler = SendMessageHandler(\n            conversation_repository=mock_conv_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = SendMessage(content=\"Hello\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_send_message_conversation_context_building(self):\n        \"\"\"Test that conversation context is built correctly for multi-turn conversations.\"\"\"\n        # Arrange\n        mock_conv_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        # Create conversation with existing messages\n        message1 = Message(id=\"msg1\", role=MessageRole.USER, content=\"First question\", citations=None, created_at=None)\n        message2 = Message(id=\"msg2\", role=MessageRole.ASSISTANT, content=\"First answer\", citations=None, created_at=None)\n        \n        conversation = Conversation(\n            id=\"conv123\",\n            notebook_id=\"notebook123\",\n            title=\"Test Conversation\",\n            messages=(message1, message2),\n            created_at=None,\n            updated_at=None,\n        )\n        mock_conv_repo.find_by_id.return_value = conversation\n        \n        # Mock retrieved chunks\n        mock_chunks = [MagicMock(), MagicMock()]\n        mock_retrieval.retrieve.return_value = mock_chunks\n        \n        # Mock RAG agent answer\n        mock_answer = MagicMock()\n        mock_answer.answer = \"This is the AI response\"\n        mock_answer.citations = []\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = SendMessageHandler(\n            conversation_repository=mock_conv_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = SendMessage(content=\"Follow-up question\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(conversation.id, cmd))\n\n        # Assert\n        mock_rag_agent.answer.assert_called_once()\n        # Check that context has previous messages plus the new user message\n        _, kwargs = mock_rag_agent.answer.call_args\n        conversation_history = kwargs.get('conversation_history', [])\n        assert len(conversation_history) >= 2  # Should have at least the previous turn\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the conversation handler module\n2. Verify tests for all happy path scenarios\n3. Verify tests for all error cases (not found conversation, not found notebook)\n4. Confirm that tests verify multi-turn conversation context is built correctly\n5. Validate that tests properly mock RAG agent and retrieval service",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "7",
        "title": "Implement Evaluation Handler Unit Tests",
        "description": "Add comprehensive unit tests for all evaluation handlers using mock repositories and services.",
        "details": "Create tests/evaluation/test_handlers.py with tests for all evaluation handlers:\n\n```python\n\"\"\"Tests for Evaluation handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\nimport uuid\nimport datetime\n\nfrom src import exceptions\nfrom src.evaluation.domain.model import Dataset, Run, TestCase, TestResult, EvaluationStatus\nfrom src.evaluation.domain.metric import MetricType\nfrom src.notebook.domain.model import Notebook\nfrom src.evaluation.handler.handlers import (\n    GenerateDatasetHandler,\n    GetDatasetHandler,\n    ListDatasetsHandler,\n    RunEvaluationHandler,\n    GetRunHandler,\n)\nfrom src.evaluation.schema.command import GenerateDataset, RunEvaluation\n\n\nclass TestGenerateDatasetHandler:\n    \"\"\"Tests for GenerateDatasetHandler.\"\"\"\n\n    def test_generate_dataset_success(self):\n        \"\"\"Test successful dataset generation.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        mock_generator = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        chunks = [MagicMock() for _ in range(5)]\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_chunk_repo.list_by_notebook.return_value = chunks\n        \n        saved_dataset = None\n        def save_dataset(dataset):\n            nonlocal saved_dataset\n            saved_dataset = dataset\n            return dataset\n            \n        mock_dataset_repo.save.side_effect = save_dataset\n        \n        handler = GenerateDatasetHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n            test_case_generator=mock_generator,\n        )\n        \n        cmd = GenerateDataset(num_cases=10, name=\"Test Dataset\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_chunk_repo.list_by_notebook.assert_called_once_with(notebook.id)\n        mock_dataset_repo.save.assert_called_once()\n        mock_generator.generate_test_cases.assert_called_once()\n        \n        assert result.id is not None\n        assert result.name == \"Test Dataset\"\n        assert result.notebook_id == notebook.id\n        assert result.status == str(EvaluationStatus.PENDING)\n\n    def test_generate_dataset_notebook_not_found(self):\n        \"\"\"Test generating dataset for non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        mock_generator = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = GenerateDatasetHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n            test_case_generator=mock_generator,\n        )\n        \n        cmd = GenerateDataset(num_cases=10, name=\"Test Dataset\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_generate_dataset_no_chunks(self):\n        \"\"\"Test generating dataset with no chunks raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        mock_chunk_repo = AsyncMock()\n        mock_generator = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_chunk_repo.list_by_notebook.return_value = []\n        \n        handler = GenerateDatasetHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n            chunk_repository=mock_chunk_repo,\n            test_case_generator=mock_generator,\n        )\n        \n        cmd = GenerateDataset(num_cases=10, name=\"Test Dataset\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.ValidationError):\n            pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n\nclass TestGetDatasetHandler:\n    \"\"\"Tests for GetDatasetHandler.\"\"\"\n\n    def test_get_dataset_success(self):\n        \"\"\"Test successful dataset retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        dataset_id = uuid.uuid4().hex\n        \n        dataset = Dataset(\n            id=dataset_id,\n            notebook_id=\"notebook123\",\n            name=\"Test Dataset\",\n            test_cases=(),\n            status=EvaluationStatus.COMPLETED,\n            created_at=datetime.datetime.now(),\n            completed_at=datetime.datetime.now(),\n        )\n        \n        mock_repo.find_by_id.return_value = dataset\n        handler = GetDatasetHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(dataset_id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(dataset_id)\n        assert result.id == dataset_id\n        assert result.name == \"Test Dataset\"\n\n    def test_get_dataset_not_found(self):\n        \"\"\"Test dataset not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetDatasetHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListDatasetsHandler:\n    \"\"\"Tests for ListDatasetsHandler.\"\"\"\n\n    def test_list_datasets_success(self):\n        \"\"\"Test successful dataset listing.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        datasets = [\n            Dataset(\n                id=uuid.uuid4().hex,\n                notebook_id=notebook.id,\n                name=\"Dataset 1\",\n                test_cases=(),\n                status=EvaluationStatus.COMPLETED,\n                created_at=datetime.datetime.now(),\n                completed_at=datetime.datetime.now(),\n            ),\n            Dataset(\n                id=uuid.uuid4().hex,\n                notebook_id=notebook.id,\n                name=\"Dataset 2\",\n                test_cases=(),\n                status=EvaluationStatus.COMPLETED,\n                created_at=datetime.datetime.now(),\n                completed_at=datetime.datetime.now(),\n            ),\n        ]\n        \n        mock_notebook_repo.find_by_id.return_value = notebook\n        mock_dataset_repo.list_by_notebook.return_value = datasets\n        \n        handler = ListDatasetsHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_dataset_repo.list_by_notebook.assert_called_once_with(notebook.id)\n        assert len(result) == 2\n        assert result[0].name == \"Dataset 1\"\n        assert result[1].name == \"Dataset 2\"\n\n    def test_list_datasets_notebook_not_found(self):\n        \"\"\"Test listing datasets for non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_notebook_repo = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = ListDatasetsHandler(\n            dataset_repository=mock_dataset_repo,\n            notebook_repository=mock_notebook_repo,\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestRunEvaluationHandler:\n    \"\"\"Tests for RunEvaluationHandler.\"\"\"\n\n    def test_run_evaluation_success(self):\n        \"\"\"Test successful evaluation run.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_run_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        \n        # Create test cases\n        test_cases = [\n            TestCase(\n                id=uuid.uuid4().hex,\n                question=\"Test question 1\",\n                expected_answer=\"Expected answer 1\",\n                source_chunk_ids=[\n                    uuid.uuid4().hex, \n                    uuid.uuid4().hex\n                ],\n            ),\n            TestCase(\n                id=uuid.uuid4().hex,\n                question=\"Test question 2\",\n                expected_answer=\"Expected answer 2\",\n                source_chunk_ids=[\n                    uuid.uuid4().hex\n                ],\n            ),\n        ]\n        \n        # Create dataset\n        dataset = Dataset(\n            id=uuid.uuid4().hex,\n            notebook_id=\"notebook123\",\n            name=\"Test Dataset\",\n            test_cases=tuple(test_cases),\n            status=EvaluationStatus.COMPLETED,\n            created_at=datetime.datetime.now(),\n            completed_at=datetime.datetime.now(),\n        )\n        \n        mock_dataset_repo.find_by_id.return_value = dataset\n        \n        # Mock retrieval results\n        def mock_retrieve(notebook_id, query, max_chunks):\n            return [MagicMock() for _ in range(2)]\n            \n        mock_retrieval.retrieve.side_effect = mock_retrieve\n        \n        # Mock run save\n        saved_run = None\n        def save_run(run):\n            nonlocal saved_run\n            saved_run = run\n            return run\n            \n        mock_run_repo.save.side_effect = save_run\n        \n        handler = RunEvaluationHandler(\n            dataset_repository=mock_dataset_repo,\n            run_repository=mock_run_repo,\n            retrieval_service=mock_retrieval,\n        )\n        \n        cmd = RunEvaluation(\n            metrics=[MetricType.PRECISION, MetricType.RECALL],\n            k_values=[1, 3, 5],\n            name=\"Test Run\",\n        )\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(dataset.id, cmd))\n\n        # Assert\n        mock_dataset_repo.find_by_id.assert_called_once_with(dataset.id)\n        assert mock_retrieval.retrieve.call_count == len(test_cases)\n        mock_run_repo.save.assert_called_once()\n        \n        assert result.id is not None\n        assert result.dataset_id == dataset.id\n        assert result.name == \"Test Run\"\n        assert len(result.test_results) == len(test_cases)\n        assert result.metrics is not None\n\n    def test_run_evaluation_dataset_not_found(self):\n        \"\"\"Test running evaluation on non-existent dataset raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_run_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        \n        mock_dataset_repo.find_by_id.return_value = None\n        \n        handler = RunEvaluationHandler(\n            dataset_repository=mock_dataset_repo,\n            run_repository=mock_run_repo,\n            retrieval_service=mock_retrieval,\n        )\n        \n        cmd = RunEvaluation(\n            metrics=[MetricType.PRECISION, MetricType.RECALL],\n            k_values=[1, 3, 5],\n            name=\"Test Run\",\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_run_evaluation_dataset_not_ready(self):\n        \"\"\"Test running evaluation on non-ready dataset raises error.\"\"\"\n        # Arrange\n        mock_dataset_repo = AsyncMock()\n        mock_run_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        \n        # Create dataset with PENDING status\n        dataset = Dataset(\n            id=uuid.uuid4().hex,\n            notebook_id=\"notebook123\",\n            name=\"Test Dataset\",\n            test_cases=(),\n            status=EvaluationStatus.PENDING,\n            created_at=datetime.datetime.now(),\n            completed_at=None,\n        )\n        \n        mock_dataset_repo.find_by_id.return_value = dataset\n        \n        handler = RunEvaluationHandler(\n            dataset_repository=mock_dataset_repo,\n            run_repository=mock_run_repo,\n            retrieval_service=mock_retrieval,\n        )\n        \n        cmd = RunEvaluation(\n            metrics=[MetricType.PRECISION, MetricType.RECALL],\n            k_values=[1, 3, 5],\n            name=\"Test Run\",\n        )\n\n        # Act/Assert\n        with pytest.raises(exceptions.ValidationError):\n            pytest.asyncio.run(handler.handle(dataset.id, cmd))\n\n\nclass TestGetRunHandler:\n    \"\"\"Tests for GetRunHandler.\"\"\"\n\n    def test_get_run_success(self):\n        \"\"\"Test successful run retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        run_id = uuid.uuid4().hex\n        \n        # Create run with test results\n        test_results = [\n            TestResult(\n                test_case_id=uuid.uuid4().hex,\n                retrieved_chunk_ids=[uuid.uuid4().hex],\n                relevant_chunk_ids=[uuid.uuid4().hex],\n                metrics={\n                    \"precision@1\": 1.0,\n                    \"recall@1\": 0.5,\n                },\n            ),\n        ]\n        \n        run = Run(\n            id=run_id,\n            dataset_id=uuid.uuid4().hex,\n            name=\"Test Run\",\n            metrics={\n                \"avg_precision@1\": 1.0,\n                \"avg_recall@1\": 0.5,\n            },\n            test_results=tuple(test_results),\n            created_at=datetime.datetime.now(),\n        )\n        \n        mock_repo.find_by_id.return_value = run\n        handler = GetRunHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(run_id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(run_id)\n        assert result.id == run_id\n        assert result.name == \"Test Run\"\n        assert len(result.test_results) == len(test_results)\n        assert result.metrics is not None\n\n    def test_get_run_not_found(self):\n        \"\"\"Test run not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetRunHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the evaluation handler module\n2. Verify tests for all happy path scenarios\n3. Verify tests for all error cases (not found, invalid state, no chunks)\n4. Confirm that tests verify metric calculations are aggregated correctly\n5. Validate that tests properly mock repositories, generators, and retrieval service",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "8",
        "title": "Implement Query Handler Unit Tests",
        "description": "Add comprehensive unit tests for the QueryNotebookHandler with mock retrieval service and RAG agent.",
        "details": "Create tests/query/test_handlers.py with tests for the query handler:\n\n```python\n\"\"\"Tests for Query handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.notebook.domain.model import Notebook\nfrom src.query.handler.handlers import QueryNotebookHandler\nfrom src.query.schema.command import QueryNotebook\nfrom src.query.schema.response import QueryResponse\nfrom src.chunk.domain.model import RetrievedChunk\n\n\nclass TestQueryNotebookHandler:\n    \"\"\"Tests for QueryNotebookHandler.\"\"\"\n\n    def test_query_notebook_success(self):\n        \"\"\"Test successful notebook query with results.\"\"\"\n        # Arrange\n        mock_notebook_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        # Create mock retrieved chunks\n        retrieved_chunks = [\n            RetrievedChunk(\n                id=\"chunk1\", \n                document_id=\"doc1\", \n                document_title=\"Doc 1\",\n                content=\"Content 1\", \n                score=0.9,\n            ),\n            RetrievedChunk(\n                id=\"chunk2\", \n                document_id=\"doc2\", \n                document_title=\"Doc 2\",\n                content=\"Content 2\", \n                score=0.8,\n            ),\n        ]\n        mock_retrieval.retrieve.return_value = retrieved_chunks\n        \n        # Create mock RAG answer\n        mock_answer = MagicMock()\n        mock_answer.answer = \"This is the answer based on retrieved chunks.\"\n        mock_answer.citations = [MagicMock(), MagicMock()]\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = QueryNotebookHandler(\n            notebook_repository=mock_notebook_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = QueryNotebook(question=\"What is the meaning of life?\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_notebook_repo.find_by_id.assert_called_once_with(notebook.id)\n        mock_retrieval.retrieve.assert_called_once_with(\n            notebook_id=notebook.id,\n            query=cmd.question,\n            max_chunks=10,\n        )\n        mock_rag_agent.answer.assert_called_once_with(\n            question=cmd.question,\n            retrieved_chunks=retrieved_chunks,\n            conversation_history=[],\n        )\n        \n        assert isinstance(result, QueryResponse)\n        assert result.answer == mock_answer.answer\n        assert len(result.citations) == len(mock_answer.citations)\n\n    def test_query_notebook_not_found(self):\n        \"\"\"Test query with non-existent notebook raises error.\"\"\"\n        # Arrange\n        mock_notebook_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        mock_notebook_repo.find_by_id.return_value = None\n        \n        handler = QueryNotebookHandler(\n            notebook_repository=mock_notebook_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = QueryNotebook(question=\"What is the meaning of life?\")\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\", cmd))\n\n    def test_query_notebook_no_results(self):\n        \"\"\"Test query with no retrieved results still gets an answer.\"\"\"\n        # Arrange\n        mock_notebook_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        # Return empty retrieval results\n        mock_retrieval.retrieve.return_value = []\n        \n        # Create mock RAG answer for no results case\n        mock_answer = MagicMock()\n        mock_answer.answer = \"I couldn't find relevant information in the notebook.\"\n        mock_answer.citations = []\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = QueryNotebookHandler(\n            notebook_repository=mock_notebook_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = QueryNotebook(question=\"What is the meaning of life?\")\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_retrieval.retrieve.assert_called_once()\n        mock_rag_agent.answer.assert_called_once_with(\n            question=cmd.question,\n            retrieved_chunks=[],  # Empty results\n            conversation_history=[],\n        )\n        \n        assert result.answer == mock_answer.answer\n        assert len(result.citations) == 0\n\n    def test_query_notebook_with_max_chunks_param(self):\n        \"\"\"Test query with custom max_chunks parameter.\"\"\"\n        # Arrange\n        mock_notebook_repo = AsyncMock()\n        mock_retrieval = AsyncMock()\n        mock_rag_agent = AsyncMock()\n        \n        notebook = Notebook.create(name=\"Test Notebook\")\n        mock_notebook_repo.find_by_id.return_value = notebook\n        \n        # Return some retrieval results\n        mock_retrieval.retrieve.return_value = [MagicMock()]\n        \n        # Create mock RAG answer\n        mock_answer = MagicMock()\n        mock_answer.answer = \"This is the answer.\"\n        mock_answer.citations = []\n        mock_rag_agent.answer.return_value = mock_answer\n        \n        handler = QueryNotebookHandler(\n            notebook_repository=mock_notebook_repo,\n            retrieval_service=mock_retrieval,\n            rag_agent=mock_rag_agent,\n        )\n        \n        cmd = QueryNotebook(\n            question=\"What is the meaning of life?\",\n            max_chunks=5,  # Custom max_chunks\n        )\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(notebook.id, cmd))\n\n        # Assert\n        mock_retrieval.retrieve.assert_called_once_with(\n            notebook_id=notebook.id,\n            query=cmd.question,\n            max_chunks=5,  # Should use the custom value\n        )\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the query handler module\n2. Verify test for the happy path (question -> retrieval -> RAG answer)\n3. Verify test for notebook not found error\n4. Verify test for empty retrieval results\n5. Validate that tests properly mock retrieval service and RAG agent\n6. Verify test for custom max_chunks parameter",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "9",
        "title": "Implement Retrieval Service Unit Tests",
        "description": "Add comprehensive unit tests for the RetrievalService which handles embedding generation, vector search, and document enrichment.",
        "details": "Create tests/query/test_retrieval.py with tests for the retrieval service:\n\n```python\n\"\"\"Tests for RetrievalService.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src.query.service.retrieval import RetrievalService\nfrom src.chunk.domain.model import Chunk, RetrievedChunk\nfrom src.document.domain.model import Document\n\n\nclass TestRetrievalService:\n    \"\"\"Tests for RetrievalService.\"\"\"\n\n    def test_retrieve_success(self):\n        \"\"\"Test successful retrieval flow.\"\"\"\n        # Arrange\n        mock_chunk_repo = AsyncMock()\n        mock_doc_repo = AsyncMock()\n        mock_embedding_provider = AsyncMock()\n        \n        # Mock embedding generation\n        query_embedding = [0.1, 0.2, 0.3]\n        mock_embedding_provider.generate.return_value = query_embedding\n        \n        # Mock vector search results\n        chunk1 = MagicMock(spec=Chunk)\n        chunk1.id = \"chunk1\"\n        chunk1.document_id = \"doc1\"\n        chunk1.content = \"Content 1\"\n        \n        chunk2 = MagicMock(spec=Chunk)\n        chunk2.id = \"chunk2\"\n        chunk2.document_id = \"doc2\"\n        chunk2.content = \"Content 2\"\n        \n        vector_search_results = [\n            (chunk1, 0.9),\n            (chunk2, 0.8),\n        ]\n        mock_chunk_repo.vector_search.return_value = vector_search_results\n        \n        # Mock document retrieval for enrichment\n        doc1 = MagicMock(spec=Document)\n        doc1.id = \"doc1\"\n        doc1.title = \"Document 1\"\n        \n        doc2 = MagicMock(spec=Document)\n        doc2.id = \"doc2\"\n        doc2.title = \"Document 2\"\n        \n        mock_doc_repo.find_by_ids.return_value = [doc1, doc2]\n        \n        service = RetrievalService(\n            chunk_repository=mock_chunk_repo,\n            document_repository=mock_doc_repo,\n            embedding_provider=mock_embedding_provider,\n        )\n\n        # Act\n        result = pytest.asyncio.run(service.retrieve(\n            notebook_id=\"notebook123\",\n            query=\"What is the meaning of life?\",\n            max_chunks=10,\n        ))\n\n        # Assert\n        mock_embedding_provider.generate.assert_called_once_with(\"What is the meaning of life?\")\n        mock_chunk_repo.vector_search.assert_called_once_with(\n            notebook_id=\"notebook123\",\n            embedding=query_embedding,\n            limit=10,\n        )\n        mock_doc_repo.find_by_ids.assert_called_once_with([\"doc1\", \"doc2\"])\n        \n        # Verify enriched results\n        assert len(result) == 2\n        assert isinstance(result[0], RetrievedChunk)\n        assert result[0].id == \"chunk1\"\n        assert result[0].document_id == \"doc1\"\n        assert result[0].document_title == \"Document 1\"\n        assert result[0].content == \"Content 1\"\n        assert result[0].score == 0.9\n        \n        assert isinstance(result[1], RetrievedChunk)\n        assert result[1].id == \"chunk2\"\n        assert result[1].document_id == \"doc2\"\n        assert result[1].document_title == \"Document 2\"\n        assert result[1].content == \"Content 2\"\n        assert result[1].score == 0.8\n\n    def test_retrieve_no_results(self):\n        \"\"\"Test retrieval with no search results.\"\"\"\n        # Arrange\n        mock_chunk_repo = AsyncMock()\n        mock_doc_repo = AsyncMock()\n        mock_embedding_provider = AsyncMock()\n        \n        # Mock embedding generation\n        query_embedding = [0.1, 0.2, 0.3]\n        mock_embedding_provider.generate.return_value = query_embedding\n        \n        # Mock empty vector search results\n        mock_chunk_repo.vector_search.return_value = []\n        \n        service = RetrievalService(\n            chunk_repository=mock_chunk_repo,\n            document_repository=mock_doc_repo,\n            embedding_provider=mock_embedding_provider,\n        )\n\n        # Act\n        result = pytest.asyncio.run(service.retrieve(\n            notebook_id=\"notebook123\",\n            query=\"What is the meaning of life?\",\n            max_chunks=10,\n        ))\n\n        # Assert\n        mock_embedding_provider.generate.assert_called_once()\n        mock_chunk_repo.vector_search.assert_called_once()\n        mock_doc_repo.find_by_ids.assert_not_called()  # No documents to find\n        \n        # Verify empty results\n        assert len(result) == 0\n\n    def test_retrieve_missing_documents(self):\n        \"\"\"Test retrieval with missing documents during enrichment.\"\"\"\n        # Arrange\n        mock_chunk_repo = AsyncMock()\n        mock_doc_repo = AsyncMock()\n        mock_embedding_provider = AsyncMock()\n        \n        # Mock embedding generation\n        query_embedding = [0.1, 0.2, 0.3]\n        mock_embedding_provider.generate.return_value = query_embedding\n        \n        # Mock vector search results\n        chunk1 = MagicMock(spec=Chunk)\n        chunk1.id = \"chunk1\"\n        chunk1.document_id = \"doc1\"\n        chunk1.content = \"Content 1\"\n        \n        chunk2 = MagicMock(spec=Chunk)\n        chunk2.id = \"chunk2\"\n        chunk2.document_id = \"doc2\"\n        chunk2.content = \"Content 2\"\n        \n        vector_search_results = [\n            (chunk1, 0.9),\n            (chunk2, 0.8),\n        ]\n        mock_chunk_repo.vector_search.return_value = vector_search_results\n        \n        # Mock document retrieval with missing document\n        doc1 = MagicMock(spec=Document)\n        doc1.id = \"doc1\"\n        doc1.title = \"Document 1\"\n        \n        # doc2 is missing\n        mock_doc_repo.find_by_ids.return_value = [doc1]\n        \n        service = RetrievalService(\n            chunk_repository=mock_chunk_repo,\n            document_repository=mock_doc_repo,\n            embedding_provider=mock_embedding_provider,\n        )\n\n        # Act\n        result = pytest.asyncio.run(service.retrieve(\n            notebook_id=\"notebook123\",\n            query=\"What is the meaning of life?\",\n            max_chunks=10,\n        ))\n\n        # Assert\n        mock_doc_repo.find_by_ids.assert_called_once_with([\"doc1\", \"doc2\"])\n        \n        # Verify enriched results - should have doc1 info but use fallback for doc2\n        assert len(result) == 2\n        assert result[0].document_title == \"Document 1\"  # From doc1\n        assert result[1].document_title is None  # Fallback for missing doc2\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 90% coverage for the retrieval service\n2. Verify test for happy path (query -> embedding -> search -> enrichment)\n3. Verify test for empty results\n4. Verify test for missing documents in enrichment\n5. Validate that tests properly mock embedding provider and repositories\n6. Check that immutable models are correctly used",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "10",
        "title": "Implement Integration Tests for Core Workflows",
        "description": "Add integration tests that test the full workflow from API to database for core features using testcontainers for PostgreSQL.",
        "details": "1. Create tests/integration/conftest.py with PostgreSQL testcontainer setup:\n\n```python\n\"\"\"Integration test fixtures.\"\"\"\n\nimport asyncio\nimport pytest\nimport os\nfrom typing import AsyncGenerator\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker\nimport alembic.config\nfrom testcontainers.postgres import PostgresContainer\n\nfrom src.database import Base\n\n\n@pytest.fixture(scope=\"session\")\ndef postgres_container():\n    \"\"\"Start a PostgreSQL container with pgvector extension.\"\"\"\n    postgres = PostgresContainer(\n        \"ankane/pgvector:latest\",\n        port=5432,\n        user=\"postgres\",\n        password=\"postgres\",\n    )\n    postgres.start()\n    \n    # Set environment variables for connection\n    os.environ[\"DATABASE_URL\"] = postgres.get_connection_url().replace(\"psycopg2\", \"postgresql+asyncpg\")\n    \n    yield postgres\n    postgres.stop()\n\n\n@pytest.fixture(scope=\"session\")\ndef event_loop(postgres_container):\n    \"\"\"Create an event loop for the session.\"\"\"\n    loop = asyncio.new_event_loop()\n    yield loop\n    loop.close()\n\n\n@pytest.fixture(scope=\"session\")\nasync def integration_engine(postgres_container):\n    \"\"\"Create database engine for integration tests.\"\"\"\n    # Create engine from environment variable\n    engine = create_async_engine(os.environ[\"DATABASE_URL\"], echo=False)\n    \n    # Run migrations\n    alembic_args = [\"--raiseerr\", \"upgrade\", \"head\"]\n    alembic.config.main(argv=alembic_args)\n    \n    return engine\n\n\n@pytest.fixture(scope=\"session\")\nasync def integration_session_factory(integration_engine):\n    \"\"\"Create session factory for integration tests.\"\"\"\n    return async_sessionmaker(integration_engine, expire_on_commit=False)\n\n\n@pytest.fixture\nasync def integration_session(\n    integration_session_factory\n) -> AsyncGenerator[AsyncSession, None]:\n    \"\"\"Create a new database session for a test.\"\"\"\n    async with integration_session_factory() as session:\n        yield session\n        await session.rollback()\n```\n\n2. Create integration test for notebook flow (create, add source, query):\n\n```python\n\"\"\"Integration test for notebook -> source -> query workflow.\"\"\"\n\nimport pytest\nimport uuid\nimport asyncio\nfrom fastapi.testclient import TestClient\n\nfrom src.main import app\n\n\n@pytest.mark.integration\ndef test_notebook_source_query_flow(integration_session):\n    \"\"\"Test end-to-end flow: create notebook -> add source -> query.\"\"\"\n    client = TestClient(app)\n    \n    # Step 1: Create notebook\n    notebook_name = f\"Test Notebook {uuid.uuid4().hex[:8]}\"\n    create_response = client.post(\n        \"/api/v1/notebooks\",\n        json={\"name\": notebook_name},\n    )\n    assert create_response.status_code == 201\n    notebook_id = create_response.json()[\"id\"]\n    \n    # Step 2: Add source (use a small, stable example URL)\n    source_url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n    add_source_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/sources\",\n        json={\"url\": source_url, \"title\": \"Python Wikipedia\"},\n    )\n    assert add_source_response.status_code == 201\n    document_id = add_source_response.json()[\"id\"]\n    \n    # Wait for background processing (in real tests, we'd mock this)\n    # This is just for illustration - in practice we'd need a more robust approach\n    asyncio.sleep(2)  \n    \n    # Step 3: Query the notebook\n    query_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/query\",\n        json={\"question\": \"What is Python?\"},\n    )\n    assert query_response.status_code == 200\n    result = query_response.json()\n    assert \"answer\" in result\n    assert isinstance(result[\"answer\"], str)\n    assert len(result[\"answer\"]) > 0\n    \n    # Step 4: Verify source can be deleted\n    delete_response = client.delete(\n        f\"/api/v1/notebooks/{notebook_id}/sources/{document_id}\",\n    )\n    assert delete_response.status_code == 204\n    \n    # Verify document is gone\n    get_doc_response = client.get(f\"/api/v1/documents/{document_id}\")\n    assert get_doc_response.status_code == 404\n```\n\n3. Create integration test for conversation flow:\n\n```python\n\"\"\"Integration test for conversation workflow.\"\"\"\n\nimport pytest\nimport uuid\nimport asyncio\nfrom fastapi.testclient import TestClient\n\nfrom src.main import app\n\n\n@pytest.mark.integration\ndef test_conversation_flow(integration_session):\n    \"\"\"Test end-to-end flow: create notebook -> create conversation -> send messages.\"\"\"\n    client = TestClient(app)\n    \n    # Step 1: Create notebook\n    notebook_name = f\"Test Notebook {uuid.uuid4().hex[:8]}\"\n    create_notebook_response = client.post(\n        \"/api/v1/notebooks\",\n        json={\"name\": notebook_name},\n    )\n    assert create_notebook_response.status_code == 201\n    notebook_id = create_notebook_response.json()[\"id\"]\n    \n    # Step 2: Create conversation\n    conversation_title = f\"Test Conversation {uuid.uuid4().hex[:8]}\"\n    create_conversation_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/conversations\",\n        json={\"title\": conversation_title},\n    )\n    assert create_conversation_response.status_code == 201\n    conversation_id = create_conversation_response.json()[\"id\"]\n    \n    # Step 3: Send message to conversation\n    message_content = \"Hello, how are you?\"\n    send_message_response = client.post(\n        f\"/api/v1/conversations/{conversation_id}/messages\",\n        json={\"content\": message_content},\n    )\n    assert send_message_response.status_code == 200\n    result = send_message_response.json()\n    assert \"content\" in result\n    assert isinstance(result[\"content\"], str)\n    assert len(result[\"content\"]) > 0\n    \n    # Step 4: Get conversation to verify message was saved\n    get_conversation_response = client.get(\n        f\"/api/v1/conversations/{conversation_id}\",\n    )\n    assert get_conversation_response.status_code == 200\n    conversation = get_conversation_response.json()\n    assert conversation[\"title\"] == conversation_title\n    assert len(conversation[\"messages\"]) >= 2  # User message + AI response\n    \n    # Verify user message content\n    user_messages = [msg for msg in conversation[\"messages\"] if msg[\"role\"] == \"user\"]\n    assert any(msg[\"content\"] == message_content for msg in user_messages)\n```\n\n4. Create integration test for evaluation flow:\n\n```python\n\"\"\"Integration test for evaluation workflow.\"\"\"\n\nimport pytest\nimport uuid\nimport asyncio\nfrom fastapi.testclient import TestClient\n\nfrom src.main import app\nfrom src.evaluation.domain.metric import MetricType\n\n\n@pytest.mark.integration\ndef test_evaluation_flow(integration_session):\n    \"\"\"Test end-to-end flow: create notebook -> add source -> generate dataset -> run evaluation.\"\"\"\n    client = TestClient(app)\n    \n    # Step 1: Create notebook\n    notebook_name = f\"Test Notebook {uuid.uuid4().hex[:8]}\"\n    create_response = client.post(\n        \"/api/v1/notebooks\",\n        json={\"name\": notebook_name},\n    )\n    assert create_response.status_code == 201\n    notebook_id = create_response.json()[\"id\"]\n    \n    # Step 2: Add source (use a small, stable example URL)\n    source_url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n    add_source_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/sources\",\n        json={\"url\": source_url, \"title\": \"Python Wikipedia\"},\n    )\n    assert add_source_response.status_code == 201\n    \n    # Wait for background processing (in real tests, we'd mock this)\n    asyncio.sleep(2)  \n    \n    # Step 3: Generate evaluation dataset\n    dataset_name = f\"Test Dataset {uuid.uuid4().hex[:8]}\"\n    generate_dataset_response = client.post(\n        f\"/api/v1/notebooks/{notebook_id}/evaluation/datasets\",\n        json={\"name\": dataset_name, \"num_cases\": 3},  # Small number for test\n    )\n    assert generate_dataset_response.status_code == 201\n    dataset_id = generate_dataset_response.json()[\"id\"]\n    \n    # Wait for dataset generation (in real tests, we'd mock this)\n    # This is just for illustration - in practice we'd need a more robust approach\n    # like polling the dataset status until it's ready\n    asyncio.sleep(5)  \n    \n    # Step 4: Run evaluation\n    run_name = f\"Test Run {uuid.uuid4().hex[:8]}\"\n    run_evaluation_response = client.post(\n        f\"/api/v1/evaluation/datasets/{dataset_id}/runs\",\n        json={\n            \"name\": run_name,\n            \"metrics\": [MetricType.PRECISION.value, MetricType.RECALL.value],\n            \"k_values\": [1, 3, 5],\n        },\n    )\n    assert run_evaluation_response.status_code == 201\n    run_id = run_evaluation_response.json()[\"id\"]\n    \n    # Step 5: Get evaluation run\n    get_run_response = client.get(f\"/api/v1/evaluation/runs/{run_id}\")\n    assert get_run_response.status_code == 200\n    run = get_run_response.json()\n    assert run[\"name\"] == run_name\n    assert run[\"dataset_id\"] == dataset_id\n    assert \"metrics\" in run\n    assert run[\"test_results\"] is not None\n```",
        "testStrategy": "1. Set up a CI pipeline that runs these integration tests using containerized PostgreSQL\n2. Verify notebook CRUD -> add source -> query flow works end-to-end\n3. Verify conversation create -> send message flow works end-to-end\n4. Verify evaluation dataset generation -> run evaluation flow works end-to-end\n5. Ensure tests use testcontainers.postgresql for PostgreSQL\n6. Tests should be marked with @pytest.mark.integration to allow separate running\n7. Ensure these tests are clearly separated from unit tests",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "11",
        "title": "Implement Chunk Handler and API Tests",
        "description": "Add comprehensive unit tests for chunk handlers and chunk-related API endpoints.",
        "details": "1. Create tests/chunk/test_handlers.py with tests for the chunk handlers:\n\n```python\n\"\"\"Tests for Chunk handlers.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src import exceptions\nfrom src.chunk.domain.model import Chunk\nfrom src.chunk.handler.handlers import GetChunkHandler, ListChunksByDocumentHandler\n\n\nclass TestGetChunkHandler:\n    \"\"\"Tests for GetChunkHandler.\"\"\"\n\n    def test_get_chunk_success(self):\n        \"\"\"Test successful chunk retrieval.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        chunk = Chunk(\n            id=\"chunk123\",\n            document_id=\"doc123\",\n            notebook_id=\"notebook123\",\n            content=\"Chunk content\",\n            embedding=None,\n            metadata={},\n        )\n        mock_repo.find_by_id.return_value = chunk\n        handler = GetChunkHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(chunk.id))\n\n        # Assert\n        mock_repo.find_by_id.assert_called_once_with(chunk.id)\n        assert result.id == chunk.id\n        assert result.document_id == chunk.document_id\n        assert result.content == chunk.content\n\n    def test_get_chunk_not_found(self):\n        \"\"\"Test chunk not found raises error.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.find_by_id.return_value = None\n        handler = GetChunkHandler(mock_repo)\n\n        # Act/Assert\n        with pytest.raises(exceptions.NotFoundError):\n            pytest.asyncio.run(handler.handle(\"nonexistent\"))\n\n\nclass TestListChunksByDocumentHandler:\n    \"\"\"Tests for ListChunksByDocumentHandler.\"\"\"\n\n    def test_list_chunks_success(self):\n        \"\"\"Test successful chunk listing.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        chunks = [\n            Chunk(\n                id=\"chunk1\",\n                document_id=\"doc123\",\n                notebook_id=\"notebook123\",\n                content=\"Chunk 1 content\",\n                embedding=None,\n                metadata={},\n            ),\n            Chunk(\n                id=\"chunk2\",\n                document_id=\"doc123\",\n                notebook_id=\"notebook123\",\n                content=\"Chunk 2 content\",\n                embedding=None,\n                metadata={},\n            ),\n        ]\n        mock_repo.list_by_document.return_value = chunks\n        handler = ListChunksByDocumentHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(\"doc123\"))\n\n        # Assert\n        mock_repo.list_by_document.assert_called_once_with(\"doc123\")\n        assert len(result) == 2\n        assert result[0].id == \"chunk1\"\n        assert result[1].id == \"chunk2\"\n        assert result[0].content == \"Chunk 1 content\"\n        assert result[1].content == \"Chunk 2 content\"\n\n    def test_list_chunks_empty(self):\n        \"\"\"Test listing chunks for document with no chunks.\"\"\"\n        # Arrange\n        mock_repo = AsyncMock()\n        mock_repo.list_by_document.return_value = []\n        handler = ListChunksByDocumentHandler(mock_repo)\n\n        # Act\n        result = pytest.asyncio.run(handler.handle(\"doc123\"))\n\n        # Assert\n        mock_repo.list_by_document.assert_called_once_with(\"doc123\")\n        assert len(result) == 0\n```\n\n2. Create tests/chunk/test_api.py with tests for chunk API endpoints:\n\n```python\n\"\"\"Tests for Chunk API endpoints.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch\nfrom fastapi.testclient import TestClient\n\nfrom src.main import app\nfrom src.chunk.domain.model import Chunk\nfrom src.chunk.handler.handlers import GetChunkHandler, ListChunksByDocumentHandler\n\n\n@patch(\"src.dependency.container.ApplicationContainer.chunk.handler.get_chunk_handler\")\ndef test_get_chunk(mock_handler, client=TestClient(app)):\n    \"\"\"Test GET /chunks/{chunk_id} endpoint.\"\"\"\n    # Arrange\n    chunk_id = \"chunk123\"\n    mock_get_chunk_handler = AsyncMock()\n    mock_handler.return_value = mock_get_chunk_handler\n    \n    from src.chunk.schema.response import ChunkDetail\n    mock_response = ChunkDetail(\n        id=chunk_id,\n        document_id=\"doc123\",\n        notebook_id=\"notebook123\",\n        content=\"Chunk content\",\n        metadata={},\n    )\n    mock_get_chunk_handler.handle.return_value = mock_response\n    \n    # Act\n    response = client.get(f\"/api/v1/chunks/{chunk_id}\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert data[\"id\"] == chunk_id\n    assert data[\"content\"] == \"Chunk content\"\n    mock_get_chunk_handler.handle.assert_called_once_with(chunk_id)\n\n\n@patch(\"src.dependency.container.ApplicationContainer.chunk.handler.get_chunk_handler\")\ndef test_get_chunk_not_found(mock_handler, client=TestClient(app)):\n    \"\"\"Test GET /chunks/{chunk_id} with non-existent chunk.\"\"\"\n    # Arrange\n    chunk_id = \"nonexistent\"\n    mock_get_chunk_handler = AsyncMock()\n    mock_handler.return_value = mock_get_chunk_handler\n    \n    from src import exceptions\n    mock_get_chunk_handler.handle.side_effect = exceptions.NotFoundError(f\"Chunk not found: {chunk_id}\")\n    \n    # Act\n    response = client.get(f\"/api/v1/chunks/{chunk_id}\")\n    \n    # Assert\n    assert response.status_code == 404\n    mock_get_chunk_handler.handle.assert_called_once_with(chunk_id)\n\n\n@patch(\"src.dependency.container.ApplicationContainer.chunk.handler.list_chunks_by_document_handler\")\ndef test_list_chunks_by_document(mock_handler, client=TestClient(app)):\n    \"\"\"Test GET /documents/{document_id}/chunks endpoint.\"\"\"\n    # Arrange\n    document_id = \"doc123\"\n    mock_list_handler = AsyncMock()\n    mock_handler.return_value = mock_list_handler\n    \n    from src.chunk.schema.response import ChunkDetail\n    mock_response = [\n        ChunkDetail(\n            id=\"chunk1\",\n            document_id=document_id,\n            notebook_id=\"notebook123\",\n            content=\"Chunk 1 content\",\n            metadata={},\n        ),\n        ChunkDetail(\n            id=\"chunk2\",\n            document_id=document_id,\n            notebook_id=\"notebook123\",\n            content=\"Chunk 2 content\",\n            metadata={},\n        ),\n    ]\n    mock_list_handler.handle.return_value = mock_response\n    \n    # Act\n    response = client.get(f\"/api/v1/documents/{document_id}/chunks\")\n    \n    # Assert\n    assert response.status_code == 200\n    data = response.json()\n    assert len(data) == 2\n    assert data[0][\"id\"] == \"chunk1\"\n    assert data[1][\"id\"] == \"chunk2\"\n    assert data[0][\"content\"] == \"Chunk 1 content\"\n    assert data[1][\"content\"] == \"Chunk 2 content\"\n    mock_list_handler.handle.assert_called_once_with(document_id)\n```\n\n3. Create tests/chunk/test_repository.py with tests for chunk repository operations with embeddings:\n\n```python\n\"\"\"Tests for Chunk repository.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\n\nfrom src.chunk.adapter.repository import ChunkRepository\nfrom src.chunk.domain.model import Chunk\n\n\ndef test_save_chunk_with_embedding():\n    \"\"\"Test saving chunk with embedding.\"\"\"\n    # Arrange\n    mock_session = AsyncMock()\n    repository = ChunkRepository(mock_session)\n    \n    # Create chunk with embedding\n    chunk = Chunk(\n        id=\"chunk123\",\n        document_id=\"doc123\",\n        notebook_id=\"notebook123\",\n        content=\"Chunk content\",\n        embedding=[0.1, 0.2, 0.3],  # Embedding vector\n        metadata={},\n    )\n    \n    # Mock add and commit methods\n    mock_session.add = MagicMock()\n    \n    # Act\n    result = pytest.asyncio.run(repository.save(chunk))\n    \n    # Assert\n    mock_session.add.assert_called_once()\n    mock_session.commit.assert_called_once()\n    assert result == chunk  # Repository returns the saved chunk\n\n\ndef test_vector_search():\n    \"\"\"Test vector similarity search.\"\"\"\n    # This test would require a more complex setup with a mock for execute\n    # and the SQLAlchemy query results, which is beyond the scope of this example\n    # In a real test, we'd mock the database response for vector search\n    pass\n\n\ndef test_delete_by_document():\n    \"\"\"Test deleting chunks by document ID.\"\"\"\n    # Arrange\n    mock_session = AsyncMock()\n    repository = ChunkRepository(mock_session)\n    document_id = \"doc123\"\n    \n    # Mock execute method to return delete count\n    mock_result = MagicMock()\n    mock_result.rowcount = 5  # 5 chunks deleted\n    mock_session.execute.return_value = mock_result\n    \n    # Act\n    delete_count = pytest.asyncio.run(repository.delete_by_document(document_id))\n    \n    # Assert\n    mock_session.execute.assert_called_once()\n    mock_session.commit.assert_called_once()\n    assert delete_count == 5  # Repository returns number of deleted chunks\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure at least 80% coverage for the chunk handler module\n2. Verify tests for listing chunks by document\n3. Verify tests for searching similar chunks\n4. Verify tests for chunk with embedding operations\n5. Ensure API tests mock the handlers correctly and validate response formats\n6. Test repository operations with embeddings",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "12",
        "title": "Implement CLI Command Tests",
        "description": "Add tests for CLI commands to ensure they correctly invoke handlers and format output.",
        "details": "Create tests/cli/test_commands.py with tests for CLI commands:\n\n```python\n\"\"\"Tests for CLI commands.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, patch, MagicMock\nfrom typer.testing import CliRunner\n\nfrom src.cli.commands.notebook import app as notebook_app\nfrom src.cli.commands.conversation import app as conversation_app\nfrom src.cli.commands.query import app as query_app\nfrom src.notebook.domain.model import Notebook\nfrom src.document.domain.model import Document\nfrom src.conversation.domain.model import Conversation, Message, MessageRole\n\n\nrunner = CliRunner()\n\n\n@pytest.fixture\ndef mock_session_context():\n    \"\"\"Mock session context for CLI tests.\"\"\"\n    mock_session = AsyncMock()\n    context_manager = MagicMock()\n    context_manager.__aenter__.return_value = mock_session\n    context_manager.__aexit__.return_value = None\n    \n    with patch(\"src.cli.utils.get_session_context\") as mock_get_session:\n        mock_get_session.return_value = context_manager\n        yield mock_session\n\n\nclass TestNotebookCommands:\n    \"\"\"Tests for notebook CLI commands.\"\"\"\n\n    def test_create_notebook(self, mock_session_context):\n        \"\"\"Test notebook create command.\"\"\"\n        # Arrange\n        notebook_name = \"Test Notebook\"\n        notebook_description = \"Test description\"\n        \n        # Mock repository save method\n        from src.notebook.adapter.repository import NotebookRepository\n        with patch.object(NotebookRepository, \"save\", new_callable=AsyncMock) as mock_save:\n            mock_save.side_effect = lambda notebook: notebook\n            \n            # Act\n            result = runner.invoke(notebook_app, [\n                \"create\", \n                notebook_name, \n                \"--description\", notebook_description\n            ])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"Notebook created:\" in result.stdout\n            assert notebook_name in result.stdout\n            assert notebook_description in result.stdout\n            mock_save.assert_called_once()\n            saved_notebook = mock_save.call_args[0][0]\n            assert saved_notebook.name == notebook_name\n            assert saved_notebook.description == notebook_description\n\n    def test_list_notebooks(self, mock_session_context):\n        \"\"\"Test notebook list command.\"\"\"\n        # Arrange\n        notebooks = [\n            Notebook.create(name=\"Notebook 1\"),\n            Notebook.create(name=\"Notebook 2\"),\n        ]\n        \n        # Mock repository list method\n        from src.notebook.adapter.repository import NotebookRepository\n        with patch.object(NotebookRepository, \"list\", new_callable=AsyncMock) as mock_list:\n            from src.common import PaginatedResult\n            mock_result = PaginatedResult(\n                items=notebooks,\n                total=len(notebooks),\n                page=1,\n                size=10,\n            )\n            mock_list.return_value = mock_result\n            \n            # Act\n            result = runner.invoke(notebook_app, [\"list\"])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"Notebook 1\" in result.stdout\n            assert \"Notebook 2\" in result.stdout\n            mock_list.assert_called_once()\n\n\nclass TestSourceCommands:\n    \"\"\"Tests for source CLI commands.\"\"\"\n\n    def test_add_source(self, mock_session_context):\n        \"\"\"Test source add command.\"\"\"\n        # Arrange\n        notebook_id = \"notebook123\"\n        source_url = \"https://example.com\"\n        source_title = \"Example Website\"\n        \n        # Mock repositories\n        from src.notebook.adapter.repository import NotebookRepository\n        from src.document.adapter.repository import DocumentRepository\n        \n        with patch.object(NotebookRepository, \"find_by_id\", new_callable=AsyncMock) as mock_find_notebook, \\\n             patch.object(DocumentRepository, \"find_by_notebook_and_url\", new_callable=AsyncMock) as mock_find_doc, \\\n             patch.object(DocumentRepository, \"save\", new_callable=AsyncMock) as mock_save_doc:\n            \n            # Mock notebook exists\n            notebook = Notebook.create(name=\"Test Notebook\")\n            mock_find_notebook.return_value = notebook\n            \n            # Mock document doesn't exist yet\n            mock_find_doc.return_value = None\n            \n            # Mock document save\n            doc = Document.create(notebook_id=notebook_id, url=source_url, title=source_title)\n            mock_save_doc.return_value = doc\n            \n            # Act\n            result = runner.invoke(notebook_app, [\n                \"source\", \"add\", \n                notebook_id, \n                source_url,\n                \"--title\", source_title\n            ])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"Source added:\" in result.stdout\n            assert doc.id in result.stdout\n            assert source_title in result.stdout\n            mock_find_notebook.assert_called_once_with(notebook_id)\n            mock_find_doc.assert_called_once()\n            mock_save_doc.assert_called_once()\n\n    def test_list_sources(self, mock_session_context):\n        \"\"\"Test source list command.\"\"\"\n        # Arrange\n        notebook_id = \"notebook123\"\n        documents = [\n            Document.create(notebook_id=notebook_id, url=\"https://example1.com\", title=\"Example 1\"),\n            Document.create(notebook_id=notebook_id, url=\"https://example2.com\", title=\"Example 2\"),\n        ]\n        \n        # Mock repositories\n        from src.notebook.adapter.repository import NotebookRepository\n        from src.document.adapter.repository import DocumentRepository\n        \n        with patch.object(NotebookRepository, \"find_by_id\", new_callable=AsyncMock) as mock_find_notebook, \\\n             patch.object(DocumentRepository, \"list_by_notebook\", new_callable=AsyncMock) as mock_list_docs:\n            \n            # Mock notebook exists\n            notebook = Notebook.create(name=\"Test Notebook\")\n            mock_find_notebook.return_value = notebook\n            \n            # Mock document listing\n            from src.common import PaginatedResult\n            mock_result = PaginatedResult(\n                items=documents,\n                total=len(documents),\n                page=1,\n                size=10,\n            )\n            mock_list_docs.return_value = mock_result\n            \n            # Act\n            result = runner.invoke(notebook_app, [\"source\", \"list\", notebook_id])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"Example 1\" in result.stdout\n            assert \"Example 2\" in result.stdout\n            assert \"https://example1.com\" in result.stdout\n            assert \"https://example2.com\" in result.stdout\n            mock_find_notebook.assert_called_once_with(notebook_id)\n            mock_list_docs.assert_called_once()\n\n\nclass TestQueryCommands:\n    \"\"\"Tests for query CLI commands.\"\"\"\n\n    def test_query_notebook(self, mock_session_context):\n        \"\"\"Test query notebook command.\"\"\"\n        # Arrange\n        notebook_id = \"notebook123\"\n        query_text = \"What is the meaning of life?\"\n        \n        # Mock repositories and services\n        from src.notebook.adapter.repository import NotebookRepository\n        from src.chunk.adapter.repository import ChunkRepository\n        from src.document.adapter.repository import DocumentRepository\n        from src.query.adapter.pydantic_ai.agent import RAGAgent\n        \n        with patch.object(NotebookRepository, \"find_by_id\", new_callable=AsyncMock) as mock_find_notebook, \\\n             patch.object(ChunkRepository, \"vector_search\", new_callable=AsyncMock) as mock_vector_search, \\\n             patch.object(DocumentRepository, \"find_by_ids\", new_callable=AsyncMock) as mock_find_docs, \\\n             patch.object(RAGAgent, \"answer\", new_callable=AsyncMock) as mock_answer:\n            \n            # Mock notebook exists\n            notebook = Notebook.create(name=\"Test Notebook\")\n            mock_find_notebook.return_value = notebook\n            \n            # Mock vector search\n            chunk1 = MagicMock()\n            chunk1.id = \"chunk1\"\n            chunk1.document_id = \"doc1\"\n            chunk1.content = \"Content 1\"\n            \n            chunk2 = MagicMock()\n            chunk2.id = \"chunk2\"\n            chunk2.document_id = \"doc2\"\n            chunk2.content = \"Content 2\"\n            \n            mock_vector_search.return_value = [(chunk1, 0.9), (chunk2, 0.8)]\n            \n            # Mock document retrieval\n            doc1 = MagicMock()\n            doc1.id = \"doc1\"\n            doc1.title = \"Document 1\"\n            \n            doc2 = MagicMock()\n            doc2.id = \"doc2\"\n            doc2.title = \"Document 2\"\n            \n            mock_find_docs.return_value = [doc1, doc2]\n            \n            # Mock RAG answer\n            from src.query.schema.response import QueryResponse, Citation\n            mock_response = QueryResponse(\n                answer=\"This is the answer to your question.\",\n                citations=[\n                    Citation(\n                        citation_index=1,\n                        chunk_id=\"chunk1\",\n                        document_id=\"doc1\",\n                        document_title=\"Document 1\",\n                        text_snippet=\"Content 1\",\n                    )\n                ],\n            )\n            mock_answer.return_value = mock_response\n            \n            # Act\n            result = runner.invoke(query_app, [\"notebook\", notebook_id, query_text])\n            \n            # Assert\n            assert result.exit_code == 0\n            assert \"This is the answer to your question.\" in result.stdout\n            assert \"Document 1\" in result.stdout  # Citation should be shown\n            mock_find_notebook.assert_called_once_with(notebook_id)\n            mock_vector_search.assert_called_once()\n            mock_find_docs.assert_called_once()\n            mock_answer.assert_called_once()\n\n\nclass TestEvaluationCommands:\n    \"\"\"Tests for evaluation CLI commands.\"\"\"\n    \n    # Add tests for evaluation CLI commands here\n    # Following the same pattern as above\n```",
        "testStrategy": "1. Run the unit tests with pytest and ensure all CLI commands have test coverage\n2. Verify tests for notebook CLI commands (create, list)\n3. Verify tests for source CLI commands (add, list, delete)\n4. Verify tests for query CLI command\n5. Verify tests for evaluation CLI commands\n6. Use typer.testing.CliRunner to invoke CLI commands\n7. Mock database session and repositories to test command logic without database access",
        "priority": "low",
        "dependencies": [
          "3"
        ],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": "13",
        "title": "Implement Crawl Domain Model",
        "description": "Create the core domain entities for URL crawling feature: CrawlJob as the aggregate root to track crawling operations, CrawlStatus enum to represent the state of a crawl job, and DiscoveredUrl value object to represent each discovered URL and its status.",
        "details": "Implement the following domain models:\n\n1. CrawlStatus enum:\n   - Values: PENDING, IN_PROGRESS, COMPLETED, FAILED, CANCELLED\n   - Methods for state transition validation:\n     - is_processable (PENDING  IN_PROGRESS)\n     - is_terminal (COMPLETED, FAILED)\n     - can_cancel (PENDING, IN_PROGRESS)\n\n2. DiscoveredUrlStatus enum:\n   - Values: PENDING, INGESTED, SKIPPED, FAILED\n   - Method to check if URL can be processed\n\n3. DiscoveredUrl value object:\n   - Immutable (frozen pydantic BaseModel)\n   - Fields: url, depth, status, document_id (optional)\n   - Factory method for creation\n   - State transition methods:\n     - mark_ingested(document_id)  new instance with INGESTED status\n     - mark_skipped()  new instance with SKIPPED status\n     - mark_failed(error)  new instance with FAILED status\n\n4. CrawlJob entity:\n   - Immutable (frozen pydantic BaseModel)\n   - Fields as per PRD: id, notebook_id, seed_url, domain, max_depth, max_pages, url_include_pattern, url_exclude_pattern, status, total_discovered, total_ingested, error_message, created_at, updated_at\n   - Factory method: create(notebook_id, seed_url, max_depth, max_pages, url_include_pattern, url_exclude_pattern)\n   - Domain validation (max_depth > 0, max_pages > 0)\n   - State transition methods:\n     - mark_in_progress()  new instance with IN_PROGRESS status\n     - mark_completed()  new instance with COMPLETED status\n     - mark_failed(error_message)  new instance with FAILED status\n     - mark_cancelled()  new instance with CANCELLED status\n     - increment_discovered()  new instance with total_discovered + 1\n     - increment_ingested()  new instance with total_ingested + 1\n\nFollow the existing pattern of immutable domain entities with state transition methods seen in Document.",
        "testStrategy": "1. Unit tests for all domain entities:\n   - Test enum state transitions and validations\n   - Test CrawlJob factory method creates valid instances\n   - Test CrawlJob domain validation rejects invalid inputs (max_depth < 1, max_pages < 1)\n   - Test DiscoveredUrl state transitions produce new instances with correct state\n   - Test CrawlJob state transitions produce new instances with correct state\n   - Test increment methods correctly update counters\n   - Test validation of URL patterns\n   - Test equality for value objects\n\n2. Use Arrange-Act-Assert pattern in tests:\n   - Arrange: Create initial entity state\n   - Act: Call state transition method\n   - Assert: Verify new state and immutability of original\n\n3. Example test case:\n   def test_crawl_job_can_be_marked_in_progress():\n       # Arrange\n       job = CrawlJob.create(notebook_id=\"123\", seed_url=\"https://example.com\")\n       \n       # Act\n       updated_job = job.mark_in_progress()\n       \n       # Assert\n       assert job.status == CrawlStatus.PENDING  # Original unchanged\n       assert updated_job.status == CrawlStatus.IN_PROGRESS",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:39:43.627Z"
      },
      {
        "id": "14",
        "title": "Implement Crawl Persistence",
        "description": "Create the database schema (ORM models) and repository for persisting CrawlJob entities and DiscoveredUrl value objects, following the same patterns as existing DocumentRepository.",
        "details": "1. Create SQLAlchemy ORM models in `src/infrastructure/models/crawl.py`:\n   - CrawlJobSchema:\n     - id: String(32), PK\n     - notebook_id: String(32), FKnotebooks.id CASCADE\n     - seed_url: Text\n     - domain: String(255)\n     - max_depth: Integer\n     - max_pages: Integer\n     - url_include_pattern: Text, nullable\n     - url_exclude_pattern: Text, nullable\n     - status: String(20), indexed\n     - total_discovered: Integer, default 0\n     - total_ingested: Integer, default 0\n     - error_message: Text, nullable\n     - created_at, updated_at: DateTime(timezone=True)\n\n   - DiscoveredUrlSchema:\n     - id: String(32), PK\n     - crawl_job_id: String(32), FKcrawl_jobs.id CASCADE\n     - url: Text\n     - depth: Integer\n     - status: String(20)\n     - document_id: String(32), FKdocuments.id, nullable\n     - error_message: Text, nullable\n     - created_at, updated_at: DateTime(timezone=True)\n     - Unique constraint: (crawl_job_id, url)\n\n2. Implement CrawlJobMapper in `src/crawl/domain/mapper.py`:\n   - to_entity(record)  CrawlJob\n   - to_record(entity)  CrawlJobSchema\n\n3. Implement DiscoveredUrlMapper:\n   - to_entity(record)  DiscoveredUrl\n   - to_record(entity)  DiscoveredUrlSchema\n\n4. Implement CrawlJobRepository in `src/crawl/adapter/repository.py`:\n   - find_by_id(id)  CrawlJob | None\n   - find_by_notebook_id_and_url(notebook_id, url)  CrawlJob | None\n   - save(entity)  CrawlJob\n   - save_batch(entities)  list[CrawlJob]\n   - update_discovered_url(crawl_id, url, update_data)  DiscoveredUrl\n   - list_by_notebook(notebook_id, query)  PaginationSchema[CrawlJob]\n   - find_discovered_urls(crawl_id, status=None)  list[DiscoveredUrl]\n   - delete(id)  bool\n\n5. Create Alembic migration script for the new tables\n\nFollowing the SQLAlchemy patterns used in the existing codebase, including:\n- Using UUIDs for primary keys\n- Foreign key relationships with CASCADE delete\n- Indexes on commonly queried fields\n- Proper timestamp handling\n- Support for pagination",
        "testStrategy": "1. Unit tests for repository operations:\n   - Test CRUD operations for CrawlJob and DiscoveredUrl\n   - Test find operations with various filters\n   - Test pagination for list operations\n   - Test cascading deletes (when a crawl job is deleted, all its discovered URLs are deleted)\n   - Test unique constraint on (crawl_job_id, url)\n\n2. Test mappers:\n   - Test bidirectional mapping (entityrecordentity)\n   - Test handling of all field types and nullable fields\n\n3. Test database constraints:\n   - Test foreign key constraints\n   - Test unique constraints\n   - Test default values\n\n4. Example test:\n   async def test_find_crawl_job_by_id():\n       # Arrange\n       job = CrawlJob.create(notebook_id=\"123\", seed_url=\"https://example.com\")\n       saved = await repository.save(job)\n       \n       # Act\n       retrieved = await repository.find_by_id(saved.id)\n       \n       # Assert\n       assert retrieved is not None\n       assert retrieved.id == saved.id\n       assert retrieved.notebook_id == \"123\"",
        "priority": "high",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:42:15.716Z"
      },
      {
        "id": "15",
        "title": "Implement Link Discovery Service",
        "description": "Create a service that fetches a web page and extracts all internal links (same domain). This service will handle URL discovery, normalization, and filtering based on domain and patterns.",
        "details": "1. Create a DiscoveredLink value object in `src/crawl/domain/model.py`:\n   - Immutable (frozen pydantic BaseModel)\n   - Fields: url (str), anchor_text (str | None)\n\n2. Implement LinkDiscoveryService in `src/crawl/service/link_discovery.py`:\n   - Method: discover_links(url: str, domain: str)  list[DiscoveredLink]\n   - Implementation:\n     a. Use existing ContentExtractorPort to fetch the page content or use httpx directly\n     b. Parse HTML to extract all <a href=\"...\"> tags and their text content\n     c. Normalize URLs (convert relative to absolute, remove fragments)\n     d. Filter to same domain only (match against base domain)\n     e. Return list of DiscoveredLink objects\n\n3. Add pattern filtering to LinkDiscoveryService:\n   - Method: filter_urls(links: list[DiscoveredLink], include_pattern: str | None, exclude_pattern: str | None)  list[DiscoveredLink]\n   - Implementation:\n     a. If include_pattern is provided, only keep URLs matching the regex pattern\n     b. If exclude_pattern is provided, exclude URLs matching the regex pattern\n     c. Return filtered list of links\n\n4. Add domain extraction utility:\n   - Method: extract_domain(url: str)  str\n   - Implementation: Parse URL and extract domain for scope limiting\n\n5. Add URL normalization utility:\n   - Method: normalize_url(url: str, base_url: str)  str\n   - Implementation:\n     a. Resolve relative URLs to absolute using base URL\n     b. Remove fragments (#section) from URLs\n     c. Optionally remove query parameters\n     d. Ensure protocol and domain are included",
        "testStrategy": "1. Unit tests for LinkDiscoveryService:\n   - Test link extraction from sample HTML content\n   - Test URL normalization (relativeabsolute, fragment removal)\n   - Test domain filtering (same domain only)\n   - Test pattern filtering (include/exclude regex patterns)\n\n2. Test with various HTML structures:\n   - Regular <a href> links\n   - Relative URLs (same path, root relative, etc.)\n   - Links with fragments and query parameters\n   - Malformed URLs\n   - Non-HTTP schemes (mailto:, javascript:, etc.)\n\n3. Test domain extraction utility:\n   - Test with various URL formats\n   - Test subdomains (should match parent domain)\n   - Test URLs with and without www prefix\n\n4. Test with mock ContentExtractorPort:\n   - Mock HTTP responses to test integration without network calls\n   - Test error handling for failed requests\n\n5. Example test:\n   async def test_extract_internal_links():\n       # Arrange\n       html = \"\"\"\n       <html><body>\n         <a href=\"https://example.com/page1\">Page 1</a>\n         <a href=\"/page2\">Page 2</a>\n         <a href=\"https://other-site.com/page3\">External</a>\n       </body></html>\n       \"\"\"\n       service = LinkDiscoveryService()\n       mock_extractor = Mock()\n       mock_extractor.extract.return_value = html\n       service._extractor = mock_extractor\n       \n       # Act\n       links = await service.discover_links(\"https://example.com\", \"example.com\")\n       \n       # Assert\n       assert len(links) == 2  # Only internal links\n       assert any(link.url == \"https://example.com/page1\" for link in links)\n       assert any(link.url == \"https://example.com/page2\" for link in links)",
        "priority": "medium",
        "dependencies": [
          "13"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:44:01.011Z"
      },
      {
        "id": "16",
        "title": "Implement Crawl Execution Service",
        "description": "Create the core crawl orchestration service that performs BFS traversal of pages, discovers links, and creates documents for each discovered URL, coordinating with the existing ingestion pipeline.",
        "details": "1. Implement CrawlService in `src/crawl/service/crawl_service.py`:\n   - Fields:\n     - _document_repository: DocumentRepository\n     - _crawl_repository: CrawlJobRepository\n     - _link_discovery: LinkDiscoveryService\n     - _ingestion_service: BackgroundIngestionService\n\n   - Method: `async def execute(self, crawl_job_id: str)  CrawlJob`:\n     - Implementation:\n       a. Load the crawl job from repository\n       b. Use BFS algorithm to traverse pages starting from seed URL:\n          - Queue = [(seed_url, 0)] (URL and depth)\n          - While queue is not empty and within limits:\n            * Dequeue (url, depth)\n            * Skip if already visited or depth > max_depth\n            * Add to visited URLs\n            * Discover links on the page\n            * Filter links by domain and patterns\n            * For each link: add to queue if not visited, depth+1 <= max_depth\n            * Create document and trigger ingestion\n       c. Update CrawlJob status and counters\n       d. Return updated CrawlJob\n\n   - Method: `async def _create_document(self, notebook_id: str, url: str, crawl_job_id: str, depth: int)  str | None`:\n     - Create Document entity\n     - Save to repository\n     - Trigger background ingestion\n     - Update DiscoveredUrl status and document_id\n     - Return document ID if successful\n\n   - Method: `async def _process_url(self, url: str, crawl_job: CrawlJob, depth: int, visited: set[str])  None`:\n     - Process a single URL\n     - Create document, trigger ingestion\n     - Update crawl job counters\n     - Discover and process new links if depth < max_depth\n\n2. Implement BackgroundCrawlService in `src/crawl/service/background_crawl_service.py`:\n   - Similar to BackgroundIngestionService, but for crawling\n   - Track active crawl tasks\n   - Start crawl jobs in the background\n   - Method: `def trigger_crawl(self, crawl_job: CrawlJob)  None`:\n     - Start an asyncio task for the crawl\n   - Method: `async def _process_with_cleanup(self, crawl_job_id: str)  None`:\n     - Process the crawl and clean up afterward\n\n3. Add deduplication checking:\n   - Check if URL already exists in the notebook (reuse find_by_notebook_and_url)\n   - Skip if already exists to avoid duplicate ingestion",
        "testStrategy": "1. Unit tests for CrawlService:\n   - Test BFS traversal algorithm with mock repositories and services\n   - Test depth limiting and max pages limiting\n   - Test URL pattern filtering\n   - Test error handling and recovery (individual page failures)\n   - Test document creation and integration with ingestion service\n   - Test deduplication of URLs\n\n2. Test BackgroundCrawlService:\n   - Test task creation and tracking\n   - Test error handling in background tasks\n   - Test cleanup after task completion or failure\n\n3. Integration tests:\n   - Test end-to-end crawl process with mock HTTP responses\n   - Test integration with document ingestion pipeline\n   - Test concurrent crawls for different notebooks\n\n4. Edge case testing:\n   - Test with cycles in the link graph\n   - Test with very large numbers of links\n   - Test with rate limiting considerations\n   - Test recovery from temporary failures\n\n5. Example test:\n   async def test_crawl_respects_depth_limit():\n       # Arrange\n       crawl_job = CrawlJob.create(\n           notebook_id=\"123\",\n           seed_url=\"https://example.com\",\n           max_depth=2,\n           max_pages=10\n       )\n       mock_repo = Mock()\n       mock_repo.find_by_id.return_value = crawl_job\n       mock_link_discovery = Mock()\n       mock_link_discovery.discover_links.side_effect = [\n           # Seed URL (depth 0) has 2 links\n           [DiscoveredLink(url=\"https://example.com/page1\"), \n            DiscoveredLink(url=\"https://example.com/page2\")],\n           # Page1 (depth 1) has 2 links\n           [DiscoveredLink(url=\"https://example.com/page1-1\"), \n            DiscoveredLink(url=\"https://example.com/page1-2\")],\n           # Page2 (depth 1) has 2 links\n           [DiscoveredLink(url=\"https://example.com/page2-1\"), \n            DiscoveredLink(url=\"https://example.com/page2-2\")],\n           # The rest are at depth 2, should not be crawled\n           [], [], [], []\n       ]\n       service = CrawlService(\n           document_repository=Mock(),\n           crawl_repository=mock_repo,\n           link_discovery=mock_link_discovery,\n           ingestion_service=Mock()\n       )\n       \n       # Act\n       await service.execute(\"crawl123\")\n       \n       # Assert\n       # Should only create documents for seed + 4 pages (2 at depth 1 + their 2 pages each at depth 2)\n       assert service._document_repository.save.call_count == 5\n       # Should only discover links from 3 pages (seed + 2 at depth 1)\n       assert mock_link_discovery.discover_links.call_count == 3",
        "priority": "high",
        "dependencies": [
          "13",
          "14",
          "15"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:45:46.231Z"
      },
      {
        "id": "17",
        "title": "Implement Crawl Command Handlers",
        "description": "Create the command handlers, schemas (command/response/query), following the existing DDD handler pattern for crawl operations like starting a crawl, getting status, and cancelling a crawl.",
        "details": "1. Create schema definitions in `src/crawl/schema/`:\n   \n   a. Command schemas (command.py):\n      - StartCrawl:\n        - url: pydantic.HttpUrl (seed URL)\n        - max_depth: int = 2\n        - max_pages: int = 50\n        - url_include_pattern: str | None = None\n        - url_exclude_pattern: str | None = None\n      - CancelCrawl: (empty, ID from path)\n   \n   b. Response schemas (response.py):\n      - CrawlJobId:\n        - id: str\n      - DiscoveredUrlDetail:\n        - url: str\n        - depth: int\n        - status: str\n        - document_id: str | None\n      - CrawlJobDetail:\n        - id, notebook_id, seed_url, domain\n        - max_depth, max_pages\n        - url_include_pattern, url_exclude_pattern\n        - status, total_discovered, total_ingested\n        - error_message\n        - created_at, updated_at\n        - discovered_urls: list[DiscoveredUrlDetail] (optional)\n   \n   c. Query schemas (query.py):\n      - ListCrawlJobs(pagination.ListQuery):\n        - notebook_id: str\n\n2. Implement handlers in `src/crawl/handler/handlers.py`:\n   \n   a. StartCrawlHandler:\n      - Dependencies: notebook_repository, crawl_repository, background_crawl_service\n      - Method: async def handle(self, notebook_id: str, cmd: command.StartCrawl)  response.CrawlJobId\n      - Implementation:\n        - Validate notebook exists\n        - Extract domain from URL\n        - Create CrawlJob in PENDING status\n        - Save to repository\n        - Trigger background crawl\n        - Return CrawlJobId\n   \n   b. GetCrawlJobHandler:\n      - Dependencies: crawl_repository\n      - Method: async def handle(self, crawl_job_id: str, include_urls: bool = False)  response.CrawlJobDetail\n      - Implementation:\n        - Find crawl job by ID\n        - If include_urls, load discovered URLs\n        - Return CrawlJobDetail\n   \n   c. ListCrawlJobsHandler:\n      - Dependencies: notebook_repository, crawl_repository\n      - Method: async def handle(self, qry: query.ListCrawlJobs)  pagination.PaginationSchema[response.CrawlJobDetail]\n      - Implementation:\n        - Validate notebook exists\n        - List crawl jobs with pagination\n        - Return paginated response\n   \n   d. CancelCrawlHandler:\n      - Dependencies: crawl_repository, background_crawl_service\n      - Method: async def handle(self, crawl_job_id: str)  None\n      - Implementation:\n        - Find crawl job by ID\n        - If in processable state, mark as CANCELLED\n        - Try to cancel background task\n        - Return None (HTTP 204)",
        "testStrategy": "1. Unit tests for command handlers:\n   - Test StartCrawlHandler creates valid crawl job and triggers background task\n   - Test GetCrawlJobHandler returns correct detail response\n   - Test ListCrawlJobsHandler returns paginated results\n   - Test CancelCrawlHandler cancels a running crawl\n\n2. Test validation logic:\n   - Test notebook existence validation\n   - Test URL validation\n   - Test invalid parameters (negative max_depth, etc.)\n   - Test error responses for not found resources\n\n3. Test with mock repositories and services:\n   - Use dependency injection for testability\n   - Mock repository responses\n   - Verify correct repository method calls\n   - Test state transitions\n\n4. Example test:\n   async def test_start_crawl_creates_job_and_triggers_background_process():\n       # Arrange\n       notebook_repo = Mock()\n       notebook_repo.find_by_id.return_value = Notebook(id=\"notebook123\", name=\"Test\")\n       \n       crawl_repo = Mock()\n       crawl_repo.save.side_effect = lambda job: job\n       \n       background_service = Mock()\n       \n       handler = StartCrawlHandler(\n           notebook_repository=notebook_repo,\n           crawl_repository=crawl_repo,\n           background_crawl_service=background_service\n       )\n       \n       cmd = command.StartCrawl(\n           url=\"https://example.com\",\n           max_depth=3,\n           max_pages=100\n       )\n       \n       # Act\n       result = await handler.handle(\"notebook123\", cmd)\n       \n       # Assert\n       assert result is not None\n       assert result.id is not None\n       assert crawl_repo.save.call_count == 1\n       saved_job = crawl_repo.save.call_args[0][0]\n       assert saved_job.notebook_id == \"notebook123\"\n       assert saved_job.seed_url == \"https://example.com\"\n       assert saved_job.max_depth == 3\n       assert saved_job.max_pages == 100\n       assert saved_job.status == CrawlStatus.PENDING\n       assert background_service.trigger_crawl.call_count == 1",
        "priority": "medium",
        "dependencies": [
          "13",
          "14",
          "16"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:47:35.516Z"
      },
      {
        "id": "18",
        "title": "Implement Crawl API Endpoints",
        "description": "Create FastAPI API endpoints for the crawl feature, following existing patterns for dependency injection, response models, and error handling.",
        "details": "1. Create API endpoints in `src/crawl/entrypoint/api.py`:\n   \n   a. POST /api/v1/notebooks/{notebook_id}/crawl\n      - Summary: \"Start crawling URLs from a seed URL\"\n      - Request body: StartCrawl schema\n      - Response: CrawlJobId\n      - Status codes: 201 Created, 404 Not Found, 422 Validation Error\n\n   b. GET /api/v1/notebooks/{notebook_id}/crawl\n      - Summary: \"List crawl jobs for a notebook\"\n      - Query parameters: page, size\n      - Response: PaginationSchema[CrawlJobDetail]\n      - Status codes: 200 OK, 404 Not Found\n\n   c. GET /api/v1/crawl/{crawl_job_id}\n      - Summary: \"Get crawl job details\"\n      - Query parameters: include_urls (bool, default=False)\n      - Response: CrawlJobDetail\n      - Status codes: 200 OK, 404 Not Found\n\n   d. POST /api/v1/crawl/{crawl_job_id}/cancel\n      - Summary: \"Cancel a crawl job\"\n      - Response: None (204 No Content)\n      - Status codes: 204 No Content, 404 Not Found\n\n2. Add API documentation:\n   - Add detailed descriptions for all endpoints\n   - Document all response models\n   - Document error responses\n\n3. Add FastAPI dependencies in `src/crawl/dependency.py`:\n   - Define dependencies for handlers\n   - Use dependency injection pattern\n\n4. Register routers in the main FastAPI app:\n   - Update `src/__main__.py` to include crawl router\n\n5. Add proper error handling:\n   - Convert domain exceptions to HTTP responses\n   - Provide meaningful error messages",
        "testStrategy": "1. Test API endpoints with pytest and async test client:\n   - Test each endpoint's happy path\n   - Test error responses for invalid inputs\n   - Test not found responses\n   - Test validation errors\n\n2. Test dependency injection:\n   - Verify handlers are correctly instantiated with dependencies\n   - Test FastAPI dependency overrides for testing\n\n3. Test HTTP status codes:\n   - 201 Created for successful creation\n   - 200 OK for successful retrieval\n   - 204 No Content for successful cancellation\n   - 404 Not Found for missing resources\n   - 422 Validation Error for invalid inputs\n\n4. Test request validation:\n   - Test URL format validation\n   - Test numeric parameter constraints\n   - Test required vs. optional fields\n\n5. Example test:\n   async def test_start_crawl_endpoint():\n       # Arrange\n       app = FastAPI()\n       app.include_router(crawl_router)\n       client = AsyncClient(app=app, base_url=\"http://test\")\n       \n       # Override dependencies\n       mock_handler = Mock()\n       mock_handler.handle.return_value = CrawlJobId(id=\"crawl123\")\n       app.dependency_overrides[get_start_crawl_handler] = lambda: mock_handler\n       \n       # Act\n       response = await client.post(\n           \"/api/v1/notebooks/notebook123/crawl\",\n           json={\"url\": \"https://example.com\", \"max_depth\": 2, \"max_pages\": 50}\n       )\n       \n       # Assert\n       assert response.status_code == 201\n       assert response.json() == {\"id\": \"crawl123\"}\n       mock_handler.handle.assert_called_once()\n       call_args = mock_handler.handle.call_args\n       assert call_args[0][0] == \"notebook123\"  # notebook_id\n       assert call_args[0][1].url == \"https://example.com\"",
        "priority": "medium",
        "dependencies": [
          "17"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:49:12.996Z"
      },
      {
        "id": "19",
        "title": "Implement Crawl CLI Commands",
        "description": "Add CLI commands for the crawl feature using Typer, including commands to start, monitor, list, and cancel crawls.",
        "details": "1. Create CLI commands in `src/cli/commands/crawl.py`:\n   \n   a. Command: `ntlm source crawl <notebook_id> <url> [--depth N] [--max-pages N] [--include PATTERN] [--exclude PATTERN]`\n      - Implementation:\n        - Create StartCrawl command\n        - Call StartCrawlHandler\n        - Display crawl job ID and monitoring instructions\n        - Optional: Track progress in real-time\n\n   b. Command: `ntlm crawl status <crawl_job_id>`\n      - Implementation:\n        - Call GetCrawlJobHandler with include_urls=True\n        - Display crawl status, progress, and discovered URLs\n        - Format output with rich tables and styling\n\n   c. Command: `ntlm crawl list <notebook_id>`\n      - Implementation:\n        - Call ListCrawlJobsHandler\n        - Display list of crawl jobs with status and progress\n        - Format as rich table\n\n   d. Command: `ntlm crawl cancel <crawl_job_id>`\n      - Implementation:\n        - Call CancelCrawlHandler\n        - Display confirmation message\n        - Handle errors (not found, already completed)\n\n2. Add rich terminal output:\n   - Use rich progress bars for status display\n   - Use rich tables for listing crawl jobs and URLs\n   - Use colored output for different statuses\n\n3. Add error handling:\n   - Handle common errors (not found, validation)\n   - Display user-friendly error messages\n   - Add retry options for transient errors\n\n4. Register CLI commands in main Typer app:\n   - Update `src/cli/__init__.py` to include crawl commands",
        "testStrategy": "1. Test CLI commands with Typer testing utilities:\n   - Test command invocation with various arguments\n   - Test output formatting\n   - Test error handling and messages\n\n2. Test integration with handlers:\n   - Verify correct handler calls\n   - Test data passing between CLI and handlers\n   - Mock handler responses for testing\n\n3. Test user experience:\n   - Test progress display updates\n   - Test formatting of tables and status info\n   - Test error messages are user-friendly\n\n4. Example test:\n   def test_crawl_status_command():\n       # Arrange\n       runner = CliRunner()\n       mock_handler = Mock()\n       mock_handler.handle.return_value = CrawlJobDetail(\n           id=\"crawl123\",\n           notebook_id=\"notebook456\",\n           seed_url=\"https://example.com\",\n           status=\"completed\",\n           total_discovered=10,\n           total_ingested=8,\n           # ... other fields\n       )\n       \n       # Patch dependency to return mock handler\n       with patch(\"src.cli.commands.crawl.get_handler\", return_value=mock_handler):\n           # Act\n           result = runner.invoke(app, [\"crawl\", \"status\", \"crawl123\"])\n           \n           # Assert\n           assert result.exit_code == 0\n           assert \"Crawl Status: completed\" in result.stdout\n           assert \"Total Discovered: 10\" in result.stdout\n           assert \"Total Ingested: 8\" in result.stdout",
        "priority": "low",
        "dependencies": [
          "17"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:49:13.610Z"
      },
      {
        "id": "20",
        "title": "Implement Dependency Injection Container and Integration",
        "description": "Create the dependency injection container for the crawl module and integrate it into the main application, wiring all components together.",
        "details": "1. Create dependency container in `src/crawl/dependency.py`:\n   \n   a. Define container with dependencies:\n   ```python\n   from dependency_injector import containers, providers\n   \n   class CrawlContainer(containers.DeclarativeContainer):\n       \"\"\"Crawl module dependency container.\"\"\"\n       \n       config = providers.Configuration()\n       db = providers.Dependency()\n       document_container = providers.Dependency()\n       \n       # Repositories\n       crawl_repository = providers.Factory(\n           CrawlJobRepository,\n           session=db.provided.session,\n       )\n       \n       # Services\n       link_discovery_service = providers.Factory(\n           LinkDiscoveryService,\n           content_extractor=document_container.content_extractor,\n       )\n       \n       crawl_service = providers.Factory(\n           CrawlService,\n           document_repository=document_container.document_repository,\n           crawl_repository=crawl_repository,\n           link_discovery=link_discovery_service,\n           ingestion_service=document_container.background_ingestion_service,\n       )\n       \n       background_crawl_service = providers.Singleton(\n           BackgroundCrawlService,\n           crawl_service=crawl_service,\n       )\n       \n       # Handlers\n       start_crawl_handler = providers.Factory(\n           StartCrawlHandler,\n           notebook_repository=document_container.notebook_repository,\n           crawl_repository=crawl_repository,\n           background_crawl_service=background_crawl_service,\n       )\n       \n       get_crawl_job_handler = providers.Factory(\n           GetCrawlJobHandler,\n           crawl_repository=crawl_repository,\n       )\n       \n       list_crawl_jobs_handler = providers.Factory(\n           ListCrawlJobsHandler,\n           notebook_repository=document_container.notebook_repository,\n           crawl_repository=crawl_repository,\n       )\n       \n       cancel_crawl_handler = providers.Factory(\n           CancelCrawlHandler,\n           crawl_repository=crawl_repository,\n           background_crawl_service=background_crawl_service,\n       )\n   ```\n\n2. Configure FastAPI dependencies:\n   ```python\n   def get_start_crawl_handler() -> StartCrawlHandler:\n       return crawl_container.start_crawl_handler()\n   \n   def get_get_crawl_job_handler() -> GetCrawlJobHandler:\n       return crawl_container.get_crawl_job_handler()\n   \n   def get_list_crawl_jobs_handler() -> ListCrawlJobsHandler:\n       return crawl_container.list_crawl_jobs_handler()\n   \n   def get_cancel_crawl_handler() -> CancelCrawlHandler:\n       return crawl_container.cancel_crawl_handler()\n   ```\n\n3. Integrate with main application container in `src/dependency/__init__.py`:\n   - Add CrawlContainer to main app container\n   - Wire dependencies between containers\n\n4. Initialize background crawl service with app lifecycle in `src/__main__.py`:\n   ```python\n   @app.on_event(\"startup\")\n   async def startup_crawl_service():\n       # Initialize crawl service\n   \n   @app.on_event(\"shutdown\")\n   async def shutdown_crawl_service():\n       # Cleanup any running crawl tasks\n   ```",
        "testStrategy": "1. Test container configuration:\n   - Test all dependencies are correctly wired\n   - Test container instantiation and resolution\n   - Test overriding dependencies for testing\n\n2. Test FastAPI dependency functions:\n   - Test dependency resolution in API endpoints\n   - Verify correct handlers are injected\n\n3. Test integration with app lifecycle:\n   - Test startup and shutdown events\n   - Verify background services are properly initialized\n   - Test cleanup on shutdown\n\n4. Test component interactions:\n   - Test end-to-end interaction between components\n   - Verify message passing between components\n   - Test correct dependency resolution for nested dependencies\n\n5. Example test:\n   def test_crawl_container_wiring():\n       # Arrange\n       container = CrawlContainer()\n       container.db = Mock()\n       container.document_container = Mock()\n       \n       # Configure mocks\n       db_session = Mock()\n       container.db.provided.session = db_session\n       container.document_container.content_extractor = Mock()\n       container.document_container.document_repository = Mock()\n       container.document_container.notebook_repository = Mock()\n       container.document_container.background_ingestion_service = Mock()\n       \n       # Act - resolve services and handlers\n       crawl_service = container.crawl_service()\n       start_handler = container.start_crawl_handler()\n       \n       # Assert\n       assert crawl_service._document_repository is container.document_container.document_repository\n       assert crawl_service._crawl_repository is not None\n       assert start_handler._notebook_repository is container.document_container.notebook_repository",
        "priority": "medium",
        "dependencies": [
          "16",
          "17",
          "18",
          "19"
        ],
        "status": "done",
        "subtasks": [],
        "updatedAt": "2026-02-08T03:50:45.394Z"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-02-08T03:50:45.394Z",
      "taskCount": 20,
      "completedCount": 8,
      "tags": [
        "master"
      ]
    }
  }
}